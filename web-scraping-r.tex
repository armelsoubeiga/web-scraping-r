\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Collecte des Données Web avec R},
  pdfauthor={Armel SOUBEIGA},
  pdfborder={0 0 0},
  breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{booktabs}
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Collecte des Données Web avec R}
\author{Armel SOUBEIGA}
\date{juillet 22, 2020}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{conjecture}{Conjecture}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{welcome}{%
\chapter*{Welcome}\label{welcome}}
\addcontentsline{toc}{chapter}{Welcome}

This is the website for \emph{Data Science at the Command Line}, published by O'Reilly October 2014 First Edition. This hands-on guide demonstrates how the flexibility of the command line can help you become a more efficient and productive data scientist. You'll learn how to combine small, yet powerful, command-line tools to quickly obtain, scrub, explore, and model your data.

To get you started---whether you're on Windows, macOS, or Linux---author \href{https://twitter.com/jeroenhjanssens}{Jeroen Janssens} has developed a \href{https://hub.docker.com/r/datascienceworkshops/data-science-at-the-command-line}{Docker image} packed with over 80 command-line tools.

Discover why the command line is an agile, scalable, and extensible technology. Even if you're already comfortable processing data with, say, Python or R, you'll greatly improve your data science workflow by also leveraging the power of the command line.

\includegraphics[width=349px,class="cover"]{images/cover}

\begin{itemize}
\tightlist
\item
  Obtain data from websites, APIs, databases, and spreadsheets
\item
  Perform scrub operations on text, CSV, HTML/XML, and JSON
\item
  Explore data, compute descriptive statistics, and create visualizations
\item
  Manage your data science workflow
\item
  Create reusable command-line tools from one-liners and existing Python or R code
\item
  Parallelize and distribute data-intensive pipelines
\item
  Model data with dimensionality reduction, clustering, regression, and classification algorithms
\end{itemize}

\begin{quote}
The Unix philosophy of simple tools, each doing one job well, then cleverly piped together, is embodied by the command line. Jeroen expertly discusses how to bring that philosophy into your work in data science, illustrating how the command line is not only the world of file input/output, but also the world of data manipulation, exploration, and even modeling.

\textbf{---Chris H. Wiggins}

Associate Professor in the Department of Applied Physics and Applied Mathematics at Columbia University and Chief Data Scientist at \emph{The New York Times}
\end{quote}

\begin{quote}
This book explains how to integrate common data science tasks into a coherent workflow. It's not just about tactics for breaking down problems, it's also about strategies for assembling the pieces of the solution.

\textbf{---John D. Cook}

Consultant in applied mathematics, statistics, and technical computing
\end{quote}

If you find this content useful, please consider supporting the work by either:

\begin{itemize}
\tightlist
\item
  Sponsoring the author on \href{https://github.com/sponsors/jeroenjanssens/}{GitHub Sponsors}.
\item
  Buying the book on \href{https://www.amazon.com/Data-Science-Command-Line-Time-Tested/dp/1491947853}{Amazon} or \href{https://www.bol.com/nl/p/data-science-at-the-command-line/9200000031673818}{bol.com}.
\item
  Writing a review on \href{https://www.amazon.com/Data-Science-Command-Line-Time-Tested/dp/1491947853}{Amazon} or \href{https://www.goodreads.com/book/show/22967424-data-science-at-the-command-line}{Goodreads}.
\item
  Starring the \href{https://github.com/jeroenjanssens/data-science-at-the-command-line}{Github repository} or \href{https://hub.docker.com/u/datascienceworkshops/}{Docker image}.
\end{itemize}

This work is licensed under the \href{https://creativecommons.org/licenses/by-nd/4.0/}{Creative Commons Attribution-NoDerivatives 4.0 International License}.

\begin{center}\includegraphics[width=350px]{images/data-science-workshops} \end{center}

Did you know that the author provides training about this topic and other topics such as R and Python? If you and your colleagues would like to learn from Jeroen in person, please contact \href{https://www.datascienceworkshops.com}{Data Science Workshops B.V.} for more information.

\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

Data science is an exciting field to work in. It's also still very young. Unfortunately, many people, and especially companies, believe that you need new technology in order to tackle the problems posed by data science. However, as this book demonstrates, many things can be accomplished by using the command line instead, and sometimes in a much more efficient way.

Around five years ago, during my PhD program, I gradually switched from using Microsoft Windows to Linux. Because it was a bit scary at first, I started with having both operating systems installed next to each other (known as dual-boot). The urge to switch back and forth between Microsoft Windows faded and at some point I was even tinkering around with Arch Linux, which allows you to build up your own custom Linux machine from scratch. All you're given is the command line, and it's up to you what you want to make of it. Out of necessity I quickly became very comfortable using the command line. Eventually, as spare time got more precious, I settled down with a Linux distribution known as Ubuntu because of its ease of use and large community. However, the command line is still where I'm spending most of time.

It actually hasn't been too long ago that I realized that the command line is not just for installing software, system configuration, and searching files. I started learning about command-line tools such as \texttt{cut}, \texttt{sort}, and \texttt{sed}. These are examples of command-line tools that take data as input, do something to it, and print the result. Ubuntu comes with quite a few of them. Once I understood the potential of combining these small tools, I was hooked.

After my PhD, when I became a data scientist, I wanted to use this approach to do data science as much as possible. Thanks to a couple of new, open-source command-line tools including \texttt{xml2json}, \texttt{jq}, and \texttt{json2csv} I was even able to use the command line for tasks such as scraping websites and processing lots of JSON data. In September 2013, I decided to write a blog post titled \emph{Seven Command-line Tools for Data Science}, which is available at \url{http://www.jeroenjanssens.com/2013/09/19/seven-command-line-tools-for-data-science.html}. To my surprise, the blog post got quite some attention and I received a lot of suggestions of other command-line tools. I started wondering whether this blog post could be turned into a book. I'm pleased that, some ten months later, with the help of many talented people (see the acknowledgments below), the answer is a yes.

I am sharing this personal story not so much because I think you should know how this book came about, but because I want to you know that I had to learn about the command line as well. Because the command line is so different from using a graphical user interface, it can seem scary at first. But if I can learn it, then you can as well. No matter what your current operating system is and no matter how you currently work with data, after reading this book you will be able to do data science at the command line. If you're already familiar with the command line, or even if you're already dreaming in shell scripts, chances are that you'll still discover a few interesting tricks or command-line tools to use for your next data science project.

\hypertarget{what-to-expect-from-this-book}{%
\section*{What to Expect from This Book}\label{what-to-expect-from-this-book}}
\addcontentsline{toc}{section}{What to Expect from This Book}

In this book, we're going to obtain, scrub, explore, and model data - a lot of it. This book is not so much about how become \emph{better} at those data science tasks. There are already great resources available that discuss, for example, when to apply which statistical test or how data can be best visualized. Instead, this practical book aims to make you more \emph{efficient} and \emph{productive} by teaching you how to perform those data science tasks at the command line.

While this book discusses over 80 command-line tools, it's not the tools themselves that matter most. Some command-line tools have been around for a very long time, while others will be replaced by better ones. There are even command-line tools that are being created as you're reading this. In the past nine months, I have discovered many amazing command-line tools. Unfortunately, some of them were discovered too late to be included in the book. In short, command-line tools come and go. But that's OK.

What matters most is the underlying idea of working with tools, pipes, and data. Most of the command-line tools do one thing and do it well. This is part the UNIX philosophy, which makes several appearances throughout the book. Once you become familiar with the command line, know how to combine command-line tools, and can even create new ones, you have developed an invaluable skill.

\hypertarget{how-to-read-this-book}{%
\section*{How to Read This Book}\label{how-to-read-this-book}}
\addcontentsline{toc}{section}{How to Read This Book}

In general, you're advised to read this book in a linear fashion. Once a concept or command-line tool has been introduced, chances are that we employ it in a later chapter. For example, in Chapter 9, we make heavy use of \texttt{parallel}, which is introduced extensively in Chapter 8.

Data science is a broad field that intersects many other fields such as programming, data visualization, and machine learning. As a result, this book touches on many interesting topics which unfortunately cannot be discussed at full length. Throughout the book, there are suggestions for additional reading. It's not required to read this material in order to follow along with the book, but when you are interested, you know that there's much more to learn.

\hypertarget{who-this-book-is-for}{%
\section*{Who This Book Is For}\label{who-this-book-is-for}}
\addcontentsline{toc}{section}{Who This Book Is For}

This book makes just one assumption about you: that you work with data. It doesn't matter which programming language or statistical computing environment you're currently using. The book explains all the necessary concepts from the beginning.

It also doesn't matter whether your operating system is Microsoft Windows, MacOS, or some form of Linux. The book comes with a Docker image, which is an easy-to-install virtual environment. It allows you to run the command-line tools and follow along with the code examples in the same environment as this book was written. You don't have to waste time figuring out how to install all the command-line tools and their dependencies.

The book contains some code in Bash, Python, and R so it's helpful if you have some programming experience, but it's by no means required to follow along with the examples.

\hypertarget{acknowledgments}{%
\section*{Acknowledgments}\label{acknowledgments}}
\addcontentsline{toc}{section}{Acknowledgments}

First of all, I'd like to thank Mike Dewar and Mike Loukides for believing that my blog post \href{http://jeroenjanssens.com/2013/09/19/seven-command-line-tools-for-data-science.html}{Seven Command-Line Tools for Data Science}, which I wrote in September 2013, could be expanded into a book.

Special thanks to my technical reviewers Mike Dewar, Brian Eoff, and Shane Reustle for reading various drafts, meticulously testing all the commands, and providing invaluable feedback. Your efforts have improved the book greatly. The remaining errors are entirely my own responsibility.

I had the privilege of working together with three amazing editors, namely: Ann Spencer, Julie Steele, and Marie Beaugureau. Thank you for your guidance and for being such great liaisons with the many talented people at O'Reilly. Those people include: Laura Baldwin, Huguette Barriere, Sophia DeMartini, Yasmina Greco, Rachel James, Ben Lorica, Mike Loukides, and Christopher Pappas. There are many others whom I haven't met because they are operating behind the scenes. Together they ensured that working with O'Reilly has truly been a pleasure.

This book discusses over 80 command-line tools. Needless to say, without these tools, this book wouldn't have existed in the first place. I'm therefore extremely grateful to all the authors who created and contributed to these tools. The complete list of authors is unfortunately too long to include here; they are mentioned in the Appendix. Thanks especially to Aaron Crow, Jehiah Czebotar, Christoph Groskopf, Dima Kogan, Sergey Lisitsyn, Francisco J. Martin, and Ole Tange for providing help with their amazing command-line tools.

Eric Postma and Jaap van den Herik, who supervised me during my PhD program, deserve a special thank you. Over the course of five years they have taught me many lessons. Although writing a technical book is quite different from writing a PhD thesis, many of those lessons proved to be very helpful in the past nine months as well.

Finally, I'd like to thank my colleagues at YPlan, my friends, my family, and especially my wife Esther for supporting me and for pulling me away from the command line at just the right times.

\hypertarget{dedication}{%
\section*{Dedication}\label{dedication}}
\addcontentsline{toc}{section}{Dedication}

\emph{To my wife, Esther. Without her encouragement, support, and patience, this book would surely have ended up in \texttt{/dev/null}.}

\hypertarget{about-the-author}{%
\section*{About the Author}\label{about-the-author}}
\addcontentsline{toc}{section}{About the Author}

\href{http://jeroenjanssens.com/}{Jeroen Janssens} is the founder and CEO of \href{https://datascienceworkshops.com}{Data Science Workshops}, which provides on-the-job training and coaching in data visualisation, machine learning, and programming. Previously, he was an assistant professor at Jheronimus Academy of Data Science and a data scientist at Elsevier in Amsterdam and startups YPlan and Outbrain in New York City. He is the author of Data Science at the Command Line, published by O'Reilly Media. Jeroen holds a PhD in machine learning from Tilburg University and an MSc in artificial intelligence from Maastricht University. He can be found on \href{https://twitter.com/jeroenhjanssens/}{Twitter}, \href{http://www.linkedin.com/in/jeroenjanssens}{LinkedIn}, and \href{https://github.com/jeroenjanssens}{GitHub}.

\hypertarget{chapter-1-introduction}{%
\chapter{Introduction}\label{chapter-1-introduction}}

This book is about doing data science at the command line. Our aim is to make you a more efficient and productive data scientist by teaching you how to leverage the power of the command line.

Having both the terms ``data science'' and ``command line'' in the title requires an explanation. How can a technology that is over 40 years old\footnote{The development of the UNIX operating system \href{http://www.unix.org/what_is_unix/history_timeline.html}{started back in 1969}. It featured a command line since the beginning, and the important concept of pipes was added in 1973.} be of any use to a field that is only a few years young?

Today, data scientists can choose from an overwhelming collection of exciting technologies and programming languages. Python, R, Hadoop, Julia, Pig, Hive, and Spark are but a few examples. You may already have experience in one or more of these. If so, then why should you still care about the command line for doing data science? What does the command line have to offer that these other technologies and programming languages do not?

These are all valid questions. In this first chapter we will answer these questions as follows. First, we provide a practical definition of data science that will act as the backbone of this book. Second, we'll list five important advantages of the command line. Third, we demonstrate the power and flexibility of the command line through a real-world use case. By the end of this chapter we hope to have convinced you that the command line is indeed worth learning for doing data science.

\hypertarget{overview}{%
\section{Overview}\label{overview}}

In this chapter, you'll learn:

\begin{itemize}
\tightlist
\item
  A practical definition of data science
\item
  What the command line is exactly and how you can use it
\item
  Why the command line is a wonderful environment for doing data science
\end{itemize}

\hypertarget{data-science-is-osemn}{%
\section{Data Science is OSEMN}\label{data-science-is-osemn}}

The field of data science is still in its infancy, and as such, there exist various definitions of what it encompasses. Throughout this book we employ a very practical definition by \citet{Mason2010}. They define data science according to the following five steps: (1) obtaining data, (2) scrubbing data, (3) exploring data, (4) modeling data, and (5) interpreting data. Together, these steps form the OSEMN model (which is pronounced as \emph{awesome}). This definition serves as the backbone of this book because each step, (except step 5, interpreting data, which we explain below) has its own chapter. Below we explain what each step entails.

\begin{rmdcomment}
Although the five steps are discussed in a linear and incremental fashion, in practice it is very common to move back and forth between them or to perform multiple steps at the same time. Doing data science is an iterative and non-linear process. For example, once you have modeled your data, and you look at the results, you may decide to go back to the scrubbing step to the adjust the features of the data set.
\end{rmdcomment}

\hypertarget{obtaining-data}{%
\subsection{Obtaining Data}\label{obtaining-data}}

Without any data, there is little data science you can do. So the first step is the obtain data. Unless you are fortunate enough to already possess data, you may need to do one or more of the following:

\begin{itemize}
\tightlist
\item
  Download data from another location (e.g., a webpage or server)
\item
  Query data from a database or API (e.g., MySQL or Twitter)
\item
  Extract data from another file (e.g., an HTML file or spreadsheet)
\item
  Generate data yourself (e.g., reading sensors or taking surveys)
\end{itemize}

In \protect\hyperlink{chapter-3-obtaining-data}{Chapter 3}, we discuss several methods for obtaining data using the command line. The obtained data will most likely be in either text, CSV, JSON of HTML/XML format. The next step is to scrub this data.

\hypertarget{scrubbing-data}{%
\subsection{Scrubbing Data}\label{scrubbing-data}}

It is not uncommon that the obtained data has missing values, inconsistencies, errors, weird characters, or uninteresting columns. In that case, you have to \emph{scrub}, or clean, the data before you can do anything interesting with it. Common scrubbing operations include:

\begin{itemize}
\tightlist
\item
  Filtering lines
\item
  Extracting certain columns
\item
  Replacing values
\item
  Extracting words
\item
  Handling missing values
\item
  Converting data from one format to another
\end{itemize}

While we data scientists love to create exciting data visualizations and insightful models (steps 3 and 4), usually much effort goes into obtaining and scrubbing the required data first (steps 1 and 2). In \emph{Data Jujitsu}, \citet{Patil2012} states that ``80\% of the work in any data project is in cleaning the data''. In \protect\hyperlink{chapter-5-scrubbing-data}{Chapter 5}, we demonstrate how the command line can help accomplish such data scrubbing operations.

\hypertarget{exploring-data}{%
\subsection{Exploring Data}\label{exploring-data}}

Once you have scrubbed your data, you are ready to explore it. This is where it gets interesting because here you will get really into your data. In \protect\hyperlink{chapter-7-exploring-data}{Chapter 7} we show you how the command line can be used to:

\begin{itemize}
\tightlist
\item
  Look at your data
\item
  Derive statistics from your data
\item
  Create interesting visualizations
\end{itemize}

Command-line tools introduced in \protect\hyperlink{chapter-7-exploring-data}{Chapter 7} include: \texttt{csvstat} \citep{csvstat}, \texttt{feedgnuplot} \citep{feedgnuplot}, and \texttt{Rio} \citep{Rio}.

\hypertarget{modeling-data}{%
\subsection{Modeling Data}\label{modeling-data}}

If you want to explain the data or predict what will happen, you probably want to create a statistical model of your data. Techniques to create a model include clustering, classification, regression, and dimensionality reduction. The command line is not suitable for implementing a new model from scratch. It is, however, very useful to be able to build a model from the command line. In \protect\hyperlink{chapter-9-modeling-data}{Chapter 9} we will introduce several command-line tools that either build a model locally or employ an API to perform the computation in the cloud.

\hypertarget{interpreting-data}{%
\subsection{Interpreting Data}\label{interpreting-data}}

The final and perhaps most important step in the OSEMN model is interpreting data. This step involves:

\begin{itemize}
\tightlist
\item
  Drawing conclusions from your data
\item
  Evaluating what your results mean
\item
  Communicating your result
\end{itemize}

To be honest, the computer is of little use here, and the command line does not really come into play at this stage. Once you have reached this step, it is up to you. This is the only step in the OSEMN model which does not have its own chapter. Instead, we kindly refer you to \emph{Thinking with Data} by \citet{Shron2014}.

\hypertarget{intermezzo-chapters}{%
\section{Intermezzo Chapters}\label{intermezzo-chapters}}

In between the chapters that cover the OSEMN steps, there are three intermezzo chapters. Each intermezzo chapter discusses a more general topic concerning data science, and how the command line is employed for that. These topics are applicable to any step in the data science process.

In \protect\hyperlink{chapter-4-creating-reusable-command-line-tools}{Chapter 4}, we discuss how to create reusable tools for the command line. These personal tools can come from both long commands that you have typed on the command line, or from existing code that you have written in, say, Python or R. Being able to create your own tools allows you to become more efficient and productive.

Because the command line is an interactive environment for doing data science, it can become challenging to keep track of your workflow. In \protect\hyperlink{chapter-6-managing-your-data-workflow}{Chapter 6}, we demonstrate a command-line tool called Drake \citep{drake}, which allows you to define your data science workflow in terms of tasks and the dependencies between them. This tool increases the reproducibility of your workflow, not only for you but also for your colleagues and peers.

In \protect\hyperlink{chapter-8-parallel-pipelines}{Chapter 8}, we explain how your commands and tools can be sped up by running them in parallel. Using a command-line tool called GNU Parallel \citep{parallel}, we can apply command-line tools to very large data sets and run them on multiple cores and remote machines.

\hypertarget{what-is-the-command-line}{%
\section{What is the Command Line?}\label{what-is-the-command-line}}

Before we discuss \emph{why} you should use the command line for data science, let's take a peek at \emph{what} the command line actually looks like (it may be already familiar to you). Figure \ref{fig:mac-terminal} and Figure \ref{fig:ubuntu-terminal} show a screenshot of the command line as it appears by default on macOS and Ubuntu, respectively. Ubuntu is a particular distribution of GNU/Linux, which we'll be assuming throughout the book.

\begin{figure}

{\centering \includegraphics[width=9.5in]{images/screenshot_terminal_mac_dst} 

}

\caption{Command line on macOS}\label{fig:mac-terminal}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=10.06in]{images/screenshot_terminal_ubuntu_dst} 

}

\caption{Command line on Ubuntu}\label{fig:ubuntu-terminal}
\end{figure}

The window shown in the two screenshots is called the terminal. This is the program that enables you to interact with the shell. It is the shell that executes the commands we type in. (On both Ubuntu and macOS, the default shell is Bash.)

\begin{rmdnote}
We're not showing the Microsoft Windows command line (also known as the Command Prompt or PowerShell), because it's fundamentally different and incompatible with the commands presented in this book. The good news is that you can install the Data Science Toolbox on Microsoft Windows, so that you're able to follow along. How to install the Data Science Toolbox is explained in \protect\hyperlink{chapter-2-getting-started}{Chapter 2}.
\end{rmdnote}

Typing commands is a very different way of interacting with your computer than through a graphical user interface. If you are mostly used to processing data in, say, Microsoft Excel, then this approach may seem intimidating at first. Don't be afraid. Trust us when we say that you'll get used to working at the command line very quickly.

In this book, the commands that we type in, and the output that they generate, is displayed as text. For example, the contents of the terminal (after the welcome message) in the two screenshots would look like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{whoami}
\ExtensionTok{vagrant}
\NormalTok{$ }\FunctionTok{hostname}
\ExtensionTok{data-science-toolbox}
\NormalTok{$ }\FunctionTok{date}
\ExtensionTok{Tue}\NormalTok{ Jul 22 02:52:09 UTC 2014}
\NormalTok{$ }\BuiltInTok{echo} \StringTok{'The command line is awesome!'} \KeywordTok{|} \ExtensionTok{cowsay}
 \ExtensionTok{______________________________}
\OperatorTok{<} \ExtensionTok{The}\NormalTok{ command line is awesome! }\OperatorTok{>}
 \ExtensionTok{------------------------------}
        \ExtensionTok{\textbackslash{} }\NormalTok{  ^__^}
         \ExtensionTok{\textbackslash{} }\NormalTok{ (oo)\textbackslash{}}\ExtensionTok{_______}
            \KeywordTok{(}\ExtensionTok{__}\KeywordTok{)}\ExtensionTok{\textbackslash{} }\NormalTok{      )\textbackslash{}/}\KeywordTok{\textbackslash{}}
                \KeywordTok{||}\ExtensionTok{----w} \KeywordTok{|}
                \KeywordTok{||}     \KeywordTok{||}
\end{Highlighting}
\end{Shaded}

You'll also notice that each command is preceded with a dollar sign (\textbf{\$}). This is called the prompt. The prompt in the two screenshots showed more information, namely the username (\texttt{vagrant}), the hostname (\texttt{data-science-toolbox}) and the current working directory (\texttt{\textbackslash{}\textasciitilde{}}). It's a convention to show only a dollar sign in examples, because the prompt (1) can change during a session (when you go to a different directory), (2) can be customized by the user (e.g., it can also show the time or the current \texttt{git} \citep{git} branch you're working on), and (3) is irrelevant for the commands themselves.

In the next chapter we'll explain much more about essential command-line concepts. Now it's time to first explain \emph{why} you should learn to use the command line for doing data science.

\hypertarget{why-data-science-at-the-command-line}{%
\section{Why Data Science at the Command Line?}\label{why-data-science-at-the-command-line}}

The command line has many great advantages that can really make you a more efficient and productive data scientist. Roughly grouping the advantages, the command line is: agile, augmenting, scalable, extensible, and ubiquitous. We elaborate on each advantage below.

\hypertarget{the-command-line-is-agile}{%
\subsection{The Command Line is Agile}\label{the-command-line-is-agile}}

The first advantage of the command line is that it allows you to be agile. Data science has a very interactive and exploratory nature, and the environment that you work in needs to allow for that. The command line achieves this by two means.

First, the command line provides a so-called read-eval-print-loop (REPL). This means that you type in command, press \textbf{}, and the command is evaluated immediately. A REPL is often much more convenient for doing data science than the edit-compile-run-debug cycle associated with scripts, large programs, and, say, Hadoop jobs. Your commands are executed immediately, may be stopped at will, and can be changed quickly. This short iteration cycle really allows you to play with your data.

Second, the command line is very close to the file system. Because data is the main ingredient for doing data science, it is important to be able to easily work with the files that contain your data set. The command line offers many convenient tools for this.

\hypertarget{the-command-line-is-augmenting}{%
\subsection{The Command Line is Augmenting}\label{the-command-line-is-augmenting}}

Whatever technology your data science workflow currently includes (whether it's R, IPython, or Hadoop), you should know that we're not suggesting you abandon that workflow. Instead, the command line is presented here as an augmenting technology that amplifies the technologies you're currently employing.

The command line integrates well with other technologies. On the one hand, you can often employ the command line from your own environment. Python and R, for instance, allow you to run command-line tools and capture their output. On the other hand, you can turn your code (e.g., a Python or R function that you have already written) into a command-line tool. We will cover this extensively in \protect\hyperlink{chapter-4-creating-reusable-command-line-tools}{Chapter 4}. Moreover, the command line can easily cooperate with various databases and file types such as Microsoft Excel.

In the end, every technology has its advantages and disadvantages (including the command line), so it's good to know several and use whichever is most appropriate for the task at hand. Sometimes that means using R, sometimes the command line, and sometimes even pen and paper. By the end of this book you'll have a solid understanding of when you could use the command line, and when you're better off continuing with your favorite programming language or statistical computing environment.

\hypertarget{the-command-line-is-scalable}{%
\subsection{The Command Line is Scalable}\label{the-command-line-is-scalable}}

Working on the command line is very different from using a graphical user interface (GUI). On the command line you do things by typing, whereas with a GUI, you do things by pointing and clicking with a mouse.

Everything that you type manually on the command line, can also be automated through scripts and tools. This makes it very easy to re-run your commands in case you made a mistake, when the data set changed, or because your colleague wants to perform the same analysis. Moreover, your commands can be run at specific intervals, on a remote server, and in parallel on many chunks of data (more on that in Chapter 8).

Because the command line is automatable, it becomes scalable and repeatable. It is not straightforward to automate pointing and clicking, which makes a GUI a less suitable environment for doing scalable and repeatable data science.

\hypertarget{the-command-line-is-extensible}{%
\subsection{The Command Line is Extensible}\label{the-command-line-is-extensible}}

The command line itself was invented over 40 year ago. Its core functionality has largely remained unchanged, but the \emph{tools}, which are the workhorses of the command-line, are being developed on a daily basis.

The command line itself is language agnostic. This allows the command-line tools to be written in many different programming languages. The open source community is producing many free and high-quality command-line tools that we can use for data science.

These command-line tools can work together, which makes the command line very flexible. You can also create your own tools, allowing you to extending the effective functionality of the command line.

\hypertarget{the-command-line-is-ubiquitous}{%
\subsection{The Command Line is Ubiquitous}\label{the-command-line-is-ubiquitous}}

Because the command line comes with any Unix-like operating system, including Ubuntu Linux and macOS, it can be found in many places. According to \href{http://top500.org/blog/lists/2013/11/press-release}{an article on Top 500 Supercomputer Sites}, 95\% of the top 500 supercomputers are running GNU/Linux. So, if you ever get your hands on one of those supercomputers (or if you ever find yourself in Jurassic Park with the doors locks not working), you better know your way around the command line!

But GNU/Linux not only runs on supercomputers. It also runs on servers, laptops, and embedded systems. These days, many companies offer cloud computing, where you can easily launch new machines on the fly. If you ever log in to such a machine (or a server in general), there's a good chance that you'll arrive at the command line.

Besides mentioning that the command line is available in a lot of places, it is also important to note that the command line is not a hype. This technology has been around for more than four decades, and we're personally convinced that it's here to stay for another four. Learning how to use the command line (for data science) is therefore a worthwhile investment.

\hypertarget{a-real-world-use-case}{%
\section{A Real-world Use Case}\label{a-real-world-use-case}}

In the previous sections, we've given you a definition of data science and explained to you why the command line can be a great environment for doing data science. Now it's time to demonstrate the power and flexibility of the command line through a real-world use case. We'll go pretty fast, so don't worry if some things don't make sense yet.

Personally, we never seem to remember when Fashion Week is happening in New York. We know it's held twice a year, but every time it comes as a surprise! In this section we'll consult the wonderful API of \emph{The New York Times} to figure out when it's being held. Once you have obtained your own API keys on \href{http://developer.nytimes.com}{the developer website}, you'll be able to, for example, search for articles, get the list of best sellers, and see a list of events.

The particular API endpoint that we're going to query is the article search one. We expect that a spike in the amount of coverage in \emph{The New York Times} about New York Fashion week indicates whether it's happening. The results from the API are paginated, which means that we have to execute the same query multiple times but with different page number. (It's like clicking Next on a search engine.) This is where GNU Parallel \citep{parallel} comes in real handy because it can act as a \texttt{for} loop. The entire command looks as follows (don't worry about all the command-line arguments given to \texttt{parallel}; we're going to discuss this in great detail in \protect\hyperlink{parallel-pipelinesux5cux255D}{Chapter 8}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{cd}\NormalTok{ ~/book/ch01/data}
\NormalTok{$ }\ExtensionTok{parallel}\NormalTok{ -j1 --progress --delay 0.1 --results results }\StringTok{"curl -sL "}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{"'http://api.nytimes.com/svc/search/v2/articlesearch.json?q=New+York+'"}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{"'Fashion+Week&begin_date=\{1\}0101&end_date=\{1\}1231&page=\{2\}&api-key='"}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{"'<your-api-key>'"}\NormalTok{ ::: }\DataTypeTok{\{2009..2013\}}\NormalTok{ ::: }\DataTypeTok{\{0..99\}} \OperatorTok{>}\NormalTok{ /dev/null}

\ExtensionTok{Computers}\NormalTok{ / CPU cores / Max jobs to run}
\ExtensionTok{1}\NormalTok{:local / 4 / 1}

\ExtensionTok{Computer}\NormalTok{:jobs running/jobs completed/%of started jobs/Average seconds to complete}
\ExtensionTok{local}\NormalTok{:1/9/100%/0.4s}
\end{Highlighting}
\end{Shaded}

Basically, we're performing the same query for years 2009-2014. The API only allows up to 100 pages (starting at 0) per query, so we're generating 100 numbers using brace expansion. These numbers are used by the \emph{page} parameter in the query. We're searching for articles that contain the search term \texttt{New+York+Fashion+Week}. Because the API has certain limits, we ensure that there's only one request at a time, with a one-second delay between them. Make sure that you replace \texttt{\textless{}your-api-key\textgreater{}} with your own API key for the article search endpoint.

Each request returns 10 articles, so that's 1,000 articles in total. These are sorted by page views, so this should give us a good estimate of the coverage. The results are in JSON format, which we store in the \emph{results} directory. The command-line tool \texttt{tree} \citep{tree} gives an overview of how the subdirectories are structured:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{tree}\NormalTok{ results }\KeywordTok{|} \FunctionTok{head}
\ExtensionTok{results}
\NormalTok{└── }\ExtensionTok{1}
\NormalTok{    ├── }\ExtensionTok{2009}
\NormalTok{    │   └── }\ExtensionTok{2}
\NormalTok{    │       ├── }\ExtensionTok{0}
\NormalTok{    │       │   ├── }\ExtensionTok{stderr}
\NormalTok{    │       │   └── }\ExtensionTok{stdout}
\NormalTok{    │       ├── }\ExtensionTok{1}
\NormalTok{    │       │   ├── }\ExtensionTok{stderr}
\NormalTok{    │       │   └── }\ExtensionTok{stdout}
\end{Highlighting}
\end{Shaded}

We can combine and process the results using \texttt{cat} \citep{cat}, \texttt{jq} \citep{jq}, and \texttt{json2csv} \citep{json2csv}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{cat}\NormalTok{ results/1/*/2/*/stdout }\KeywordTok{|}                                             
\OperatorTok{>} \ExtensionTok{jq}\NormalTok{ -c }\StringTok{'.response.docs[] | \{date: .pub_date, type: .document_type, '}\DataTypeTok{\textbackslash{} }    
\OperatorTok{>} \StringTok{'title: .headline.main \}'} \KeywordTok{|} \ExtensionTok{json2csv}\NormalTok{ -p -k date,type,title }\OperatorTok{>}\NormalTok{ fashion.csv }
\end{Highlighting}
\end{Shaded}

Let's break down this command:

\begin{itemize}
\tightlist
\item
  We combine the output of each of the 500 \texttt{parallel} jobs (or API requests).
\item
  We use \texttt{jq} to extract the publication date, the document type, and the headline of each article.
\item
  We convert the JSON data to CSV using \texttt{json2csv} and store it as \emph{fashion.csv}.
\end{itemize}

With \texttt{wc\ -l} \citep{wc}, we find out that this data set contains 4,855 articles (and not 5,000 because we probably retrieved everything from 2009):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{wc}\NormalTok{ -l fashion.csv}
\ExtensionTok{4856}\NormalTok{ fashion.csv}
\end{Highlighting}
\end{Shaded}

Let's inspect the first 10 articles to verify that we have succeeded in obtaining the data. Note that we're applying \texttt{cols} \citep{cols} and \texttt{cut} \citep{cut} to the \emph{date} column in order to leave out the time and timezone information in the table:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{fashion.csv}\NormalTok{ cols -c date cut -dT -f1 }\KeywordTok{|} \FunctionTok{head} \KeywordTok{|} \ExtensionTok{csvlook}
\KeywordTok{|}\ExtensionTok{-------------+------------+-----------------------------------------}\KeywordTok{|}
\KeywordTok{|}  \FunctionTok{date}       \KeywordTok{|} \BuiltInTok{type}       \KeywordTok{|} \ExtensionTok{title}                                   \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{-------------+------------+-----------------------------------------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{2009-02-15} \KeywordTok{|} \ExtensionTok{multimedia} \KeywordTok{|} \ExtensionTok{Michael}\NormalTok{ Kors                            }\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{2009-02-20} \KeywordTok{|} \ExtensionTok{multimedia} \KeywordTok{|} \ExtensionTok{Recap}\NormalTok{: Fall Fashion Week, New York      }\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{2009-09-17} \KeywordTok{|} \ExtensionTok{multimedia} \KeywordTok{|} \ExtensionTok{UrbanEye}\NormalTok{: Backstage at Marc Jacobs      }\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{2009-02-16} \KeywordTok{|} \ExtensionTok{multimedia} \KeywordTok{|} \ExtensionTok{Bill}\NormalTok{ Cunningham on N.Y. Fashion Week    }\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{2009-02-12} \KeywordTok{|} \ExtensionTok{multimedia} \KeywordTok{|} \ExtensionTok{Alexander}\NormalTok{ Wang                          }\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{2009-09-17} \KeywordTok{|} \ExtensionTok{multimedia} \KeywordTok{|} \ExtensionTok{Fashion}\NormalTok{ Week Spring 2010                }\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{2009-09-11} \KeywordTok{|} \ExtensionTok{multimedia} \KeywordTok{|} \ExtensionTok{Of}\NormalTok{ Color }\KeywordTok{|} \ExtensionTok{Diversity}\NormalTok{ Beyond the Runway  }\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{2009-09-14} \KeywordTok{|} \ExtensionTok{multimedia} \KeywordTok{|} \ExtensionTok{A}\NormalTok{ Designer Reinvents Himself            }\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{2009-09-12} \KeywordTok{|} \ExtensionTok{multimedia} \KeywordTok{|} \ExtensionTok{On}\NormalTok{ the Street }\KeywordTok{|} \ExtensionTok{Catwalk}                 \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{-------------+------------+-----------------------------------------}\KeywordTok{|}
\end{Highlighting}
\end{Shaded}

That seems to have worked! In order to gain any insight, we'd better visualize the data. Figure \ref{fig:fashion-week} contains a line graph created with R \citep{R}, \texttt{Rio} \citep{Rio}, and \texttt{ggplot2} \citep{Wickham2009}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{fashion.csv}\NormalTok{ Rio -ge }\StringTok{'g + geom_freqpoly(aes(as.Date(date), color=type), '}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{'binwidth=7) + scale_x_date() + labs(x="date", title="Coverage of New York'}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{' Fashion Week in New York Times")'} \KeywordTok{|} \ExtensionTok{display}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=32.81in]{images/nyt-fashion-week-multi} 

}

\caption{Coverage of New York Fashion Week in the New York Times}\label{fig:fashion-week}
\end{figure}

By looking at the line graph we can infer that New York Fashion Week happens two times per year. And now we know when: once in February and once in September. Let's hope that it's going to be the same this year so that we can prepare ourselves! In any case, we hope that with this example, we've shown that \emph{The New York Times} API is an interesting source of data. More importantly, we hope that we've convinced you that the command line can be a very powerful approach for doing data science.

In this section we've peeked at some important concepts and some exciting command-line tools. Don't worry if some things don't make sense yet. Most of the concepts will be discussed in \protect\hyperlink{chapter-2-getting-started}{Chapter 2}, and in the subsequent chapters we'll go into more detail for all the command-line tools used in this section.

\hypertarget{further-reading}{%
\section{Further Reading}\label{further-reading}}

\begin{itemize}
\tightlist
\item
  Mason, Hilary, and Chris H. Wiggins. 2010. ``A Taxonomy of Data Science.'' http://www.dataists.com/2010/09/a-taxonomy-of-data-science.
\item
  Patil, DJ. 2012. Data Jujitsu. O'Reilly Media.
\item
  O'Neil, C. \& Schutt, R. 2013. Doing Data Science. O'Reilly Media.
\item
  Shron, Max. 2014. Thinking with Data. O'Reilly Media.
\end{itemize}

\hypertarget{chapter-2-getting-started}{%
\chapter{Getting Started}\label{chapter-2-getting-started}}

In this chapter we are going to make sure that you have all the prerequisites for doing data science at the command line. The prerequisites fall into two parts: (1) having a proper environment with all the command-line tools that we employ in this book installed, and (2) understanding the essential concepts that come into play when using the command line.

First, we describe how to install the Docker image, which is a virtual environment based on Linux that contains all the necessary command-line tools. Subsequently, we explain the essential command-line concepts through examples.

By the end of this chapter, you'll have everything you need in order to continue with the first step of doing data science, namely obtaining data.

\hypertarget{overview}{%
\section{Overview}\label{overview}}

In this chapter, you'll learn:

\begin{itemize}
\tightlist
\item
  How to install the Docker image.
\item
  Essential concepts and tools necessary to perform data science at the command line.
\end{itemize}

\hypertarget{docker-image}{%
\section{Installing the Docker Image}\label{docker-image}}

\begin{rmdnote}
This section used to be called \emph{Setting up the Data Science Toolbox} and described how to install a Vagrant box containing all the command-line tools. This Vagrant box was created in 2014, and because technology around virtualisation and containerisation has moved on, it became high time for an update. So now, instead of a Vagrant box, we use a Docker image.
\end{rmdnote}

In this book we use many different command-line tools. Linux often comes with a whole bunch of command-line tools pre-installed. Moreover, Linux offers many packages that contain other, relevant tools. Installing these packages yourself is not too difficult. However, we also use tools that are not available as packages and require a more manual, and more involved, installation. In order to acquire the necessary command-line tools without having to go through the involved installation process of each, we encourage you to install a Docker image that was created specifically for this book.

\begin{rmdtip}
If you still prefer to run the command-line tools natively rather than inside a Docker image, then you can, of course, install the command-line tools individually yourself. Please be aware that this is a very time-consuming process. The Appendix lists all the command-line tools used in the book. The installation instructions are for Ubuntu only. The scripts and data sets used in the book can be obtained by cloning this book's \href{https://github.com/jeroenjanssens/data-science-at-the-command-line}{GitHub repository}.
\end{rmdtip}

To install the Docker image, you first need to download and install Docker itself from \href{https://www.docker.com/products/docker}{the Docker website}.
Once Docker is installed, you invoke the following command on your terminal or command prompt to download the Docker image (don't type the dollar sign):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{docker}\NormalTok{ pull datascienceworkshops/data-science-at-the-command-line}
\end{Highlighting}
\end{Shaded}

You can run the Docker image as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{docker}\NormalTok{ run --rm -it datascienceworkshops/data-science-at-the-command-line}
\end{Highlighting}
\end{Shaded}

You're now inside an isolated Linux environment---known as a \emph{Docker container}---with all the necessary command-line tools installed. If the following command produces an enthusiastic cow, then you know everything is working correctly:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{cowsay} \StringTok{"Let's go!"}
 \ExtensionTok{___________}
\OperatorTok{<} \ExtensionTok{Let}\StringTok{'s go! >}
\StringTok{ -----------}
\StringTok{        \textbackslash{}   ^__^}
\StringTok{         \textbackslash{}  (oo)\textbackslash{}_______}
\StringTok{            (__)\textbackslash{}       )\textbackslash{}/\textbackslash{}}
\StringTok{                ||----w |}
\StringTok{                ||     ||}
\end{Highlighting}
\end{Shaded}

Run \texttt{exit} to exit the container. If you want to get data in and out of the container, you can add a volume, which means that a local directory gets mapped to a directory inside the container. We recommend that you create a new directory, navigate to this new directory, and then run the following when you're on macOS or Linux:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{docker}\NormalTok{ run --rm -it -v}\KeywordTok{`}\BuiltInTok{pwd}\KeywordTok{`}\NormalTok{:/data datascienceworkshops/data-science-at-the-command-line}
\end{Highlighting}
\end{Shaded}

Or the following when you're on Windows and using the command line:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{docker}\NormalTok{ run --rm -it -v %cd%:/data datascienceworkshops/data-science-at-the-command-line}
\end{Highlighting}
\end{Shaded}

Or the following when you're using Windows PowerShell:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{docker}\NormalTok{ run --rm -it -v }\VariableTok{$\{PWD\}}\NormalTok{:/data datascienceworkshops/data-science-at-the-command-line}
\end{Highlighting}
\end{Shaded}

In the above commands, the option \texttt{-v} instructs \texttt{docker} to map the current directory to the \emph{/data} directory inside the container, so this is the place to get data in and out of the Docker container.

\begin{rmdnote}
If you would like to know more about the Docker image you can \href{https://hub.docker.com/r/datascienceworkshops/data-science-at-the-command-line/}{visit it on Docker Hub}.
\end{rmdnote}

\hypertarget{essential-gnulinux-concepts}{%
\section{Essential GNU/Linux Concepts}\label{essential-gnulinux-concepts}}

In \protect\hyperlink{chapter-1-introduction}{Chapter 1}, we briefly showed you what the command line is. Now that you are running the \protect\hyperlink{docker-image}{Docker image}, we can really get started. In this section, we discuss several concepts and tools that you will need to know in order to feel comfortable doing data science at the command line. If, up to now, you have been mainly working with graphical user interfaces, then this might be quite a change. But don't worry, we'll start at the beginning, and very gradually go to more advanced topics.

\begin{rmdnote}
This section is not a complete course in GNU/Linux. We will only explain the concepts and tools that are relevant for to doing data science. One of the advantages of the \protect\hyperlink{docker-image}{Docker image} is that a lot is already set up. If you wish to know more about GNU/Linux, consult the Further Reading Section at the end of this chapter.
\end{rmdnote}

\hypertarget{the-environment}{%
\subsection{The Environment}\label{the-environment}}

So you've just logged into a brand new environment. Before we do anything, it is worthwhile to get a high-level understanding of this environment. The environment is roughly defined by four layers, which we briefly discuss from the top down.

\begin{description}
\item[Command-line tools]
First and foremost, there are the command-line tools that you work with. We use them by typing their corresponding commands. There are different types of command-line tools, which we will discuss in the next section. Examples of tools are: \texttt{ls} \citep{ls}, \texttt{cat} \citep{cat}, and \texttt{jq} \citep{jq}.
\item[Terminal]
The terminal, which is the second layer, is the application where we type our commands in. If you see the following text:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{seq}\NormalTok{ 3}
\ExtensionTok{1}
\ExtensionTok{2}
\ExtensionTok{3}
\end{Highlighting}
\end{Shaded}

then you would type \texttt{seq\ 3} into your terminal and press \texttt{\textless{}Enter\textgreater{}}. (The command-line tool \texttt{seq} \citep{seq} generates a sequence of numbers.) You do not type the dollar sign. It is just there to tell you that this a command you can type in the terminal. This dollar sign is known as the prompt. The text below \texttt{seq\ 3} is the output of the command. In \protect\hyperlink{chapter-1-introduction}{Chapter 1}, we showed you two screenshots of how the default terminal looks like in macOS and Ubuntu with various commands and their output.
\item[Shell]
The third layer is the shell. Once we have typed in our command and pressed \texttt{\textless{}Enter\textgreater{}}, the terminal sends that command to the shell. The shell is a program that interprets the command. The \protect\hyperlink{docker-image}{Docker image} uses Bash as the shell, but there are many others available. Once you have become a bit more proficient at the command line, you may want to look into a shell called the Z shell. It offers many additional features that may increase your productivity at the command line.
\item[Operating system]
The fourth layer is the operating system, which is GNU/Linux in our case. Linux is the name of the kernel, which is the heart of the operating system. The kernel is in direct contact with the CPU, disks, and other hardware. The kernel also executes our command-line tools. GNU, which stands for GNU's not UNIX, refers to the set of basic tools. The \protect\hyperlink{docker-image}{Docker image} is based on a particular Linux distribution called Alpine Linux.
\end{description}

\hypertarget{executing-a-command-line-tool}{%
\subsection{Executing a Command-line Tool}\label{executing-a-command-line-tool}}

Now that you have a basic understanding of the environment, it is high time that you try out some commands. Type the following in your terminal (without the dollar sign) and press \texttt{\textless{}Enter\textgreater{}}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{pwd}
\ExtensionTok{/home/vagrant}
\end{Highlighting}
\end{Shaded}

Sometimes we are using commands and pipelines that are too long to fit on the page. In that case you'll see something like the following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{echo} \StringTok{'Hello'}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{' world'} \KeywordTok{|}
\OperatorTok{>} \FunctionTok{wc}
\end{Highlighting}
\end{Shaded}

The greater-than sign is the continuation prompt, which indicates that this line is a continuation of the previous one. A long command can be broken up with either a backslash or a pipe symbol. Be sure to first match any quotation marks. The following command is exactly the same:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{echo} \StringTok{'Hello world'} \KeywordTok{|} \FunctionTok{wc}
\end{Highlighting}
\end{Shaded}

This is as simple as it gets. You just executed a command that contained a single command-line tool. The command-line tool \texttt{pwd} \citep{pwd} prints the name of the directory where you currently are. By default, when you login, this is your home directory. You can view the contents of this directory with \texttt{ls} \citep{ls}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{ls}
\ExtensionTok{book}
\end{Highlighting}
\end{Shaded}

The command-line tool \texttt{cd}, which is a Bash builtin, allows you to navigate to a different directory:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{cd}\NormalTok{ book/ch02/}
\NormalTok{$ }\BuiltInTok{cd}\NormalTok{ data}
\NormalTok{$ }\BuiltInTok{pwd}
\ExtensionTok{/home/vagrant/book/ch02/data}
\NormalTok{$ }\BuiltInTok{cd}\NormalTok{ ..}
\NormalTok{$ }\BuiltInTok{pwd}
\ExtensionTok{/home/vagrant/book/ch02/}
\end{Highlighting}
\end{Shaded}

The part after \texttt{cd} specifies to which directory you want to navigate to. Values that come after the command are called command-line arguments or options. The two dots refer to the parent directory. Let's try a different command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{head}\NormalTok{ -n 3 data/movies.txt}
\ExtensionTok{Matrix}
\ExtensionTok{Star}\NormalTok{ Wars}
\ExtensionTok{Home}\NormalTok{ Alone}
\end{Highlighting}
\end{Shaded}

Here we pass three command-line arguments to \texttt{head} \citep{head}. The first one is an option. The second one is a value that belongs to the option. The third one is a filename. This particular command outputs the first three lines of file \emph{book/ch02/data/movies.txt}.

\hypertarget{five-types-of-command-line-tools}{%
\subsection{Five Types of Command-line Tools}\label{five-types-of-command-line-tools}}

We employ the term command-line tool a lot, but so far, we have not yet explained what we actually mean by it. We use it as an umbrella term for \emph{anything} that can be executed from the command line. Under the hood, each command-line tool is one of the following five types:

\begin{itemize}
\tightlist
\item
  A binary executable.
\item
  A shell builtin.
\item
  An interpreted script.
\item
  A shell function.
\item
  An alias.
\end{itemize}

It's good to know the difference between the types. The command-line tools that come pre-installed with the \protect\hyperlink{docker-image}{Docker image} mostly comprise of the first two types (binary executable and shell builtin). The other three types (interpreted script, shell function, and alias) allow us to further build up our data science toolbox and become more efficient and more productive data scientists.

\begin{description}
\item[Binary Executable]
Binary executables are programs in the classical sense. A binary executable is created by compiling source code to machine code. This means that when you open the file in a text editor you cannot read it.
\item[Shell Builtin]
Shell builtins are command-line tools provided by the shell, which is Bash in our case. Examples include \texttt{cd} and \texttt{help}. These cannot be changed. Shell builtins may differ between shells. Like binary executables, they cannot be easily inspected or changed.
\item[Interpreted Script]
An interpreted script is a text file that is executed by a binary executable. Examples include: Python, R, and Bash scripts. One great advantage of an interpreted script is that you can read and change it. Example \ref{exm:script-fac} shows a script named \emph{\textasciitilde{}/book/ch02/fac.py}. This script is interpreted by Python not because of the file extension \emph{.py}, but because the first line of the script defines the binary that should execute it.

\begin{example}[Python script that computes the factorial of an integer]
\protect\hypertarget{exm:script-fac}{}{\label{exm:script-fac} \iffalse (Python script that computes the factorial of an integer) \fi{} }
\end{example}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{##!/usr/bin/env python}

\KeywordTok{def}\NormalTok{ factorial(x):}
\NormalTok{    result }\OperatorTok{=} \DecValTok{1}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{xrange}\NormalTok{(}\DecValTok{2}\NormalTok{, x }\OperatorTok{+} \DecValTok{1}\NormalTok{):}
\NormalTok{        result }\OperatorTok{*=}\NormalTok{ i}
    \ControlFlowTok{return}\NormalTok{ result}

\ControlFlowTok{if} \VariableTok{__name__} \OperatorTok{==} \StringTok{"__main__"}\NormalTok{:}
    \ImportTok{import}\NormalTok{ sys}
\NormalTok{    x }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(sys.argv[}\DecValTok{1}\NormalTok{])}
    \BuiltInTok{print}\NormalTok{ factorial(x)}
\end{Highlighting}
\end{Shaded}

This script computes the factorial of the integer that we pass as a parameter. It can be invoked from the command line as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{book/ch02/fac.py}\NormalTok{ 5}
\ExtensionTok{120}
\end{Highlighting}
\end{Shaded}

In \protect\hyperlink{chapter-4-creating-reusable-command-line-tools}{Chapter 4}, we'll discuss in great detail how to create reusable command-line tools using interpreted scripts.
\item[Shell Function]
A shell function is a function that is, in our case executed by Bash. They provide similar functionality to a Bash script, but they are usually (but not necessarily) smaller than scripts. They also tend to be more personal. The following command defines a function called \texttt{fac}, which, just like the interpreted Python script above, computes the factorial of the integer we pass as a parameter. It does by generating a list of numbers using \texttt{seq}, putting those numbers on one line with \texttt{*} as the delimiter using \texttt{paste} \citep{paste}, and passing this equation into \texttt{bc} \citep{bc}, which is evaluates it and outputs the result.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{fac()} \KeywordTok{\{} \KeywordTok{(}\BuiltInTok{echo}\NormalTok{ 1}\KeywordTok{;} \FunctionTok{seq} \VariableTok{$1}\KeywordTok{)} \KeywordTok{|} \ExtensionTok{paste}\NormalTok{ -s -d}\DataTypeTok{\textbackslash{}*}\NormalTok{ - }\KeywordTok{|} \FunctionTok{bc}\KeywordTok{;} \KeywordTok{\}}
\NormalTok{$ }\ExtensionTok{fac}\NormalTok{ 5}
\ExtensionTok{120}
\end{Highlighting}
\end{Shaded}

The file \emph{.bashrc}, which is a configuration file for Bash, is a good place to define your shell functions, so that they are always available.
\item[Alias]
Aliases are like macros. If you often find yourself executing a certain command with the same parameters (or a part of it), you can define an alias for this. Aliases are also very useful when you continue to misspell a certain command (see \url{https://github.com/chrishwiggins/mise/blob/master/sh/aliases-public.sh} for a long list of useful aliases). The following command defines such an alias:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{alias}\NormalTok{ l=}\StringTok{'ls -1 --group-directories-first'}
\NormalTok{$ }\BuiltInTok{alias}\NormalTok{ moer=more}
\end{Highlighting}
\end{Shaded}

Now, if you type the following on the command line, the shell will replace each alias it finds with its value:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{cd}\NormalTok{ ~}
\NormalTok{$ }\ExtensionTok{l}
\ExtensionTok{book}
\end{Highlighting}
\end{Shaded}

Aliases are simpler than shell functions as they don't allow parameters. The function \texttt{fac} could not have been defined using an alias because of the parameter. Still, aliases allow you to save lots of keystrokes. Like shell functions, aliases are often defined in \emph{.bashrc} or \emph{.bash\_aliases} configuration files, which are located in your home directory. To see all aliases currently defined, you simply run \texttt{alias} without arguments. Try it, what do you see?
\end{description}

In this book we will focus mostly on the last three types of command-line tools: interpreted scripts, shell functions, and aliases. This is because these can easily be changed. The purpose of a command-line tool is to make your life on the command line easier, and to make you a more productive and more efficient data scientist. You can find out the type of a command-line tool with \texttt{type} (which is itself a shell builtin):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{type}\NormalTok{ -a pwd}
\BuiltInTok{pwd}\NormalTok{ is a shell builtin}
\BuiltInTok{pwd}\NormalTok{ is /bin/pwd}
\NormalTok{$ }\BuiltInTok{type}\NormalTok{ -a cd}
\BuiltInTok{cd}\NormalTok{ is a shell builtin}
\NormalTok{$ }\BuiltInTok{type}\NormalTok{ -a fac}
\ExtensionTok{fac}\NormalTok{ is a function}
\FunctionTok{fac ()}
\KeywordTok{\{}
    \KeywordTok{(} \BuiltInTok{echo}\NormalTok{ 1}\KeywordTok{;}
    \FunctionTok{seq} \VariableTok{$1} \KeywordTok{)} \KeywordTok{|} \ExtensionTok{paste}\NormalTok{ -s -d}\DataTypeTok{\textbackslash{}*}\NormalTok{ - }\KeywordTok{|} \FunctionTok{bc}
\KeywordTok{\}}
\NormalTok{$ }\BuiltInTok{type}\NormalTok{ -a l}
\ExtensionTok{l}\NormalTok{ is aliased to }\KeywordTok{`}\FunctionTok{ls}\NormalTok{ -1 --group-directories-first}\StringTok{'}
\end{Highlighting}
\end{Shaded}

\texttt{type} returns two command-line tools for \texttt{pwd}. In that case, the first reported command-line tool is used when you type \texttt{pwd}. In the next section we will look at how to combine command-line tools.

\hypertarget{combining-command-line-tools}{%
\subsection{Combining Command-line Tools}\label{combining-command-line-tools}}

Because most command-line tools adhere to the UNIX philosophy, they are designed to do only thing, and do it really well. For example, the command-line tool \texttt{grep} \citep{grep} can filter lines, \texttt{wc} \citep{wc} can count lines, and \texttt{sort} \citep{sort} can sort lines. The power of the command line comes from its ability to combine these small, yet powerful command-line tools. The most important way of combining command-line tools is through a so-called pipe. The output from the first tool is passed to the second tool. There are virtually no limits to this.

Consider, for example, the command-line tool \texttt{seq}, which generates a sequence of numbers. Let us generate a sequence of five numbers.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{seq}\NormalTok{ 5}
\ExtensionTok{1}
\ExtensionTok{2}
\ExtensionTok{3}
\ExtensionTok{4}
\ExtensionTok{5}
\end{Highlighting}
\end{Shaded}

The output of a command-line tool is by default passed on to the terminal, which displays it on our screen. We can \emph{pipe} the ouput of \texttt{seq} to a second tool, called \texttt{grep}, which can be used to filter lines. Imagine that we only want to see numbers that contain a ``3''. We can combine \texttt{seq} and \texttt{grep} as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{seq}\NormalTok{ 30 }\KeywordTok{|} \FunctionTok{grep}\NormalTok{ 3}
\ExtensionTok{3}
\ExtensionTok{13}
\ExtensionTok{23}
\ExtensionTok{30}
\end{Highlighting}
\end{Shaded}

And if we wanted to know \emph{how many} numbers between 1 and 100 that contain a three, we can use \texttt{wc}, that is very good at counting things:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{seq}\NormalTok{ 100 }\KeywordTok{|} \FunctionTok{grep}\NormalTok{ 3 }\KeywordTok{|} \FunctionTok{wc}\NormalTok{ -l}
\ExtensionTok{19}
\end{Highlighting}
\end{Shaded}

The option \texttt{-l} specifies that \texttt{wc} should only output the number of lines that are pass into it. By default it also returns the number of characters and words.

You may start to see that combining command-line tools is a very powerful concept. In the rest of the book you will be introduced to many more tools and the functionality they offer when combining them.

\hypertarget{redirecting-input-and-output}{%
\subsection{Redirecting Input and Output}\label{redirecting-input-and-output}}

We mentioned that, by default, the output of the last command-line tool in the pipeline is outputted to the terminal. You can also save this output to a file. This is called output redirection, and works as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{seq}\NormalTok{ 10 }\OperatorTok{>}\NormalTok{ data/ten-numbers}
\end{Highlighting}
\end{Shaded}

Here, we save the output of the \texttt{seq} tool to a file named \emph{ten-numbers} in the directory \emph{\textasciitilde{}/book/ch02/data}. If this file does not exist yet, it is created. If this file already did exist, its contents would have been overwritten. You can also append the output to a file with \texttt{\textgreater{}\textgreater{}}, meaning the output is put after the original contents:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{echo}\NormalTok{ -n }\StringTok{"Hello"} \OperatorTok{>}\NormalTok{ hello-world}
\NormalTok{$ }\BuiltInTok{echo} \StringTok{" World"} \OperatorTok{>>}\NormalTok{ hello-world}
\end{Highlighting}
\end{Shaded}

The tool echo just outputs the value you specify. The \texttt{-n} option specifies that \texttt{echo} should not output a trailing newline.

Saving the output to a file is useful if you need to store intermediate results, for example for continuing with your analysis at a later stage. To use the contents of the file \emph{hello-world} again, we can use \texttt{cat} \citep{cat}, which reads a file prints it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{cat}\NormalTok{ hello-world }\KeywordTok{|} \FunctionTok{wc}\NormalTok{ -w}
\ExtensionTok{2}
\end{Highlighting}
\end{Shaded}

(Note that the \texttt{-w} option indicates \texttt{wc} to only count words.) The same result can be achieved with the following notation:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{hello-world}\NormalTok{ wc -w}
\ExtensionTok{2}
\end{Highlighting}
\end{Shaded}

This way, you are directly passing the file to the standard input of \texttt{wc} without running an additional process. If the command-line tool also allows files to be specified as command-line arguments, which many do, you can also do the following for \texttt{wc}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{wc}\NormalTok{ -w hello-world}
\ExtensionTok{2}\NormalTok{ hello-world}
\end{Highlighting}
\end{Shaded}

\hypertarget{working-with-files}{%
\subsection{Working With Files}\label{working-with-files}}

As data scientists, we work with a lot of data. This data is often stored in files. It is important to know how to work with files (and the directories they live in) on the command line. Every action that you can do using a graphical user interface, you can do with command-line tools (and much more). In this section we introduce the most important ones to create, move, copy, rename, and delete files and directories.

You have already seen how we can create new files by redirecting the output with either \texttt{\textgreater{}} or \texttt{\textgreater{}\textgreater{}}. In case you need to move a file to a different directory you can use \texttt{mv} \citep{mv}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{mv}\NormalTok{ hello.txt ~/book/ch02/data/}
\end{Highlighting}
\end{Shaded}

You can also rename files with \texttt{mv}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{cd}\NormalTok{ data}
\NormalTok{$ }\FunctionTok{mv}\NormalTok{ hello.txt bye.txt}
\end{Highlighting}
\end{Shaded}

You can also rename or move entire directories. In case you no longer need a file, you delete (or remove) it with \texttt{rm} \citep{rm}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{rm}\NormalTok{ bye.txt}
\end{Highlighting}
\end{Shaded}

In case you want to remove an entire directory with all its contents, specify the \texttt{-r} option, which stands for recursive:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{rm}\NormalTok{ -r book/ch02/data/old}
\end{Highlighting}
\end{Shaded}

In case you want to copy a file, use \texttt{cp} \citep{cp}. This is useful for creating backups:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{cp}\NormalTok{ server.log server.log.bak}
\end{Highlighting}
\end{Shaded}

You can create directories using \texttt{mkdir} \citep{mkdir}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{cd}\NormalTok{ data}
\NormalTok{$ }\FunctionTok{mkdir}\NormalTok{ logs}
\end{Highlighting}
\end{Shaded}

Using the command-line tools to manage your files can be scary at first, because you have no graphical overview of the file system to provide immediate feedback.

All of the above command-line tools accept the \texttt{-v} option, which stands for verbose, so that they output what's going on. All but \texttt{mkdir} accept the \texttt{-i} option, which stands for interactive, and causes the tools to ask you for confirmation.

\hypertarget{help}{%
\subsection{Help!}\label{help}}

As you are finding your way around the command-line, it may happen that you need help. Even the most-seasoned Linux users need help at some point. It is impossible to remember all the different command-line tools and their possible arguments. Fortunately, the command line offers severals ways to get help.

The most important command to get help is perhaps \texttt{man} \citep{man}, which is short for \emph{manual}. It contains information for most command-line tools. Imagine that we forgot the different options to the tool \texttt{cat}. You can access its man page using:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{man}\NormalTok{ cat }\KeywordTok{|} \FunctionTok{head}\NormalTok{ -n 20}
\ExtensionTok{CAT}\NormalTok{(1)                           }\ExtensionTok{User}\NormalTok{ Commands                          CAT(1)}



\ExtensionTok{NAME}
       \FunctionTok{cat}\NormalTok{ - concatenate files and print on the standard output}

\ExtensionTok{SYNOPSIS}
       \FunctionTok{cat}\NormalTok{ [OPTION]... [FILE]...}

\ExtensionTok{DESCRIPTION}
       \ExtensionTok{Concatenate}\NormalTok{ FILE(s), }\ExtensionTok{or}\NormalTok{ standard input, to standard output.}

       \ExtensionTok{-A}\NormalTok{, --show-all}
              \ExtensionTok{equivalent}\NormalTok{ to -vET}

       \ExtensionTok{-b}\NormalTok{, --number-nonblank}
              \ExtensionTok{number}\NormalTok{ nonempty output lines, overrides -n}

       \ExtensionTok{-e}\NormalTok{     equivalent to -vE}
\end{Highlighting}
\end{Shaded}

\begin{rmdtip}
Sometimes you'll see us use \texttt{head}, \texttt{fold}, or \texttt{cut} at the end of a command. This is only to ensure that the output of the command fits on the page; you don't have to type these. For example, \texttt{head\ -n\ 5} only prints the first five lines, \texttt{fold} wraps long lines to 80 characters, and \texttt{cut\ -c1-80} trims lines that are long than 80 characters.
\end{rmdtip}

Not every command-line tool has a man page. For shell builtins, such as \texttt{cd}, you need to use the \texttt{help} command-line tool:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{help}\NormalTok{ cd }\KeywordTok{|} \FunctionTok{head}\NormalTok{ -n 20}
\ExtensionTok{cd}\NormalTok{: cd [-L}\KeywordTok{|}\NormalTok{[}\ExtensionTok{-P}\NormalTok{ [-e]] [-@]] [dir]}
    \ExtensionTok{Change}\NormalTok{ the shell working directory.}

    \ExtensionTok{Change}\NormalTok{ the current directory to DIR.  The default DIR is the value of the}
    \ExtensionTok{HOME}\NormalTok{ shell variable.}

    \ExtensionTok{The}\NormalTok{ variable CDPATH defines the search path for the directory containing}
    \ExtensionTok{DIR.}\NormalTok{  Alternative directory names in CDPATH are separated by a colon (:)}\ExtensionTok{.}
    \ExtensionTok{A}\NormalTok{ null directory name is the same as the current directory.  If DIR begins}
    \ExtensionTok{with}\NormalTok{ a slash (/), }\KeywordTok{then} \ExtensionTok{CDPATH}\NormalTok{ is not used.}

    \ExtensionTok{If}\NormalTok{ the directory is not found, and the shell option }\KeywordTok{`}\ExtensionTok{cdable_vars}\StringTok{' is set,}
\StringTok{    the word is assumed to be  a variable name.  If that variable has a value,}
\StringTok{    its value is used for DIR.}

\StringTok{    Options:}
\StringTok{        -L      force symbolic links to be followed: resolve symbolic links in}
\StringTok{        DIR after processing instances of `..'}
        \ExtensionTok{-P}\NormalTok{      use the physical directory structure without following symbolic}
        \ExtensionTok{links}\NormalTok{: resolve symbolic links in DIR before processing instances}
\end{Highlighting}
\end{Shaded}

This help also covers other topics of Bash, in case you are interested (try \texttt{help} without command-line arguments for a list of topics). Remember that you can use \texttt{type} to figure out the kind of a specific command-line tool.

Newer tools that can be used from the command-line, often lack a man page as well. In that case, your best bet is to invoke the tool with the \texttt{-\/-help} option (and sometimes the \texttt{-h} option). For example:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{jq}\NormalTok{ --help}

\ExtensionTok{jq}\NormalTok{ - commandline JSON processor [version 1.4]}
\ExtensionTok{Usage}\NormalTok{: jq [options] }\OperatorTok{<}\NormalTok{jq filter}\OperatorTok{>}\NormalTok{ [file...]}

\ExtensionTok{For}\NormalTok{ a description of the command line options and}
\ExtensionTok{how}\NormalTok{ to write jq filters (and why you might want to)}
\ExtensionTok{see}\NormalTok{ the jq manpage, or the online documentation at}
\ExtensionTok{http}\NormalTok{://stedolan.github.com/jq}
\end{Highlighting}
\end{Shaded}

Specifying the \texttt{-\/-help} option also works for the GNU command-line tools such as \texttt{cat}. However, the corresponding man page often provides more information. If, after trying these three approaches, you are still stuck, then it is perfectly acceptable to consult the Internet. In the appendix, there's a list of all command-line tools used in this book. Besides how each command-line tool can be installed, it also shows how you can get help.

\hypertarget{further-reading}{%
\section{Further Reading}\label{further-reading}}

\begin{itemize}
\tightlist
\item
  Heddings, Lowell. 2006. ``Keyboard Shortcuts for Bash.'' http://www.howtogeek.com/howto/ubuntu/keyboard-shortcuts-for-bash-command-shell-for-ubuntu-debian-suse-redhat-linux-etc.
\item
  Peek, Jerry, Shelley Powers, Tim O'Reilly, and Mike Loukides. 2002. Unix Power Tools. 3rd Ed. O'Reilly Media.
\end{itemize}

\hypertarget{methods}{%
\chapter{Methods}\label{methods}}

We describe our methods in this chapter.

\hypertarget{chapter-3-obtaining-data}{%
\chapter{Obtaining Data}\label{chapter-3-obtaining-data}}

This chapter deals with the first step of the OSEMN model: obtaining data. After all, without any data, there is not much data science that we can do. We assume that the data that is needed to solve the data science problem at hand already exists at some location in some form. Our goal is to get this data onto your computer (or into your Data Science Toolbox) in a form that we can work with.

According to the Unix philosophy, text is a universal interface. Almost every command-line tool takes text as input, produces text as output, or both. This is the main reason why command-line tools can work so well together. However, as we'll see, even just text can come in multiple forms.

Data can be obtained in several ways---for example by downloading it from a server, by querying a database, or by connecting to a Web API. Sometimes, the data comes in a compressed form or in a binary format such as Microsoft Excel. In this chapter, we discuss several tools that help tackle this from the command line, including: \texttt{curl} \citep{curl}, \texttt{in2csv} \citep{in2csv}, \texttt{sql2csv} \citep{sql2csv}, and \texttt{tar} \citep{tar}.

\hypertarget{overview}{%
\section{Overview}\label{overview}}

In this chapter, you'll learn how to:

\begin{itemize}
\tightlist
\item
  Obtain data from the Internet
\item
  Query databases
\item
  Connect to Web APIs
\item
  Decompress files
\item
  Convert Microsoft Excel spreadsheets into usable data
\end{itemize}

\hypertarget{copying-local-files-to-the-data-science-toolbox}{%
\section{Copying Local Files to the Data Science Toolbox}\label{copying-local-files-to-the-data-science-toolbox}}

A common situation is that you already have the necessary files on your own computer. This section explains how you can get those files onto the local or remote version of the Data Science Toolbox.

\hypertarget{local-version-of-data-science-toolbox}{%
\subsection{Local Version of Data Science Toolbox}\label{local-version-of-data-science-toolbox}}

We mentioned in \protect\hyperlink{chapter-2-getting-started}{Chapter 2} that the Vagrant version of the Data Science Toolbox is an isolated virtual environment. Luckily there is one exception to that: files can be transfered in and out the Data Science Toolbox. The local directory from which you ran \texttt{vagrant\ up} (which is the one that contains the file \emph{Vagrantfile}), is mapped to a directory in the Data Science Toolbox. This directory is called \emph{/vagrant}. Please note that this is not your home directory. Let us check the contents of this directory:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{ls}\NormalTok{ -1 /vagrant}
\ExtensionTok{build}
\ExtensionTok{Vagrantfile}
\end{Highlighting}
\end{Shaded}

If you have a file on your local computer, and you want to apply some command-line tools to it, all you have to do is copy or move the file to that directory. Let's assume that you have a file called \emph{logs.csv} on your Desktop. If you are running Linux or macOS, execute the following command on your operating system (and not inside the Data Science Toolbox):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{cp}\NormalTok{ ~/Desktop/logs.csv .}
\end{Highlighting}
\end{Shaded}

And if you are running Windows, you can run the following commands on the command prompt:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{> }\FunctionTok{cd}\NormalTok{ %UserProfile%\textbackslash{}Desktop}
\NormalTok{> }\FunctionTok{copy}\NormalTok{ logs.}\FunctionTok{csv}\NormalTok{ MyDataScienceToolbox\textbackslash{}}
\end{Highlighting}
\end{Shaded}

You may also drag-and-drop the file into the directory using Windows Explorer.

The file is now located in the directory \emph{/vagrant}. It is a good idea to keep your data in a separate directory, like we have \emph{\textasciitilde{}/book/ch03/data}. So, after you have copied the file, you can move it by running:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{mv}\NormalTok{ /vagrant/logs.csv ~/book/ch03/data}
\NormalTok{$ }\BuiltInTok{cd}\NormalTok{ ~/book/ch03}
\NormalTok{$ }\FunctionTok{cat}\NormalTok{ data/logs.csv}
\end{Highlighting}
\end{Shaded}

\hypertarget{remote-version-of-data-science-toolbox}{%
\subsection{Remote Version of Data Science Toolbox}\label{remote-version-of-data-science-toolbox}}

If you are running Linux or macOS, you can use \texttt{scp} \citep{scp}, which stands for \emph{secure copy}, to copy files onto the EC2 instance. You will need the same key pair file that you used to login to the EC2 instance.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{scp}\NormalTok{ -i  mykey.pem ~/Desktop/logs.csv \textbackslash{}}
\OperatorTok{>}\NormalTok{ ubuntu@ec2-184-73-72-150.compute-1.amazonaws.com:data}
\end{Highlighting}
\end{Shaded}

Replace the host name in the example \emph{ec2-184-73-72-150.compute-1.amazonaws.com} with the value you see on the EC2 overview page in the AWS console.

\hypertarget{decompressing-files}{%
\section{Decompressing Files}\label{decompressing-files}}

If the original data set is very large or it's a collection of many files, the file may be a (compressed) archive. Data sets which contain many repeated values (such as the words in a text file or the keys in a JSON file) are especially well suited for compression.

Common file extensions of compressed archives are: \emph{.tar.gz}, \emph{.zip}, and \emph{.rar}. To decompress these, you would use the command-line tools \texttt{tar} \citep{tar}, \texttt{unzip} \citep{unzip}, and \texttt{unrar} \citep{unrar}, respectively. There exists a few more, though less common, file extensions for which you would need yet other tools. For example, in order to extract a file named \emph{logs.tar.gz}, you would use:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{cd}\NormalTok{ ~/book/ch03}
\NormalTok{$ }\FunctionTok{tar}\NormalTok{ -xzvf data/logs.tar.gz}
\end{Highlighting}
\end{Shaded}

Indeed, \texttt{tar} is notorious for its many command-line arguments. In this case, the four command-line arguments \texttt{x}, \texttt{z}, \texttt{v}, and \texttt{f} specify that \texttt{tar} should \emph{extract} files from an archive, use \emph{gzip} as the decompression algorithm, be \emph{verbose} and use file \emph{logs.tar.gz}. In time, you'll get used to typing these four characters, but there's a more convenient way.

Rather than remembering the different command-line tools and their options, there's a handy script called \texttt{unpack} \citep{unpack}, which will decompress many different formats. \texttt{unpack} looks at the extension of the file that you want to decompress, and calls the appropriate command-line tool.

The \texttt{unpack} tool is part of the Data Science Toolbox. Remember that you can look up how it can be installed in the appendix. Example \ref{exm:script-unpack} shows the source of \texttt{unpack}. Although Bash scripting is not the focus of this book, it's still useful to take a moment to figure out how it works.

\begin{example}[Decompress various file formats]
\protect\hypertarget{exm:script-unpack}{}{\label{exm:script-unpack} \iffalse (Decompress various file formats) \fi{} }
\end{example}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#!/usr/bin/env bash}
\CommentTok{# unpack: Extract common file formats}

\CommentTok{# Display usage if no parameters given}
\KeywordTok{if [[} \OtherTok{-z} \StringTok{"}\VariableTok{$@}\StringTok{"}\KeywordTok{ ]]}\NormalTok{; }\KeywordTok{then}
    \BuiltInTok{echo} \StringTok{" }\VariableTok{$\{0##}\NormalTok{*/}\VariableTok{\}}\StringTok{ <archive> - extract common file formats)"}
    \BuiltInTok{exit}
\KeywordTok{fi}

\CommentTok{# Required program(s)}
\VariableTok{req_progs=(}\NormalTok{7z unrar unzip}\VariableTok{)}
\KeywordTok{for} \ExtensionTok{p}\NormalTok{ in }\VariableTok{$\{req_progs[@]\}}\KeywordTok{;} \KeywordTok{do}
    \BuiltInTok{hash} \StringTok{"}\VariableTok{$p}\StringTok{"} \OperatorTok{2>}\KeywordTok{&}\ExtensionTok{-} \KeywordTok{||} \KeywordTok{\textbackslash{}}
    \KeywordTok{\{} \BuiltInTok{echo} \OperatorTok{>&2} \StringTok{" Required program }\DataTypeTok{\textbackslash{}"}\VariableTok{$p}\DataTypeTok{\textbackslash{}"}\StringTok{ not installed."}\KeywordTok{;} \BuiltInTok{exit}\NormalTok{ 1}\KeywordTok{;} \KeywordTok{\}}
\KeywordTok{done}

\CommentTok{# Test if file exists}
\KeywordTok{if}\BuiltInTok{ [} \OtherTok{!} \OtherTok{-f} \StringTok{"}\VariableTok{$@}\StringTok{"}\BuiltInTok{ ]}\NormalTok{; }\KeywordTok{then}
    \BuiltInTok{echo} \StringTok{"File "}\VariableTok{$@}\StringTok{" doesn't exist"}
    \BuiltInTok{exit}
\KeywordTok{fi}

\CommentTok{# Extract file by using extension as reference}
\KeywordTok{case} \StringTok{"}\VariableTok{$@}\StringTok{"}\KeywordTok{ in}
\NormalTok{    *.7z }\KeywordTok{)} \ExtensionTok{7z}\NormalTok{ x }\StringTok{"}\VariableTok{$@}\StringTok{"} \KeywordTok{;;}
    \ExtensionTok{*.tar.bz2}\NormalTok{ ) }\FunctionTok{tar}\NormalTok{ xvjf }\StringTok{"}\VariableTok{$@}\StringTok{"} \KeywordTok{;;}
    \ExtensionTok{*.bz2}\NormalTok{ ) }\FunctionTok{bunzip2} \StringTok{"}\VariableTok{$@}\StringTok{"} \KeywordTok{;;}
    \ExtensionTok{*.deb}\NormalTok{ ) }\FunctionTok{ar}\NormalTok{ vx }\StringTok{"}\VariableTok{$@}\StringTok{"} \KeywordTok{;;}
    \ExtensionTok{*.tar.gz}\NormalTok{ ) }\FunctionTok{tar}\NormalTok{ xvf }\StringTok{"}\VariableTok{$@}\StringTok{"} \KeywordTok{;;}
    \ExtensionTok{*.gz}\NormalTok{ ) }\FunctionTok{gunzip} \StringTok{"}\VariableTok{$@}\StringTok{"} \KeywordTok{;;}
    \ExtensionTok{*.tar}\NormalTok{ ) }\FunctionTok{tar}\NormalTok{ xvf }\StringTok{"}\VariableTok{$@}\StringTok{"} \KeywordTok{;;}
    \ExtensionTok{*.tbz2}\NormalTok{ ) }\FunctionTok{tar}\NormalTok{ xvjf }\StringTok{"}\VariableTok{$@}\StringTok{"} \KeywordTok{;;}
    \ExtensionTok{*.tar.xz}\NormalTok{ ) }\FunctionTok{tar}\NormalTok{ xvf }\StringTok{"}\VariableTok{$@}\StringTok{"} \KeywordTok{;;}
    \ExtensionTok{*.tgz}\NormalTok{ ) }\FunctionTok{tar}\NormalTok{ xvzf }\StringTok{"}\VariableTok{$@}\StringTok{"} \KeywordTok{;;}
    \ExtensionTok{*.rar}\NormalTok{ ) }\ExtensionTok{unrar}\NormalTok{ x }\StringTok{"}\VariableTok{$@}\StringTok{"} \KeywordTok{;;}
    \ExtensionTok{*.zip}\NormalTok{ ) }\FunctionTok{unzip} \StringTok{"}\VariableTok{$@}\StringTok{"} \KeywordTok{;;}
    \ExtensionTok{*.Z}\NormalTok{ ) }\ExtensionTok{uncompress} \StringTok{"}\VariableTok{$@}\StringTok{"} \KeywordTok{;;}
    \ExtensionTok{*}\NormalTok{ ) }\BuiltInTok{echo} \StringTok{" Unsupported file format"} \KeywordTok{;;}
\KeywordTok{esac}
\end{Highlighting}
\end{Shaded}

Now, in order to decompress this same file, you would simply use:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{unpack}\NormalTok{ logs.tar.gz}
\end{Highlighting}
\end{Shaded}

\hypertarget{converting-microsoft-excel-spreadsheets}{%
\section{Converting Microsoft Excel Spreadsheets}\label{converting-microsoft-excel-spreadsheets}}

For many people, Microsoft Excel offers an intuitive way to work with small data sets and perform calculations on them. As a result, a lot of data is embedded into Microsoft Excel spreadsheets. These spreadsheets are, depending on the extension of the filename, stored in either a proprietary binary format (\emph{.xls}) or as a collection of compressed XML files (\emph{.xlsx}). In both cases, the data is not readily usable by most command-line tools. It would be a shame if we could not use those valuable data sets just because they are stored this way.

Luckily, there is a command-line tool called \texttt{in2csv} \citep{in2csv}, which is able to convert Microsoft Excel spreadsheets to CSV files. CSV stands for comma-separated values. Working with CSV can be tricky because it lacks a formal specification. \href{http://www.ietf.org/rfc/rfc4180.txt}{RFC 4180} defines the CSV format according to the following three points:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Each record is located on a separate line, delimited by a line break (CRLF). For example:

\begin{verbatim}
aaa,bbb,ccc CRLF
zzz,yyy,xxx CRLF
\end{verbatim}
\item
  The last record in the file may or may not have an ending line break. For example:

\begin{verbatim}
aaa,bbb,ccc CRLF
zzz,yyy,xxx
\end{verbatim}
\item
  There maybe an optional header line appearing as the first line of the file with the same format as normal record lines. This header will contain names corresponding to the fields in the file and should contain the same number of fields as the records in the rest of the file (the presence or absence of the header line should be indicated via the optional header parameter of this MIME type). For example:

\begin{verbatim}
field_name,field_name,field_name CRLF
aaa,bbb,ccc CRLF
zzz,yyy,xxx CRLF
\end{verbatim}
\end{enumerate}

Let's demonstrate \texttt{in2csv} using a spreadsheet that contains the top 250 movies from the Internet Movie Database (IMDb). The file is named \emph{imdb-250.xlsx} and can be obtained from \url{http://www.overthinkingit.com/2011/10/11/imdb-top-250-movies-4th-edition/2}. To extract its data, we invoke \texttt{in2csv} as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{cd}\NormalTok{ book/ch03}
\NormalTok{$ }\ExtensionTok{in2csv}\NormalTok{ data/imdb-250.xlsx }\OperatorTok{>}\NormalTok{ data/imdb-250.csv}
\end{Highlighting}
\end{Shaded}

The format of the file is automatically determined by the extension, \emph{.xlsx} in this case. If we were to pipe the data into \texttt{in2csv}, we would have to specify the format explicitly. Let's look at the data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{in2csv}\NormalTok{ imdb-250.xlsx }\KeywordTok{|} \FunctionTok{head} \KeywordTok{|} \FunctionTok{cut}\NormalTok{ -c1-80}
\ExtensionTok{Title}\NormalTok{,title trim,Year,Rank,Rank (desc),}\ExtensionTok{Rating}\NormalTok{,New in 2011 from 2010?,2010 rank,R}
\ExtensionTok{Sherlock}\NormalTok{ Jr. (1924),}\ExtensionTok{SherlockJr.}\NormalTok{(1924),}\ExtensionTok{1924}\NormalTok{,221,30,8,y,n/a,n/a,}
\ExtensionTok{The}\NormalTok{ Passion of Joan of Arc (1928),}\ExtensionTok{ThePassionofJoanofArc}\NormalTok{(1928),}\ExtensionTok{1928}\NormalTok{,212,39,8,y,n/}
\ExtensionTok{His}\NormalTok{ Girl Friday (1940),}\ExtensionTok{HisGirlFriday}\NormalTok{(1940),}\ExtensionTok{1940}\NormalTok{,250,1,8,y,n/a,n/a,}
\ExtensionTok{Tokyo}\NormalTok{ Story (1953),}\ExtensionTok{TokyoStory}\NormalTok{(1953),}\ExtensionTok{1953}\NormalTok{,248,3,8,y,n/a,n/a,}
\ExtensionTok{The}\NormalTok{ Man Who Shot Liberty Valance (1962),}\ExtensionTok{TheManWhoShotLibertyValance}\NormalTok{(1962),}\ExtensionTok{1962}\NormalTok{,2}
\ExtensionTok{Persona}\NormalTok{ (1966),}\ExtensionTok{Persona}\NormalTok{(1966),}\ExtensionTok{1966}\NormalTok{,200,51,8,y,n/a,n/a,}
\ExtensionTok{Stalker}\NormalTok{ (1979),}\ExtensionTok{Stalker}\NormalTok{(1979),}\ExtensionTok{1979}\NormalTok{,243,8,8,y,n/a,n/a,}
\ExtensionTok{Fanny}\NormalTok{ and Alexander (1982),}\ExtensionTok{FannyandAlexander}\NormalTok{(1982),}\ExtensionTok{1982}\NormalTok{,210,41,8,y,n/a,n/a,}
\ExtensionTok{Beauty}\NormalTok{ and the Beast (1991),}\ExtensionTok{BeautyandtheBeast}\NormalTok{(1991),}\ExtensionTok{1991}\NormalTok{,249,2,8,y,n/a,n/a,}
\end{Highlighting}
\end{Shaded}

As you can see, CSV by default is not too readable. You can pipe the data to a tool called \texttt{csvlook} \citep{csvlook}, which will nicely format the data into a table. Here, we'll display a subset of the columns using \texttt{csvcut} such that the table fits on the page:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{in2csv}\NormalTok{ data/imdb-250.xlsx }\KeywordTok{|} \FunctionTok{head} \KeywordTok{|} \ExtensionTok{csvcut}\NormalTok{ -c Title,Year,Rating }\KeywordTok{|} \ExtensionTok{csvlook}
\KeywordTok{|}\ExtensionTok{------------------------------------------+------+---------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{Title}                                   \KeywordTok{|} \ExtensionTok{Year} \KeywordTok{|} \ExtensionTok{Rating}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{------------------------------------------+------+---------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{Sherlock}\NormalTok{ Jr. (1924)                     }\KeywordTok{|} \ExtensionTok{1924} \KeywordTok{|} \ExtensionTok{8}       \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{The}\NormalTok{ Passion of Joan of Arc (1928)       }\KeywordTok{|} \ExtensionTok{1928} \KeywordTok{|} \ExtensionTok{8}       \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{His}\NormalTok{ Girl Friday (1940)                  }\KeywordTok{|} \ExtensionTok{1940} \KeywordTok{|} \ExtensionTok{8}       \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{Tokyo}\NormalTok{ Story (1953)                      }\KeywordTok{|} \ExtensionTok{1953} \KeywordTok{|} \ExtensionTok{8}       \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{The}\NormalTok{ Man Who Shot Liberty Valance (1962) }\KeywordTok{|} \ExtensionTok{1962} \KeywordTok{|} \ExtensionTok{8}       \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{Persona}\NormalTok{ (1966)                          }\KeywordTok{|} \ExtensionTok{1966} \KeywordTok{|} \ExtensionTok{8}       \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{Stalker}\NormalTok{ (1979)                          }\KeywordTok{|} \ExtensionTok{1979} \KeywordTok{|} \ExtensionTok{8}       \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{Fanny}\NormalTok{ and Alexander (1982)              }\KeywordTok{|} \ExtensionTok{1982} \KeywordTok{|} \ExtensionTok{8}       \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{Beauty}\NormalTok{ and the Beast (1991)             }\KeywordTok{|} \ExtensionTok{1991} \KeywordTok{|} \ExtensionTok{8}       \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{------------------------------------------+------+---------}\KeywordTok{|}
\end{Highlighting}
\end{Shaded}

A spreadsheet can contain multiple worksheets. By default, \texttt{in2csv} extracts the first worksheet. To extract a different worksheet, you need to pass the name of worksheet to the \texttt{-\/-sheet} option.

The tools \texttt{in2csv}, \texttt{csvcut}, and \texttt{csvlook} are actually part of Csvkit, which is collection of command-line tools to work with CSV data. Csvkit will be used quite often in this book because it has so many valuable tools. If you're running the Data Science Toolbox, you already have Csvkit installed. Otherwise, see the appendix for instructions on how to install it.

\begin{rmdnote}
An alternative approach to \texttt{in2csv} is to open the spreadsheet in Microsoft Excel or an open source variant such as LibreOffice Calc, and manually export it to CSV. While this works as a one-off solution, the disadvantage is that it does not scale well to multiple files and is not automatable. Furthermore, when you are working on the command line of a remote server, chances are that you don't have such an application available.
\end{rmdnote}

\hypertarget{querying-relational-databases}{%
\section{Querying Relational Databases}\label{querying-relational-databases}}

Most companies store their data in a relational database. Examples of relational databases are MySQL, PostgreSQL, and SQLite. These databases all have a slightly different way of interfacing with them. Some provide a command-line tool or a command-line interface, while others do not. Moreover, they are not very consistent when it comes to their usage and output.

Fortunately, there is a command-line tool called \texttt{sql2csv}, which is part of the Csvkit suite. Because it leverages the Python SQLAlchemy package, we only have to use one tool to execute queries on many different databases through a common interface, including MySQL, Oracle, PostgreSQL, SQLite, Microsoft SQL Server, and Sybase. The output of \texttt{sql2csv} is, as its name suggests, in CSV format.

We can obtain data from relational databases by executing a \texttt{SELECT} query on them. (\texttt{sql2csv} also support \texttt{INSERT}, \texttt{UPDATE}, and \texttt{DELETE} queries, but that's not the purpose of this chapter.) To select a specific set of data from an SQLite database named \emph{iris.db}, \texttt{sql2csv} can be invoked as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{sql2csv}\NormalTok{ --db }\StringTok{'sqlite:///data/iris.db'}\NormalTok{ --query }\StringTok{'SELECT * FROM iris '}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{'WHERE sepal_length > 7.5'}
\ExtensionTok{sepal_length}\NormalTok{,sepal_width,petal_length,petal_width,species}
\ExtensionTok{7.6}\NormalTok{,3.0,6.6,2.1,Iris-virginica}
\ExtensionTok{7.7}\NormalTok{,3.8,6.7,2.2,Iris-virginica}
\ExtensionTok{7.7}\NormalTok{,2.6,6.9,2.3,Iris-virginica}
\ExtensionTok{7.7}\NormalTok{,2.8,6.7,2.0,Iris-virginica}
\ExtensionTok{7.9}\NormalTok{,3.8,6.4,2.0,Iris-virginica}
\ExtensionTok{7.7}\NormalTok{,3.0,6.1,2.3,Iris-virginica}
\end{Highlighting}
\end{Shaded}

Here, we are selecting all rows where \texttt{sepal\textbackslash{}\_length} is larger than 7.5. The \texttt{-\/-db} option specifies the database URL, of which the typical form is: \texttt{dialect+driver://username:password@host:port/database}.

\hypertarget{downloading-from-the-internet}{%
\section{Downloading from the Internet}\label{downloading-from-the-internet}}

The Internet provides without a doubt the largest resource for data. This data is available in various forms, using various protocols. The command-line tool cURL \citep{curl} can be considered the command line's Swiss Army knife when it comes to downloading data from the Internet.

When you access a URL, which stands for \emph{uniform resource locator}, through your browser, the data that is being downloaded can be interpreted. For example, an HTML file is rendered as a website, an MP3 file may be automatically played, and a PDF file may be automatically downloaded or opened by a viewer. However, when cURL is used to access a URL, the data is downloaded as is printed to standard output. Other command-line tools may then be used to process this data further.

The easiest invocation of cURL is to simply specify a URL as a command-line argument. For example, to download the book \emph{Adventures of Huckleberry Finn} by Mark Twain from Project Gutenberg, we can run the following command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{curl}\NormalTok{ -s http://www.gutenberg.org/files/76/76-0.txt }\KeywordTok{|} \FunctionTok{head}\NormalTok{ -n 10}

\ExtensionTok{The}\NormalTok{ Project Gutenberg EBook of Adventures of Huckleberry Finn, Complete}
\ExtensionTok{by}\NormalTok{ Mark Twain (Samuel Clemens)}

\ExtensionTok{This}\NormalTok{ eBook is for the use of anyone anywhere at no cost and with almost}
\ExtensionTok{no}\NormalTok{ restrictions whatsoever. You may copy it, give it away or re-use}
\ExtensionTok{it}\NormalTok{ under the terms of the Project Gutenberg License included with this}
\ExtensionTok{eBook}\NormalTok{ or online at www.gutenberg.net}
\end{Highlighting}
\end{Shaded}

By default, cURL outputs a progress meter that shows how the download rate and the expected time of completion. If you are piping the output directly to another command-line tool, such as \texttt{head}, be sure to specify the \texttt{-s} command-line argument, which stands for \emph{silent}, so that the progress meter is disabled. Compare, for example, the output with the following command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{curl}\NormalTok{ http://www.gutenberg.org/files/76/76-0.txt }\KeywordTok{|} \FunctionTok{head}\NormalTok{ -n 10}
  \ExtensionTok{%}\NormalTok{ Total    % Received % Xferd  Average Speed   Time    Time     Time  Current}
                                 \ExtensionTok{Dload}\NormalTok{  Upload   Total   Spent    Left  Speed}

  \ExtensionTok{0}\NormalTok{     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--}

\ExtensionTok{The}\NormalTok{ Project Gutenberg EBook of Adventures of Huckleberry Finn, Complete}
\ExtensionTok{by}\NormalTok{ Mark Twain (Samuel Clemens)}

\ExtensionTok{This}\NormalTok{ eBook is for the use of anyone anywhere at no cost and with almost}
\ExtensionTok{no}\NormalTok{ restrictions whatsoever. You may copy it, give it away or re-use}
\ExtensionTok{it}\NormalTok{ under the terms of the Project Gutenberg License included with this}
\ExtensionTok{eBook}\NormalTok{ or online at www.gutenberg.net}
\end{Highlighting}
\end{Shaded}

Note that the output of the second command, where we do not disable the progress meter, contains the unwanted text and even an error message. If you save the data to a file, then you do not need to necessarily specify the \texttt{-s} option:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{curl}\NormalTok{ http://www.gutenberg.org/files/76/76-0.txt }\OperatorTok{>}\NormalTok{ data/finn.txt}
\end{Highlighting}
\end{Shaded}

You can also save the data by explicitly specifying the output file with the \texttt{-o} option:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{curl}\NormalTok{ -s http://www.gutenberg.org/files/76/76-0.txt -o data/finn.txt}
\end{Highlighting}
\end{Shaded}

When downloading data from the Internet, the URL will most likely use the protocols HTTP or HTTPS. To download from an FTP server, which stands for File Transfer Protocol, you use cURL in exactly the same way. When the URL is password protected, you can specify a username and a password as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{curl}\NormalTok{ -u username:password ftp://host/file}
\end{Highlighting}
\end{Shaded}

If the specified URL is a directory, \texttt{curl} will list the contents of that directory.

When you access a shortened URL, such as the ones that start with \emph{\url{http://bit.ly/}} or \emph{\url{http://t.co/}}, your browser automatically redirects you to the correct location. With \texttt{curl}, however, you need to specify the \texttt{-L} or \texttt{-\/-location} option in order to be redirected:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{curl}\NormalTok{ -L j.mp/locatbbar}
\end{Highlighting}
\end{Shaded}

If you do not specify the \texttt{-L} or \texttt{-\/-location} option, you may get something like:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{curl}\NormalTok{ j.mp/locatbbar}
\OperatorTok{<}\ExtensionTok{html}\OperatorTok{>}
\OperatorTok{<}\FunctionTok{head}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{title}\OperatorTok{>}\NormalTok{bit.ly}\OperatorTok{<}\NormalTok{/title}\OperatorTok{>}
\OperatorTok{<}\NormalTok{/}\ExtensionTok{head}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{body}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{a}\NormalTok{ href=}\StringTok{"http://en.wikipedia.org/wiki/List_of_countries_and_territories_by_bo}
\StringTok{rder/area_ratio"}\OperatorTok{>}\NormalTok{moved here}\OperatorTok{<}\NormalTok{/a}\OperatorTok{>}
\OperatorTok{<}\NormalTok{/}\ExtensionTok{body}\OperatorTok{>}
\end{Highlighting}
\end{Shaded}

By specifying the \texttt{-I} or \texttt{-\/-head} option, \texttt{curl} fetches only the HTTP header of the response:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{curl}\NormalTok{ -I j.mp/locatbbar}
\ExtensionTok{HTTP/1.1}\NormalTok{ 301 Moved Permanently}
\ExtensionTok{Server}\NormalTok{: nginx}
\ExtensionTok{Date}\NormalTok{: Wed, 21 May 2014 18:50:28 GMT}
\ExtensionTok{Content-Type}\NormalTok{: text/html}\KeywordTok{;} \VariableTok{charset=}\NormalTok{utf-8}
\ExtensionTok{Connection}\NormalTok{: keep-alive}
\ExtensionTok{Cache-Control}\NormalTok{: private}\KeywordTok{;} \ExtensionTok{max-age}\NormalTok{=90}
\ExtensionTok{Content-Length}\NormalTok{: 175}
\ExtensionTok{Location}\NormalTok{: http://en.wikipedia.org/wiki/List_of_countries_and_territories_by_bo}
\ExtensionTok{Mime-Version}\NormalTok{: 1.0}
\ExtensionTok{Set-Cookie}\NormalTok{: _bit=537cf574-002ba-07d79-2e1cf10a}\KeywordTok{;}\VariableTok{domain=}\NormalTok{.j.mp;}\VariableTok{expires=}\NormalTok{Mon }\ExtensionTok{Nov}\NormalTok{ 17}
\end{Highlighting}
\end{Shaded}

The first line indicates the HTTP status code, which is 301 (moved permanently) in this case. You can also see the location this URL redirects to: \url{http://en.wikipedia.org/wiki/List_of_countries_and_territories_by_border/area_ratio}. Inspecting the header and getting the status code is a useful debugging tool in case \texttt{curl} does not give you the expected result. Other common HTTP status codes include 404 (not found) and 403 (forbidden). This page lists all HTTP status codes: \url{http://en.wikipedia.org/wiki/List_of_HTTP_status_codes}.

To conclude this section, cURL is a straight-forward command-line tool for downloading data from the Internet. Its three most common command-line arguments are \texttt{-s} to suppress the progress meter, \texttt{-u} to specify a username and password, and \texttt{-L} to automatically follow redirects. See its man page for more information.

\hypertarget{calling-a-web-api}{%
\section{Calling a Web API}\label{calling-a-web-api}}

In the previous section we explained how to download individual files from the Internet. Another way data can come from the Internet is through a web API, which stands for \emph{Application Programming Interface}. The number of APIs that are being offered by organizations is growing at increasing rate, which means a lot of interesting data for us data scientists.

Web APIs are not meant to be presented in nice layout, such as websites. Instead, most web APIs return data in a structured format, such as JSON or XML. Having data in a structured form has the advantage that the data can be easily processed by other tools, such as \texttt{jq}. For example, the API from \url{https://randomuser.me} returns data in the following JSON structure.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{curl}\NormalTok{ -s https://randomuser.me/api/1.2/ }\KeywordTok{|} \ExtensionTok{jq}\NormalTok{ .}
\KeywordTok{\{}
  \StringTok{"results"}\NormalTok{:}\BuiltInTok{ [}
\NormalTok{    \{}
      \StringTok{"gender"}\NormalTok{: }\StringTok{"male"}\NormalTok{,}
      \StringTok{"name"}\NormalTok{: \{}
        \StringTok{"title"}\NormalTok{: }\StringTok{"mr"}\NormalTok{,}
        \StringTok{"first"}\NormalTok{: }\StringTok{"jeffrey"}\NormalTok{,}
        \StringTok{"last"}\NormalTok{: }\StringTok{"lawson"}
\NormalTok{      \},}
      \StringTok{"location"}\NormalTok{: \{}
        \StringTok{"street"}\NormalTok{: }\StringTok{"838 miller ave"}\NormalTok{,}
        \StringTok{"city"}\NormalTok{: }\StringTok{"washington"}\NormalTok{,}
        \StringTok{"state"}\NormalTok{: }\StringTok{"maryland"}\NormalTok{,}
        \StringTok{"postcode"}\NormalTok{: 81831,}
        \StringTok{"coordinates"}\NormalTok{: \{}
          \StringTok{"latitude"}\NormalTok{: }\StringTok{"81.9488"}\NormalTok{,}
          \StringTok{"longitude"}\NormalTok{: }\StringTok{"-67.8247"}
\NormalTok{        \},}
        \StringTok{"timezone"}\NormalTok{: \{}
          \StringTok{"offset"}\NormalTok{: }\StringTok{"+4:00"}\NormalTok{,}
          \StringTok{"description"}\NormalTok{: }\StringTok{"Abu Dhabi, Muscat, Baku, Tbilisi"}
\NormalTok{        \}}
\NormalTok{      \},}
      \StringTok{"email"}\NormalTok{: }\StringTok{"jeffrey.lawson@example.com"}\NormalTok{,}
      \StringTok{"login"}\NormalTok{: \{}
        \StringTok{"uuid"}\NormalTok{: }\StringTok{"78918f6c-2658-4915-bebf-bfaa61a1624c"}\NormalTok{,}
        \StringTok{"username"}\NormalTok{: }\StringTok{"silverzebra774"}\NormalTok{,}
        \StringTok{"password"}\NormalTok{: }\StringTok{"treble"}\NormalTok{,}
        \StringTok{"salt"}\NormalTok{: }\StringTok{"iAtIKhvB"}\NormalTok{,}
        \StringTok{"md5"}\NormalTok{: }\StringTok{"4c02abeca4d6ca4dbfc0ddb33dcef29f"}\NormalTok{,}
        \StringTok{"sha1"}\NormalTok{: }\StringTok{"36e109513abf73df460cead89b78c749abe908fa"}\NormalTok{,}
        \StringTok{"sha256"}\NormalTok{: }\StringTok{"0155d9e6cabedfc3ad0f21d18b3ca3e738a8f17811dd57dc3b4dd386cd021963"}
\NormalTok{      \},}
      \StringTok{"dob"}\NormalTok{: \{}
        \StringTok{"date"}\NormalTok{: }\StringTok{"1996-07-04T02:49:46Z"}\NormalTok{,}
        \StringTok{"age"}\NormalTok{: 22}
\NormalTok{      \},}
      \StringTok{"registered"}\NormalTok{: \{}
        \StringTok{"date"}\NormalTok{: }\StringTok{"2013-01-13T13:37:21Z"}\NormalTok{,}
        \StringTok{"age"}\NormalTok{: 5}
\NormalTok{      \},}
      \StringTok{"phone"}\NormalTok{: }\StringTok{"(406)-041-2792"}\NormalTok{,}
      \StringTok{"cell"}\NormalTok{: }\StringTok{"(831)-085-8264"}\NormalTok{,}
      \StringTok{"id"}\NormalTok{: \{}
        \StringTok{"name"}\NormalTok{: }\StringTok{"SSN"}\NormalTok{,}
        \StringTok{"value"}\NormalTok{: }\StringTok{"629-40-9671"}
\NormalTok{      \},}
      \StringTok{"picture"}\NormalTok{: \{}
        \StringTok{"large"}\NormalTok{: }\StringTok{"https://randomuser.me/api/portraits/men/62.jpg"}\NormalTok{,}
        \StringTok{"medium"}\NormalTok{: }\StringTok{"https://randomuser.me/api/portraits/med/men/62.jpg"}\NormalTok{,}
        \StringTok{"thumbnail"}\NormalTok{: }\StringTok{"https://randomuser.me/api/portraits/thumb/men/62.jpg"}
\NormalTok{      \},}
      \StringTok{"nat"}\NormalTok{: }\StringTok{"US"}
\NormalTok{    \}}
\NormalTok{  ],}
  \StringTok{"info"}\NormalTok{: \{}
    \StringTok{"seed"}\NormalTok{: }\StringTok{"4bd9f66fd83a6ec7"}\NormalTok{,}
    \StringTok{"results"}\NormalTok{: 1,}
    \StringTok{"page"}\NormalTok{: 1,}
    \StringTok{"version"}\NormalTok{: }\StringTok{"1.2"}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The data is piped to a command-line tool \texttt{jq} in order to display it in a nice way. \texttt{jq} has many more possibilities that we will explore in \protect\hyperlink{chapter-5-scrubbing-data}{Chapter 5}.

Some web APIs return data in a streaming manner. This means that once you connect to it, the data will continue to pour in forever. A well-known example is the Twitter ``firehose'', which constantly streams all the tweets being sent around the world. Luckily, most command-line tools that we use also operate in a streaming matter, so that we also use this kind of data.

Some APIs require you to log in using the OAuth protocol. There is a handy command-line tool called \texttt{curlicue} \citep{curlicue} that assists in performing the so-called ``OAuth dance''. Once this has been set up, it \texttt{curlicue} will call \texttt{curl} with the correct headers. First, you set things up once for a particular API with \texttt{curlicue-setup}, and then you can call that API using \texttt{curlicue}. For example, to use \texttt{curlicue} with the Twitter API you would run:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{curlicue-setup}\NormalTok{ \textbackslash{}}
\OperatorTok{>} \StringTok{'https://api.twitter.com/oauth/request_token'}\NormalTok{ \textbackslash{}}
\OperatorTok{>} \StringTok{'https://api.twitter.com/oauth/authorize?oauth_token=$oauth_token'}\NormalTok{ \textbackslash{}}
\OperatorTok{>} \StringTok{'https://api.twitter.com/oauth/access_token'}\NormalTok{ \textbackslash{}}
\OperatorTok{>}\NormalTok{ credentials}
\NormalTok{$ }\ExtensionTok{curlicue}\NormalTok{ -f credentials \textbackslash{}}
\OperatorTok{>} \StringTok{'https://api.twitter.com/1/statuses/home_timeline.xml'}
\end{Highlighting}
\end{Shaded}

For more popular APIs, there are specialized command-line tools available. These are wrappers that provide a convenient way to connect to the API. In \protect\hyperlink{chapter-9-modeling-data}{Chapter 9}, for example, we'll be using the command-line tool \texttt{bigmler} that only connects to BigML's prediction API.

\hypertarget{further-reading}{%
\section{Further Reading}\label{further-reading}}

\begin{itemize}
\tightlist
\item
  Molinaro, Anthony. 2005. SQL Cookbook. O'Reilly Media.
\item
  Wikipedia. 2014. ``List of Http Status Codes.'' http://en.wikipedia.org/wiki/List\_of\_HTTP\_status\_codes.
\end{itemize}

\hypertarget{chapter-4-creating-reusable-command-line-tools}{%
\chapter{Creating Reusable Command-line Tools}\label{chapter-4-creating-reusable-command-line-tools}}

Throughout the book, we use a lot of commands and pipelines that basically fit on one line. Let us call those one-liners. Being able to perform complex tasks with just a one-liner is what makes the command line powerful. It's a very different experience from writing traditional programs.

Some tasks you perform only once, and some you perform more often. Some tasks are very specific and others can be generalized. If you foresee or notice that you need to repeat a certain one-liner on a regular basis, it is worthwhile to turn this into a command-line tool of its own. So, both one-liners and command-line tools have their uses. Recognizing the opportunity requires practice and skill. The advantage of a command-line tool is that you do not have to remember the entire one-liner and that it improves readability if you include it into some other pipeline.

The benefit of a working with a programming language, however, is that you have the code in a file. This means that you can easily reuse that code. If the code has parameters it can even be applied to problems that follow a similar pattern.

Command-line tools have the best of both worlds: they can be used from the command line, accept parameters, and only have to be created once. In this chapter we're going to get familiar creating reusable command-line tools in two ways. First, we explain to turn those one-liners into reusable command-line tools. By adding parameters to our commands, we can add the same flexibility that a programming language offers. Subsequently, we demonstrate how to create reusable command-line tools from code you have written in a programming language. By following the UNIX philosophy, your code can be combined with other command-line tools, which may be written in an entirely different language. We will focus on three programming languages: Python, R, and Java.

We believe that creating reusable command-line tools makes you a more efficient and productive data scientist in the long run. You gradually build up your own data science toolbox from which you can draw existing tools and apply it to problems you have encountered previously. It requires practice in order to be able to recognize the opportunity to turn a one-liner or existing code into a command-line tool.

In order to turn a one-liner into a shell script, we need to use some shell scripting. We shall only demonstrate the usefulness a small subset of concepts from shell scripting. This subset includes variables, conditionals, and loops. A complete course in shell scripting deserves a book on its own, and is therefore beyond the scope of this one. If you want to dive deeper into shell scripting, we recommend \emph{Classic Shell Scripting} by \citet{Robbins2005}.

\hypertarget{overview}{%
\section{Overview}\label{overview}}

In this chapter, you'll learn how to:

\begin{itemize}
\tightlist
\item
  Convert one-liners into shell scripts.
\item
  Make existing Python, R, and Java code part of the command line.
\end{itemize}

\hypertarget{converting-one-liners-into-shell-scripts}{%
\section{Converting One-liners into Shell Scripts}\label{converting-one-liners-into-shell-scripts}}

In this section we are going to explain how to turn a one-liner into a reusable command-line tool. Imagine that we have the following one-liner:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{curl}\NormalTok{ -s http://www.gutenberg.org/files/76/76-0.txt }\KeywordTok{|}
\OperatorTok{>} \FunctionTok{tr} \StringTok{'[:upper:]'} \StringTok{'[:lower:]'} \KeywordTok{|} 
\OperatorTok{>} \FunctionTok{grep}\NormalTok{ -oE }\StringTok{'\textbackslash{}w+'} \KeywordTok{|}             
\OperatorTok{>} \FunctionTok{sort} \KeywordTok{|}                       
\OperatorTok{>} \FunctionTok{uniq}\NormalTok{ -c }\KeywordTok{|}                    
\OperatorTok{>} \FunctionTok{sort}\NormalTok{ -nr }\KeywordTok{|}                   
\OperatorTok{>} \FunctionTok{head}\NormalTok{ -n 10                   }
   \ExtensionTok{6441}\NormalTok{ and}
   \ExtensionTok{5082}\NormalTok{ the}
   \ExtensionTok{3666}\NormalTok{ i}
   \ExtensionTok{3258}\NormalTok{ a}
   \ExtensionTok{3022}\NormalTok{ to}
   \ExtensionTok{2567}\NormalTok{ it}
   \ExtensionTok{2086}\NormalTok{ t}
   \ExtensionTok{2044}\NormalTok{ was}
   \ExtensionTok{1847}\NormalTok{ he}
   \ExtensionTok{1778}\NormalTok{ of}
\end{Highlighting}
\end{Shaded}

In short, as you may have guessed from the output, this one-liner returns the top ten words of the e-book version of \emph{Adventures of Huckleberry Finn}. It accomplishes this by:

\begin{itemize}
\item
  Downloading an ebook using \texttt{curl}.
\item
  Converting the entire text to lowercase using \texttt{tr} \citep{tr}.
\item
  Extracting all the words using \texttt{grep} \citep{grep} and put each word on separate line.
\item
  Sort these words in alphabetical order using \texttt{sort} \citep{sort}.
\item
  Remove all the duplicates and count how often each word appears in the list using \texttt{uniq} \citep{uniq}.
\item
  Sort this list of unique words by their count in descending order using \texttt{sort}.
\item
  Keep only the top 10 lines (i.e., words) using \texttt{head}.
\end{itemize}

\begin{rmdtip}
Each command-line tool used in this one-liner offers a man page. So in case you would like to know more about, say, \texttt{grep}, you can run \texttt{man\ grep} from the command line. The command-line tools \texttt{tr}, \texttt{grep}, \texttt{uniq}, and \texttt{sort} will be discussed in more detail in the next chapter.
\end{rmdtip}

There is nothing wrong with running this one-liner just once. However, imagine if we wanted to have the top 10 words of every e-book on Project Gutenberg. Or imagine that we wanted the top 10 words of a news website on a hourly basis. In those cases, it would be best to have this one-liner as a separate building block that can be part of something bigger. Because we want to add some flexibility to this one-liner in terms of parameters, we will turn it into a shell script.

Because we use Bash as our shell, the script will be written in the programming language Bash. This allows us to take the one-liner as the starting point, and gradually improve on it. To turn this one-liner into a reusable command-line tool, we'll walk you through the following six steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Copy and paste the one-liner into a file.
\item
  Add execute permissions.
\item
  Define a so-called shebang.
\item
  Remove the fixed input part.
\item
  Add a parameter.
\item
  Optionally extend your PATH.
\end{enumerate}

\hypertarget{step-1-copy-and-paste}{%
\subsection{Step 1: Copy and Paste}\label{step-1-copy-and-paste}}

The first step is to create a new file. Open your favorite text editor and copy and paste our one-liner. We use name the file \emph{top-words-1.sh} (The \emph{1} stands for the first step towards our new command-line tool), and put it in the \emph{\textasciitilde{}/book/ch04} directory, but you may choose a different name and location. The contents of the file should look something like Example \ref{exm:top-words-1}.

\begin{example}[~/book/ch04/top-words-1.sh]
\protect\hypertarget{exm:top-words-1}{}{\label{exm:top-words-1} \iffalse (\textasciitilde{}/book/ch04/top-words-1.sh) \fi{} }
\end{example}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{curl}\NormalTok{ -s http://www.gutenberg.org/files/76/76-0.txt }\KeywordTok{|}
\FunctionTok{tr} \StringTok{'[:upper:]'} \StringTok{'[:lower:]'} \KeywordTok{|} \FunctionTok{grep}\NormalTok{ -oE }\StringTok{'\textbackslash{}w+'} \KeywordTok{|} \FunctionTok{sort} \KeywordTok{|}
\FunctionTok{uniq}\NormalTok{ -c }\KeywordTok{|} \FunctionTok{sort}\NormalTok{ -nr }\KeywordTok{|} \FunctionTok{head}\NormalTok{ -n 10}
\end{Highlighting}
\end{Shaded}

We are using the file extension \emph{.sh} to make clear that we are creating a shell script. However, command-line tools do not need to have an extension. In fact, command-line tools rarely have extensions.

\begin{rmdtip}
Here is a nice little command-line trick. On the command-line, \texttt{!!} will be substituted with the previous command. So, if you realize you needed superuser privileges for the previous command, you can run \texttt{sudo\ !!} \citep{sudo}. And if you want to save the previous command into a file without have to copy and paste it, you can run \texttt{echo\ "!!"\ \textgreater{}\ scriptname}. Be sure to check the contents of the file \emph{scriptname} for correctness before executing it because it may not always work when your command has quotes.
\end{rmdtip}

We can now use \texttt{bash} \citep{bash} to interpret and execute the commands in the file:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{bash}\NormalTok{ book/ch04/top-words-1.sh}
   \ExtensionTok{6441}\NormalTok{ and}
   \ExtensionTok{5082}\NormalTok{ the}
   \ExtensionTok{3666}\NormalTok{ i}
   \ExtensionTok{3258}\NormalTok{ a}
   \ExtensionTok{3022}\NormalTok{ to}
   \ExtensionTok{2567}\NormalTok{ it}
   \ExtensionTok{2086}\NormalTok{ t}
   \ExtensionTok{2044}\NormalTok{ was}
   \ExtensionTok{1847}\NormalTok{ he}
   \ExtensionTok{1778}\NormalTok{ of}
\end{Highlighting}
\end{Shaded}

This already saves us from typing the one-liner. Because the file cannot be executed on its own, it is not yet a true command-line tool. Let us change that in the next step.

\hypertarget{step-2-add-permission-to-execute}{%
\subsection{Step 2: Add Permission to Execute}\label{step-2-add-permission-to-execute}}

The reason we cannot execute our file directly is that we do not have the correct access permissions. In particular, you, as a user, need to have the permission to execute the file. In this section we change the access permissions of our file.

\begin{rmdnote}
In order to compare differences between steps, we copy the file to \emph{top-words-2.sh} using \texttt{cp\ top-words-\{1,2\}.sh}. You can keep working with the same file if you want to.
\end{rmdnote}

To change the access permissions of a file, we need to use a command-line tool called \texttt{chmod} \citep{chmod}, which stands for \emph{change mode}. It changes the file mode bits of a specific file. The following command gives the user, you, the permission to execute \emph{top-words-2.sh}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{cd}\NormalTok{ ~/book/ch04/}
\NormalTok{$ }\FunctionTok{chmod}\NormalTok{ u+x top-words-2.sh}
\end{Highlighting}
\end{Shaded}

The command-line argument \texttt{u+x} consists of three characters: (1) \texttt{u} indicates that we want to change the permissions for the user who owns the file, which is you, because you created the file; (2) \texttt{+} indicates that we want to add a permission; and (3) \texttt{x}, which indicates the permissions to execute. Let us now have a look at the access permissions of both files:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{ls}\NormalTok{ -l top-words-}\DataTypeTok{\{1,2\}}\NormalTok{.sh}
\ExtensionTok{-rw-rw-r--}\NormalTok{ 1 vagrant vagrant 145 Jul 20 23:33 top-words-1.sh}
\ExtensionTok{-rwxrw-r--}\NormalTok{ 1 vagrant vagrant 143 Jul 20 23:34 top-words-2.sh}
\end{Highlighting}
\end{Shaded}

The first column shows the access permissions for each file. For \emph{top-words-2.sh}, this is \texttt{-rwxrw-r-\/-}. The first character \texttt{-} indicates the file type. A \texttt{-} means regular file and a \texttt{d} means directory. The next three characters \texttt{rwx} indicate the access permissions for the user who owns the file. The \texttt{r} and \texttt{w} mean read and write respectively. (As you can see, \emph{top-words-1.sh} has a \texttt{-} instead of an \texttt{x}, which means that we cannot execute that file.) The next three characters \texttt{rw-} indicate the access permissions for all members of the group that owns the file. Finally, the last three characters in the column \texttt{r-\/-} indicate access permissions for all other users.

Now you can execute the file as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{book/ch04/top-words-2.sh}
   \ExtensionTok{6441}\NormalTok{ and}
   \ExtensionTok{5082}\NormalTok{ the}
   \ExtensionTok{3666}\NormalTok{ i}
   \ExtensionTok{3258}\NormalTok{ a}
   \ExtensionTok{3022}\NormalTok{ to}
   \ExtensionTok{2567}\NormalTok{ it}
   \ExtensionTok{2086}\NormalTok{ t}
   \ExtensionTok{2044}\NormalTok{ was}
   \ExtensionTok{1847}\NormalTok{ he}
   \ExtensionTok{1778}\NormalTok{ of}
\end{Highlighting}
\end{Shaded}

Note that if you're ever in the same directory as the executable, you need to execute it as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{cd}\NormalTok{ ~/book/ch04}
\NormalTok{$ }\ExtensionTok{./top-words-2.sh}
\end{Highlighting}
\end{Shaded}

If you try to execute a file for which you do not have the correct access permissions, as with \emph{top-words-1.sh}, you will see the following error message:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{./top-words-1.sh}
\ExtensionTok{bash}\NormalTok{: ./top-words-1.sh: Permission denied}
\end{Highlighting}
\end{Shaded}

\hypertarget{step-3-define-shebang}{%
\subsection{Step 3: Define Shebang}\label{step-3-define-shebang}}

Although we can already execute the file on its own, we should add a so-called \emph{shebang} to the file. The shebang is a special line in the script, which instructs the system which executable should be used to interpret the commands.

In our case we want to use \texttt{bash} to interpret our commands. Example \ref{exm:top-words-3} shows what the file \emph{top-words-3.sh} looks like with a shebang.

\begin{example}[~/book/ch04/top-words-3.sh]
\protect\hypertarget{exm:top-words-3}{}{\label{exm:top-words-3} \iffalse (\textasciitilde{}/book/ch04/top-words-3.sh) \fi{} }
\end{example}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#!/usr/bin/env bash}
\ExtensionTok{curl}\NormalTok{ -s http://www.gutenberg.org/files/76/76-0.txt }\KeywordTok{|}
\FunctionTok{tr} \StringTok{'[:upper:]'} \StringTok{'[:lower:]'} \KeywordTok{|} \FunctionTok{grep}\NormalTok{ -oE }\StringTok{'\textbackslash{}w+'} \KeywordTok{|} \FunctionTok{sort} \KeywordTok{|}
\FunctionTok{uniq}\NormalTok{ -c }\KeywordTok{|} \FunctionTok{sort}\NormalTok{ -nr }\KeywordTok{|} \FunctionTok{head}\NormalTok{ -n 10}
\end{Highlighting}
\end{Shaded}

The name shebang comes from the first two characters: a hash (she) and an exclamation mark (bang). It is not a good idea to leave it out, as we have done in the previous step, because then the behavior of the script is undefined. The Bash shell, which is the one that we are using, uses the executable \emph{/bin/sh} by default. Other shells may have different defaults.

Sometimes you will come across scripts that have a shebang in the form of \emph{!/usr/bin/bash} or \emph{!/usr/bin/python} (in the case of Python, as we will see in the next section). While this generally works, if the \texttt{bash} or \texttt{python} \citep{python} executables are installed in a different location than \emph{/usr/bin}, then the script does not work anymore. It is better to use the form that we present here, namely \emph{!/usr/bin/env bash} and \emph{!/usr/bin/env python}, because the \texttt{env} \citep{env} executable is aware where \texttt{bash} and \texttt{python} are installed. In short, using \texttt{env} makes your scripts more portable.

\hypertarget{step-4-remove-fixed-input}{%
\subsection{Step 4: Remove Fixed Input}\label{step-4-remove-fixed-input}}

We know have a valid command-line tool that we can execute from the command line. But we can do better than this. We can make our command-line tool more reusable. The first command in our file is \texttt{curl}, which downloads the text from which we wish to obtain the top 10 most-used words. So, the data and operations are combined into one.

What if we wanted to obtain the top 10 most-used words from another e-book, or any other text for that matter? The input data is fixed within the tools itself. It would be better to separate the data from the command-line tool.

If we assume that the user of the command-line tool will provide the text, it will become generally applicable. So, the solution is to simply remove the \texttt{curl} command from the script. See Example \ref{exm:top-words-4} for the updated script named \emph{top-words-4.sh}.

\begin{example}[~/book/ch04/top-words-4.sh]
\protect\hypertarget{exm:top-words-4}{}{\label{exm:top-words-4} \iffalse (\textasciitilde{}/book/ch04/top-words-4.sh) \fi{} }
\end{example}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#!/usr/bin/env bash}
\FunctionTok{tr} \StringTok{'[:upper:]'} \StringTok{'[:lower:]'} \KeywordTok{|} \FunctionTok{grep}\NormalTok{ -oE }\StringTok{'\textbackslash{}w+'} \KeywordTok{|} \FunctionTok{sort} \KeywordTok{|}
\FunctionTok{uniq}\NormalTok{ -c }\KeywordTok{|} \FunctionTok{sort}\NormalTok{ -nr }\KeywordTok{|} \FunctionTok{head}\NormalTok{ -n 10}
\end{Highlighting}
\end{Shaded}

This works because if a script starts with a command that needs data from standard input, like \texttt{tr}, it will take the input that is given to the command-line tools. For example:

\begin{verbatim}
$ cat data/finn.txt | top-words-4.sh
\end{verbatim}

\begin{rmdtip}
Although we have not done so in our script, the same principle holds for saving data. It is, in general, better to let the user take care of that. Of course, if you intend to use a command-line tool only for own projects, then there are no limits to how specific you can be.
\end{rmdtip}

\hypertarget{step-5-parametrize}{%
\subsection{Step 5: Parametrize}\label{step-5-parametrize}}

There is one more step that we can perform in order to make our command-line tool even more reusable: parameters. In our command-line tool there are a number of fixed command-line arguments, for example \texttt{-nr} for \texttt{sort} and \texttt{-n\ 10} for \texttt{head}. It is probably best to keep the former argument fixed. However, it would be very useful to allow for different values for the \texttt{head} command. This would allow the end user to set the number of most-often used words to be outputted. Example \ref{exm:top-words-5} shows what our file \emph{top-words-5.sh} looks like if we parametrize \texttt{head}.

\begin{example}[~/book/ch04/top-words-5.sh]
\protect\hypertarget{exm:top-words-5}{}{\label{exm:top-words-5} \iffalse (\textasciitilde{}/book/ch04/top-words-5.sh) \fi{} }
\end{example}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#!/usr/bin/env bash}
\VariableTok{NUM_WORDS=}\StringTok{"}\VariableTok{$1}\StringTok{"}                                        
\FunctionTok{tr} \StringTok{'[:upper:]'} \StringTok{'[:lower:]'} \KeywordTok{|} \FunctionTok{grep}\NormalTok{ -oE }\StringTok{'\textbackslash{}w+'} \KeywordTok{|} \FunctionTok{sort} \KeywordTok{|}
\FunctionTok{uniq}\NormalTok{ -c }\KeywordTok{|} \FunctionTok{sort}\NormalTok{ -nr }\KeywordTok{|} \FunctionTok{head}\NormalTok{ -n }\VariableTok{$NUM_WORDS}               
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  The variable \emph{NUM\_WORDS} is set to the value of \emph{\$1}, which is a special variable in Bash. It holds the value of the first command-line argument passed to our command-line tool. The table below lists the other special variables that Bash offers.
\item
  Note that in order to use the value of the \emph{\$NUM\_WORDS} variable, you need to put a dollar sign in front of it. When you set it, you do not write a dollar sign.
\end{itemize}

\begin{rmdtip}
We could have also used \emph{\$1} directly as an argument for \texttt{head} and not bother creating an extra variable such \emph{NUM\_WORDS}. However, with larger scripts and a few more command-line arguments such as \emph{\$2} and \emph{\$3}, the code becomes more readable when you use named variables.
\end{rmdtip}

Now if we wanted to see the top 5 most-used words of our text, we would invoke our command-line tool as follows:

\begin{verbatim}
$ cat data/finn.txt | top-words-5 5
\end{verbatim}

If the user does not provide an argument then \texttt{head} will return an error message, because the value of \emph{\$1}, and therefore \emph{\$NUM\_WORDS} will be an empty string.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{cat}\NormalTok{ data/finn.txt }\KeywordTok{|} \ExtensionTok{top-words-5}
\ExtensionTok{head}\NormalTok{: option requires an argument -- }\StringTok{'n'}
\ExtensionTok{Try} \StringTok{'head --help'}\NormalTok{ for more information.}
\end{Highlighting}
\end{Shaded}

\hypertarget{step-6-extend-your-path}{%
\subsection{Step 6: Extend Your PATH}\label{step-6-extend-your-path}}

After the previous five steps we are finally finished building a reusable command-line tool. There is, however, one more step that can be very useful. In this optional step we are going to ensure that you can execute your command-line tools from everywhere.

Currently, when you want to execute your command-line tool, you either have to navigate to the directory it is in or include the full path name as shown in step 2. This is fine if the command-line tool is specifically built for, say, a certain project. However, if your command-line tool could be applied in multiple situations, then it is useful to be able to execute form everywhere, just like the command-line tools that come with Ubuntu.

To accomplish this, Bash needs to know where to look for your command-line tools. It does this by traversing a list of directories which are stored in an environment variable called \emph{PATH}. In a fresh Data Science Toolbox, the \emph{PATH} looks like this:

\begin{verbatim}
$ echo $PATH | fold
\end{verbatim}

The directories are delimited by colons. Here is the list of directories:

\begin{verbatim}
$ echo $PATH | tr ':' '\n'
\end{verbatim}

To change the \emph{PATH} permanently, you'll need to edit the \emph{.bashrc} or \emph{.profile} file located in your home directory. If you put all your custom command-line tools into one directory, say, \emph{\textasciitilde{}/tools}, then you only change the \emph{PATH} once. As you can see, the Data Science Toolbox already has \emph{/home/vagrant/.bin} in its \emph{PATH}. Now, you no longer need to add the \emph{./}, but you can just use the filename. Moreover, you do no longer need to remember where the command-line tool is located.

\hypertarget{creating-command-line-tools-with-python-and-r}{%
\section{Creating Command-line Tools with Python and R}\label{creating-command-line-tools-with-python-and-r}}

The command-line tool that we created in the previous section was written in Bash. (Sure, not every feature of the Bash language was employed, but the interpreter still was \texttt{bash}.) As you may know by now, the command line is language agnostic, so we do not necessarily have to use Bash for creating command-line tools.

In this section we are going demonstrate that command-line tools can be created in other programming languages as well. We will focus on Python and R because these are currently the two most popular programming languages within the data science community. We cannot offer a complete introduction to either language, so we assume that you have some familiarity with Python and or R. Programming languages such as Java, Go, and Julia, follow a similar pattern when it comes to creating command-line tools.

There are three main reasons for creating command-line tools in a programming language instead of Bash. First, you may have existing code that you wish be able to use from the command line. Second, the command-line tool would end up encompassing more than a hundred lines of code. Third, the command-line tool needs to be very fast.

The six steps that we discussed in the previous section roughly apply to creating command-line tools in other programming languages as well. The first step, however, would not be copy pasting from the command line, but rather copy pasting the relevant code into a new file. Command-line tools in Python and R need to specify \texttt{python} \citep{python} and \texttt{Rscript} \citep{R}, respectively, as the interpreter after the shebang.

When it comes to creating command-line tools using Python and R, there are two more aspects that deserve special attention, which will be discuss below. First, processing standard input, which comes natural to shell scripts, has to be taken care of explicitly in Python and R. Second, as command-line tools written in Python and R tend to be more complex, we may also want to offer the user the ability to specify more complex command-line arguments.

\hypertarget{porting-the-shell-script}{%
\subsection{Porting The Shell Script}\label{porting-the-shell-script}}

As a starting point, let's see how we would port the prior shell script to both Python and R. In other words, what Python and R code gives us the top most-often used words from standard input? It is not important whether implementing this task in anything else than a shell programming language is a good idea. What matters is that it gives us a good opportunity to compare Bash with Python and R.

We will first show the two files \emph{top-words.py} and \emph{top-words.R} and then discuss the differences with the shell code. In Python, the code could would look something like Example \ref{exm:top-words-py}.

\begin{example}[~/book/ch04/top-words.py]
\protect\hypertarget{exm:top-words-py}{}{\label{exm:top-words-py} \iffalse (\textasciitilde{}/book/ch04/top-words.py) \fi{} }
\end{example}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#!/usr/bin/env python}
\ImportTok{import}\NormalTok{ re}
\ImportTok{import}\NormalTok{ sys}
\ImportTok{from}\NormalTok{ collections }\ImportTok{import}\NormalTok{ Counter}
\NormalTok{num_words }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(sys.argv[}\DecValTok{1}\NormalTok{])}
\NormalTok{text }\OperatorTok{=}\NormalTok{ sys.stdin.read().lower()}
\NormalTok{words }\OperatorTok{=}\NormalTok{ re.split(}\StringTok{'\textbackslash{}W+'}\NormalTok{, text)}
\NormalTok{cnt }\OperatorTok{=}\NormalTok{ Counter(words)}
\ControlFlowTok{for}\NormalTok{ word, count }\KeywordTok{in}\NormalTok{ cnt.most_common(num_words):}
    \BuiltInTok{print} \StringTok{"}\SpecialCharTok{%7d}\StringTok{ }\SpecialCharTok{%s}\StringTok{"} \OperatorTok{%}\NormalTok{ (count, word)}
\end{Highlighting}
\end{Shaded}

\begin{rmdnote}
Example
\ref{exm:top-words-py}
uses pure Python. When you want to do advanced text processing we recommend you check out the NLTK package \citep{Perkins2010}. If you are going to work with a lot of numerical data, then we recommend you use the Pandas package \citep{McKinney2012}.
\end{rmdnote}

And in R, the code would look something like Example \ref{exm:top-words-5} (thanks to Hadley Wickham):

\begin{example}[~/book/ch04/top-words.R]
\protect\hypertarget{exm:top-words-5}{}{\label{exm:top-words-5} \iffalse (\textasciitilde{}/book/ch04/top-words.R) \fi{} }
\end{example}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#!/usr/bin/env Rscript}
\NormalTok{n <-}\StringTok{ }\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{commandArgs}\NormalTok{(}\DataTypeTok{trailingOnly =} \OtherTok{TRUE}\NormalTok{))}
\NormalTok{f <-}\StringTok{ }\KeywordTok{file}\NormalTok{(}\StringTok{"stdin"}\NormalTok{)}
\NormalTok{lines <-}\StringTok{ }\KeywordTok{readLines}\NormalTok{(f)}
\NormalTok{words <-}\StringTok{ }\KeywordTok{tolower}\NormalTok{(}\KeywordTok{unlist}\NormalTok{(}\KeywordTok{strsplit}\NormalTok{(lines, }\StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{W+"}\NormalTok{)))}
\NormalTok{counts <-}\StringTok{ }\KeywordTok{sort}\NormalTok{(}\KeywordTok{table}\NormalTok{(words), }\DataTypeTok{decreasing =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{counts_n <-}\StringTok{ }\NormalTok{counts[}\DecValTok{1}\OperatorTok{:}\NormalTok{n]}
\KeywordTok{cat}\NormalTok{(}\KeywordTok{sprintf}\NormalTok{(}\StringTok{"%7d %s}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, counts_n, }\KeywordTok{names}\NormalTok{(counts_n)), }\DataTypeTok{sep =} \StringTok{""}\NormalTok{)}
\KeywordTok{close}\NormalTok{(f)}
\end{Highlighting}
\end{Shaded}

Let's check that all three implementations (i.e., Bash, Python, and R) return the same top 5 words with the same counts:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{data/76.txt}\NormalTok{ top-words.sh 5}
   \ExtensionTok{6441}\NormalTok{ and}
   \ExtensionTok{5082}\NormalTok{ the}
   \ExtensionTok{3666}\NormalTok{ i}
   \ExtensionTok{3258}\NormalTok{ a}
   \ExtensionTok{3022}\NormalTok{ to}
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{data/76.txt}\NormalTok{ top-words.py 5}
   \ExtensionTok{6441}\NormalTok{ and}
   \ExtensionTok{5082}\NormalTok{ the}
   \ExtensionTok{3666}\NormalTok{ i}
   \ExtensionTok{3258}\NormalTok{ a}
   \ExtensionTok{3022}\NormalTok{ to}
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{data/76.txt}\NormalTok{ top-words.R 5}
   \ExtensionTok{6441}\NormalTok{ and}
   \ExtensionTok{5082}\NormalTok{ the}
   \ExtensionTok{3666}\NormalTok{ i}
   \ExtensionTok{3258}\NormalTok{ a}
   \ExtensionTok{3022}\NormalTok{ to}
\end{Highlighting}
\end{Shaded}

Wonderful! Sure, the output itself is not very exciting. What is exciting is the observation that we can accomplish the same task with multiple approaches. Let's have a look at the differences between the approaches.

First, what's immediately obvious is the difference in amount of code. For this specific task, both Python and R require much more code than Bash. This illustrates that, for some tasks, it is better to use the command line. For other tasks, you may better off using a programming language. As you gain more experience on the command-line, you will start to recognize when to use which approach. When everything is a command-line tool, you can even split up the task into subtasks, and combine a Bash command-line tool with a, say, Python command-line tool. Whichever approach works best for the task at hand.

\hypertarget{processing-streaming-data-from-standard-input}{%
\subsection{Processing Streaming Data from Standard Input}\label{processing-streaming-data-from-standard-input}}

In the previous two code snippets, both Python and R read the complete standard input at once. On the command line, most command-line tools pipe data to the next command-line tool in a streaming fashion. (There are a few command-line tools which require the complete data before they write any data to standard output, like \texttt{sort} and \texttt{awk} \citep{awk}.) This means the pipeline is blocked by such command-line tools. This does not have to be a problem when the input data is finite, like a file. However, when the input data is a non-stop stream, such blocking command-line tools are useless.

Luckily Python and R support processing streaming data. You can apply a function on a line-per-line basis, for example. Example \ref{exm:stream-py} and Example \ref{exm:stream-r} are two minimal examples that demonstrate how this works in Python and R, respectively.

\begin{example}[~/book/ch04/stream.py]
\protect\hypertarget{exm:stream-py}{}{\label{exm:stream-py} \iffalse (\textasciitilde{}/book/ch04/stream.py) \fi{} }
\end{example}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#!/usr/bin/env python}
\ImportTok{from}\NormalTok{ sys }\ImportTok{import}\NormalTok{ stdin, stdout}
\ControlFlowTok{while} \VariableTok{True}\NormalTok{:}
\NormalTok{    line }\OperatorTok{=}\NormalTok{ stdin.readline()}
    \ControlFlowTok{if} \KeywordTok{not}\NormalTok{ line:}
        \ControlFlowTok{break}
\NormalTok{    stdout.write(}\StringTok{"}\SpecialCharTok{%d}\CharTok{\textbackslash{}n}\StringTok{"} \OperatorTok{%} \BuiltInTok{int}\NormalTok{(line)}\OperatorTok{**}\DecValTok{2}\NormalTok{)}
\NormalTok{    stdout.flush()}
\end{Highlighting}
\end{Shaded}

\begin{example}[~/book/ch04/stream.R]
\protect\hypertarget{exm:stream-r}{}{\label{exm:stream-r} \iffalse (\textasciitilde{}/book/ch04/stream.R) \fi{} }
\end{example}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#!/usr/bin/env Rscript}
\NormalTok{f <-}\StringTok{ }\KeywordTok{file}\NormalTok{(}\StringTok{"stdin"}\NormalTok{)}
\KeywordTok{open}\NormalTok{(f)}
\ControlFlowTok{while}\NormalTok{(}\KeywordTok{length}\NormalTok{(line <-}\StringTok{ }\KeywordTok{readLines}\NormalTok{(f, }\DataTypeTok{n =} \DecValTok{1}\NormalTok{)) }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{) \{}
        \KeywordTok{write}\NormalTok{(}\KeywordTok{as.integer}\NormalTok{(line)}\OperatorTok{^}\DecValTok{2}\NormalTok{, }\KeywordTok{stdout}\NormalTok{())}
\NormalTok{\}}
\KeywordTok{close}\NormalTok{(f)}
\end{Highlighting}
\end{Shaded}

\hypertarget{further-reading}{%
\section{Further Reading}\label{further-reading}}

\begin{itemize}
\tightlist
\item
  Docopt. 2014. ``Command-Line Interface Description Language.'' http://docopt.org.
\item
  Robbins, Arnold, and Nelson H. F. Beebe. 2005. Classic Shell Scripting. O'Reilly Media.
\item
  Peek, Jerry, Shelley Powers, Tim O'Reilly, and Mike Loukides. 2002. Unix Power Tools. 3rd Ed. O'Reilly Media.
\item
  Perkins, Jacob. 2010. Python Text Processing with Nltk 2.0 Cookbook. Packt Publishing.
\item
  McKinney, Wes. 2012. Python for Data Analysis. O'Reilly Media.
\item
  Rossant, Cyrille. 2013. Learning Ipython for Interactive Computing and Data Visualization. Packt Publishing.
\item
  Wirzenius, Lars. 2013. ``Writing Manual Pages.'' http://liw.fi/manpages/.
\item
  Raymond, Eric Steven. 2014. ``Basics of the Unix Philosophy.'' http://www.faqs.org/docs/artu/ch01s06.html.
\end{itemize}

\hypertarget{chapter-5-scrubbing-data}{%
\chapter{Scrubbing Data}\label{chapter-5-scrubbing-data}}

Two chapters ago, in Step 1 of the OSEMN model for data science, we looked at how to obtain data from a variety of sources. It's not uncommon for this data to have missing values, inconsistencies, errors, weird characters, or uninteresting columns. Sometimes we only need a specific portion of the data. And sometimes we need the data to be in a different format. In those cases, we have to scrub, or clean, the data before we can move on to Step 3: Exploring Data.

The data we obtained in Chapter 3 can come in a variety of formats. The most common ones are plain text, CSV, JSON, and HTML/XML. Since most command-line tools operate on one format only, it is worthwhile to be able to convert data from one format to another.

CSV, which is the main format we're working with in this chapter, is actually not the easiest format to work with. Many CSV data sets are broken or incompatible with each other because there is no standard syntax, unlike XML and JSON.

Once our data is in the format we want it to be, we can apply common scrubbing operations. These include filtering, replacing, and merging data. The command line is especially well-suited for these kind of operations, as there exist many powerful command-line tools that are optimized for handling large amounts of data. Tools that we'll discuss in this chapter include classic ones such as: \texttt{cut} \citep{cut} and \texttt{sed} \citep{sed}, and newer ones such as \texttt{jq} \citep{jq} and \texttt{csvgrep} \citep{csvgrep}.

The scrubbing tasks that we discuss in this chapter not only apply to the input data. Sometimes, we also need to reformat the output of some command-line tools. For example, to transform the output of \texttt{uniq\ -c} to a CSV data set, we could use \texttt{awk} \citep{awk} and \texttt{header}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{printf} \StringTok{'foo\textbackslash{}nbar\textbackslash{}nfoo'} \KeywordTok{|} \FunctionTok{sort} \KeywordTok{|} \FunctionTok{uniq}\NormalTok{ -c }\KeywordTok{|} \FunctionTok{sort}\NormalTok{ -nr}
      \ExtensionTok{2}\NormalTok{ foo}
      \ExtensionTok{1}\NormalTok{ bar}
\NormalTok{$ }\BuiltInTok{printf} \StringTok{'foo\textbackslash{}nbar\textbackslash{}nfoo'} \KeywordTok{|} \FunctionTok{sort} \KeywordTok{|} \FunctionTok{uniq}\NormalTok{ -c }\KeywordTok{|} \FunctionTok{sort}\NormalTok{ -nr }\KeywordTok{|}
\OperatorTok{>} \FunctionTok{awk} \StringTok{'\{print $2","$1\}'} \KeywordTok{|} \ExtensionTok{header}\NormalTok{ -a }\StringTok{'value, count'}
\ExtensionTok{value}\NormalTok{,count}
\ExtensionTok{foo}\NormalTok{,2}
\ExtensionTok{bar}\NormalTok{,1}
\end{Highlighting}
\end{Shaded}

If your data requires additional functionality than that is offered by (a combination of) these command-line tools, you can use \texttt{csvsql}. This is a new command-line tool that allow you to perform SQL queries directly on CSV files. And remember, if after reading this chapter you still need more flexibility, you're free to use R, Python, or whatever programming language you prefer.

The command-line tools will be introduced on a need-to-use basis. You will notice that sometimes we can use the same command-line tool to perform multiple operations, or vice versa, multiple command-line tools to perform the same operation. This chapter is more structured like a cookbook, where the focus is on the problems or recipes, rather than on the command-line tools.

\hypertarget{overview}{%
\section{Overview}\label{overview}}

In this chapter, you'll learn how to:

\begin{itemize}
\item
  Convert data from one format to another.
\item
  Apply SQL queries to CSV.
\item
  Filter lines.
\item
  Extract and replace values.
\item
  Split, merge, and extract columns.
\end{itemize}

\hypertarget{common-scrub-operations-for-plain-text}{%
\section{Common Scrub Operations for Plain Text}\label{common-scrub-operations-for-plain-text}}

In this section we describe common scrubbing operations for plain text. Formally, plain text refers to a sequence of human-readable characters and optionally, some specific types of control characters (for example a tab or a newline) (see: \url{http://www.linfo.org/plain_text.html}). Examples include: e-books, emails, log files, and source code.

For the purpose of this book, we assume that the plain text contains some data, and that it has no clear tabular structure (like the CSV format) or nested structure (like the JSON and HTML formats). We discuss those formats later in this chapter. Although these operations can also be applied to CSV, JSON and XML/HTML formats, keep in mind that the tools treat the data as plain text.

\hypertarget{filtering-lines}{%
\subsection{Filtering Lines}\label{filtering-lines}}

The first scrubbing operation is filtering lines. This means that from the input data, each line will be evaluated whether it may be passed on as output.

\hypertarget{based-on-location}{%
\subsubsection{Based on Location}\label{based-on-location}}

The most straightforward way to filter lines is based on their location. This may be useful when you want to inspect, say, the top 10 lines of a file, or when you extract a specific row from the output of another command-line tool. To illustrate how to filter based on location, let's create a dummy file that contains 10 lines:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{seq}\NormalTok{ -f }\StringTok{"Line %g"}\NormalTok{ 10 }\KeywordTok{|} \FunctionTok{tee}\NormalTok{ data/lines}
\ExtensionTok{Line}\NormalTok{ 1}
\ExtensionTok{Line}\NormalTok{ 2}
\ExtensionTok{Line}\NormalTok{ 3}
\ExtensionTok{Line}\NormalTok{ 4}
\ExtensionTok{Line}\NormalTok{ 5}
\ExtensionTok{Line}\NormalTok{ 6}
\ExtensionTok{Line}\NormalTok{ 7}
\ExtensionTok{Line}\NormalTok{ 8}
\ExtensionTok{Line}\NormalTok{ 9}
\ExtensionTok{Line}\NormalTok{ 10}
\end{Highlighting}
\end{Shaded}

We can print the first 3 lines using either \texttt{head}, \texttt{sed}, or \texttt{awk}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{lines}\NormalTok{ head -n 3}
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{lines}\NormalTok{ sed -n }\StringTok{'1,3p'}
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{lines}\NormalTok{ awk }\StringTok{'NR<=3'}
\ExtensionTok{Line}\NormalTok{ 1}
\ExtensionTok{Line}\NormalTok{ 2}
\ExtensionTok{Line}\NormalTok{ 3}
\end{Highlighting}
\end{Shaded}

Similarly, we can print the last 3 lines using \texttt{tail} \citep{tail}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{lines}\NormalTok{ tail -n 3}
\ExtensionTok{Line}\NormalTok{ 8}
\ExtensionTok{Line}\NormalTok{ 9}
\ExtensionTok{Line}\NormalTok{ 10}
\end{Highlighting}
\end{Shaded}

You can also you use \texttt{sed} and \texttt{awk} for this, but \texttt{tail} is much faster.

Removing the first 3 lines goes as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{lines}\NormalTok{ tail -n +4}
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{lines}\NormalTok{ sed }\StringTok{'1,3d'}
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{lines}\NormalTok{ sed -n }\StringTok{'1,3!p'}
\ExtensionTok{Line}\NormalTok{ 4}
\ExtensionTok{Line}\NormalTok{ 5}
\ExtensionTok{Line}\NormalTok{ 6}
\ExtensionTok{Line}\NormalTok{ 7}
\ExtensionTok{Line}\NormalTok{ 8}
\ExtensionTok{Line}\NormalTok{ 9}
\ExtensionTok{Line}\NormalTok{ 10}
\end{Highlighting}
\end{Shaded}

Please notice that with tail you have to add one.

Removing the last 3 lines can be done with \texttt{head}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{lines}\NormalTok{ head -n -3}
\ExtensionTok{Line}\NormalTok{ 1}
\ExtensionTok{Line}\NormalTok{ 2}
\ExtensionTok{Line}\NormalTok{ 3}
\ExtensionTok{Line}\NormalTok{ 4}
\ExtensionTok{Line}\NormalTok{ 5}
\ExtensionTok{Line}\NormalTok{ 6}
\ExtensionTok{Line}\NormalTok{ 7}
\end{Highlighting}
\end{Shaded}

You can print (or extract) specific lines (4, 5, and 6 in this case) using a either \texttt{sed}, \texttt{awk}, or a combination of \texttt{head} and \texttt{tail}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{lines}\NormalTok{ sed -n }\StringTok{'4,6p'}
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{lines}\NormalTok{ awk }\StringTok{'(NR>=4)&&(NR<=6)'}
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{lines}\NormalTok{ head -n 6 }\KeywordTok{|} \FunctionTok{tail}\NormalTok{ -n 3}
\ExtensionTok{Line}\NormalTok{ 4}
\ExtensionTok{Line}\NormalTok{ 5}
\ExtensionTok{Line}\NormalTok{ 6}
\end{Highlighting}
\end{Shaded}

Print odd lines with \texttt{sed} by specifying a start and a step, or with \texttt{awk} by using the modulo operator:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{lines}\NormalTok{ sed -n }\StringTok{'1~2p'}
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{lines}\NormalTok{ awk }\StringTok{'NR%2'}
\ExtensionTok{Line}\NormalTok{ 1}
\ExtensionTok{Line}\NormalTok{ 3}
\ExtensionTok{Line}\NormalTok{ 5}
\ExtensionTok{Line}\NormalTok{ 7}
\ExtensionTok{Line}\NormalTok{ 9}
\end{Highlighting}
\end{Shaded}

Printing even lines works in a similar manner:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{lines}\NormalTok{ sed -n }\StringTok{'0~2p'}
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{lines}\NormalTok{ awk }\StringTok{'(NR+1)%2'}
\ExtensionTok{Line}\NormalTok{ 2}
\ExtensionTok{Line}\NormalTok{ 4}
\ExtensionTok{Line}\NormalTok{ 6}
\ExtensionTok{Line}\NormalTok{ 8}
\ExtensionTok{Line}\NormalTok{ 10}
\end{Highlighting}
\end{Shaded}

\hypertarget{based-on-pattern}{%
\subsubsection{Based on Pattern}\label{based-on-pattern}}

Sometimes you want to extract or remove lines based on their contents. Using \texttt{grep}, the canonical command-line tool for filtering lines, we can print every line that matches a certain pattern or regular expression. For example, to extract all the chapter headings from \emph{Alice's Adventures in Wonderland}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{grep}\NormalTok{ -i chapter alice.txt}
\ExtensionTok{CHAPTER}\NormalTok{ I. Down the Rabbit-Hole}
\ExtensionTok{CHAPTER}\NormalTok{ II. The Pool of Tears}
\ExtensionTok{CHAPTER}\NormalTok{ III. A Caucus-Race and a Long Tale}
\ExtensionTok{CHAPTER}\NormalTok{ IV. The Rabbit Sends in a Little Bill}
\ExtensionTok{CHAPTER}\NormalTok{ V. Advice from a Caterpillar}
\ExtensionTok{CHAPTER}\NormalTok{ VI. Pig and Pepper}
\ExtensionTok{CHAPTER}\NormalTok{ VII. A Mad Tea-Party}
\ExtensionTok{CHAPTER}\NormalTok{ VIII. The Queen}\StringTok{'s Croquet-Ground}
\StringTok{CHAPTER IX. The Mock Turtle'}\NormalTok{s Story}
\ExtensionTok{CHAPTER}\NormalTok{ X. The Lobster Quadrille}
\ExtensionTok{CHAPTER}\NormalTok{ XI. Who Stole the Tarts?}
\ExtensionTok{CHAPTER}\NormalTok{ XII. Alice}\StringTok{'s Evidence}
\end{Highlighting}
\end{Shaded}

Here, \texttt{-i} means case-insensitive. We can also specify a regular expression. For example, if we only wanted to print out the headings which start with \emph{The}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{grep}\NormalTok{ -E }\StringTok{'^CHAPTER (.*)\textbackslash{}. The'}\NormalTok{ alice.txt}
\ExtensionTok{CHAPTER}\NormalTok{ II. The Pool of Tears}
\ExtensionTok{CHAPTER}\NormalTok{ IV. The Rabbit Sends in a Little Bill}
\ExtensionTok{CHAPTER}\NormalTok{ VIII. The Queen}\StringTok{'s Croquet-Ground}
\StringTok{CHAPTER IX. The Mock Turtle'}\NormalTok{s Story}
\ExtensionTok{CHAPTER}\NormalTok{ X. The Lobster Quadrille}
\end{Highlighting}
\end{Shaded}

Please note that you have to specify the \texttt{-E} command-line argument in order to enable regular expressions. Otherwise, \texttt{grep} interprets the pattern as a literal string.

\hypertarget{based-on-randomness}{%
\subsubsection{Based on Randomness}\label{based-on-randomness}}

When you're in the process of formulating your data pipeline and you have a lot of data, then debugging your pipeline can be cumbersome. In that case, sampling from the data might be useful. The main purpose of the command-line tool \texttt{sample} \citep{sample} is to get a subset of the data by outputting only a certain percentage of the input on a line-by-line basis.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{seq}\NormalTok{ 1000 }\KeywordTok{|} \ExtensionTok{sample}\NormalTok{ -r 1% }\KeywordTok{|} \ExtensionTok{jq}\NormalTok{ -c }\StringTok{'\{line: .\}'}
\DataTypeTok{\{"line":53\}}
\DataTypeTok{\{"line":119\}}
\DataTypeTok{\{"line":141\}}
\DataTypeTok{\{"line":228\}}
\DataTypeTok{\{"line":464\}}
\DataTypeTok{\{"line":476\}}
\DataTypeTok{\{"line":523\}}
\DataTypeTok{\{"line":657\}}
\DataTypeTok{\{"line":675\}}
\DataTypeTok{\{"line":865\}}
\DataTypeTok{\{"line":948\}}
\end{Highlighting}
\end{Shaded}

Here, every input line has a one percent chance of being forwarded to \texttt{jq}. This percentage could also have been specified as a fraction (\emph{1/100}) or as a probability (\emph{0.01}).

\texttt{sample} has two other purposes, which can be useful when you're in debugging. First, it's possible to add some delay to the output. This comes in handy when the input is a constant stream (for example, the Twitter firehose), and the data comes in too fast to see what's going on. Secondly, you can put a timer on \texttt{sample}. This way, you don't have to kill the ongoing process manually. To add a 1 second delay between each output line to the previous command and to only run for 5 seconds:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{seq}\NormalTok{ 10000 }\KeywordTok{|} \ExtensionTok{sample}\NormalTok{ -r 1% -d 1000 -s 5 }\KeywordTok{|} \ExtensionTok{jq}\NormalTok{ -c }\StringTok{'\{line: .\}'}
\end{Highlighting}
\end{Shaded}

In order to prevent unnecessary computation, try to put \texttt{sample} as early as possible in your pipeline (this argument holds any command-line tool that reduces data, like \texttt{head} and \texttt{tail}). Once you're done debugging you can simply take it out of the pipeline.

\hypertarget{extracting-values}{%
\subsection{Extracting Values}\label{extracting-values}}

To extract the actual chapter headings from our example earlier, we can take a simple approach by piping the output of \texttt{grep} to \texttt{cut}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{grep}\NormalTok{ -i chapter alice.txt }\KeywordTok{|} \FunctionTok{cut}\NormalTok{ -d}\StringTok{' '}\NormalTok{ -f3-}
\ExtensionTok{Down}\NormalTok{ the Rabbit-Hole}
\ExtensionTok{The}\NormalTok{ Pool of Tears}
\ExtensionTok{A}\NormalTok{ Caucus-Race and a Long Tale}
\ExtensionTok{The}\NormalTok{ Rabbit Sends in a Little Bill}
\ExtensionTok{Advice}\NormalTok{ from a Caterpillar}
\ExtensionTok{Pig}\NormalTok{ and Pepper}
\ExtensionTok{A}\NormalTok{ Mad Tea-Party}
\ExtensionTok{The}\NormalTok{ Queen}\StringTok{'s Croquet-Ground}
\StringTok{The Mock Turtle'}\NormalTok{s Story}
\ExtensionTok{The}\NormalTok{ Lobster Quadrille}
\ExtensionTok{Who}\NormalTok{ Stole the Tarts?}
\ExtensionTok{Alice}\StringTok{'s Evidence}
\end{Highlighting}
\end{Shaded}

Here, each line that's passed to \texttt{cut} is being split on spaces into fields, and then the third field to the last field is being printed. The total number of fields may be different per input line. With \texttt{sed} we can accomplish the same task in a much more complex manner:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{sed}\NormalTok{ -rn }\StringTok{'s/^CHAPTER ([IVXLCDM]\{1,\})\textbackslash{}. (.*)$/\textbackslash{}2/p'}\NormalTok{ alice.txt }\OperatorTok{>}\NormalTok{ /dev/null}
\end{Highlighting}
\end{Shaded}

(Since the output is the same it's omitted by redirecting it to \emph{/dev/null}.) This approach uses a regular expression and a back reference. Here, \texttt{sed} also takes over the work done by \texttt{grep}. This complex approach is only advisable when a simpler one would not work. For example, if \emph{chapter} was ever part of the text itself and not just used to indicate the start of a new chapter. Of course there are many levels of complexity which would have worked around this, but this was to illustrate an extremely strict approach. In practice, the challenge is to find a good balance between complexity and flexibility.

It's worth noting that \texttt{cut} can also split on characters positions. This is useful for when you want to extract (or remove) the same set of characters per input line:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{grep}\NormalTok{ -i chapter alice.txt }\KeywordTok{|} \FunctionTok{cut}\NormalTok{ -c 9-}
\ExtensionTok{I.}\NormalTok{ Down the Rabbit-Hole}
\ExtensionTok{II.}\NormalTok{ The Pool of Tears}
\ExtensionTok{III.}\NormalTok{ A Caucus-Race and a Long Tale}
\ExtensionTok{IV.}\NormalTok{ The Rabbit Sends in a Little Bill}
\ExtensionTok{V.}\NormalTok{ Advice from a Caterpillar}
\ExtensionTok{VI.}\NormalTok{ Pig and Pepper}
\ExtensionTok{VII.}\NormalTok{ A Mad Tea-Party}
\ExtensionTok{VIII.}\NormalTok{ The Queen}\StringTok{'s Croquet-Ground}
\StringTok{IX. The Mock Turtle'}\NormalTok{s Story}
\ExtensionTok{X.}\NormalTok{ The Lobster Quadrille}
\ExtensionTok{XI.}\NormalTok{ Who Stole the Tarts?}
\ExtensionTok{XII.}\NormalTok{ Alice}\StringTok{'s Evidence}
\end{Highlighting}
\end{Shaded}

\texttt{grep} has a great feature that outputs every match onto a separate line:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{alice.txt}\NormalTok{ grep -oE }\StringTok{'\textbackslash{}w\{2,\}'} \KeywordTok{|} \FunctionTok{head}
\ExtensionTok{Project}
\ExtensionTok{Gutenberg}
\ExtensionTok{Alice}
\ExtensionTok{Adventures}
\KeywordTok{in}
\ExtensionTok{Wonderland}
\ExtensionTok{by}
\ExtensionTok{Lewis}
\ExtensionTok{Carroll}
\ExtensionTok{This}
\end{Highlighting}
\end{Shaded}

But what if we wanted to create a data set of all the words that start with an \emph{a} and end with an \emph{e}. Well, of course there's a pipeline for that too:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{alice.txt}\NormalTok{ tr }\StringTok{'[:upper:]'} \StringTok{'[:lower:]'} \KeywordTok{|} \FunctionTok{grep}\NormalTok{ -oE }\StringTok{'\textbackslash{}w\{2,\}'} \KeywordTok{|}
\OperatorTok{>} \FunctionTok{grep}\NormalTok{ -E }\StringTok{'^a.*e$'} \KeywordTok{|} \FunctionTok{sort} \KeywordTok{|} \FunctionTok{uniq}\NormalTok{ -c }\KeywordTok{|} \FunctionTok{sort}\NormalTok{ -nr }\KeywordTok{|}
\OperatorTok{>} \FunctionTok{awk} \StringTok{'\{print $2","$1\}'} \KeywordTok{|} \ExtensionTok{header}\NormalTok{ -a word,count }\KeywordTok{|} \FunctionTok{head} \KeywordTok{|} \ExtensionTok{csvlook}
\KeywordTok{|}\ExtensionTok{-------------+--------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{word}       \KeywordTok{|} \ExtensionTok{count}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{-------------+--------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{alice}      \KeywordTok{|} \ExtensionTok{403}    \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{are}        \KeywordTok{|} \ExtensionTok{73}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{archive}    \KeywordTok{|} \ExtensionTok{13}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{agree}      \KeywordTok{|} \ExtensionTok{11}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{anyone}     \KeywordTok{|} \ExtensionTok{5}      \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{alone}      \KeywordTok{|} \ExtensionTok{5}      \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{age}        \KeywordTok{|} \ExtensionTok{4}      \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{applicable} \KeywordTok{|} \ExtensionTok{3}      \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{anywhere}   \KeywordTok{|} \ExtensionTok{3}      \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{alive}      \KeywordTok{|} \ExtensionTok{3}      \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{-------------+--------}\KeywordTok{|}
\end{Highlighting}
\end{Shaded}

\hypertarget{replacing-and-deleting-values}{%
\subsection{Replacing and Deleting Values}\label{replacing-and-deleting-values}}

You can use the command-line tool \texttt{tr}, which stands for translate, to replace individual characters. For example, spaces can be replaced by underscores as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{echo} \StringTok{'hello world!'} \KeywordTok{|} \FunctionTok{tr} \StringTok{' '} \StringTok{'_'}
\ExtensionTok{hello_world}\NormalTok{!}
\end{Highlighting}
\end{Shaded}

If more than one character needs to be replaced, then you can combine that:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{echo} \StringTok{'hello world!'} \KeywordTok{|} \FunctionTok{tr} \StringTok{' !'} \StringTok{'_?'}
\ExtensionTok{hello_world?}
\end{Highlighting}
\end{Shaded}

\texttt{tr} can also be used to delete individual characters by specifying the argument \texttt{-d}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{echo} \StringTok{'hello world!'} \KeywordTok{|} \FunctionTok{tr}\NormalTok{ -d -c }\StringTok{'[a-z]'}
\ExtensionTok{helloworld}
\end{Highlighting}
\end{Shaded}

Here, we've actually used two more features. First we've specified a set of characters (all lowercase letters). Second we've indicated that the complement \texttt{-c} should be used. In other words, this command only retains lowercase letters. We can even use \texttt{tr} to convert our text to uppercase:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{echo} \StringTok{'hello world!'} \KeywordTok{|} \FunctionTok{tr} \StringTok{'[a-z]'} \StringTok{'[A-Z]'}
\ExtensionTok{HELLO}\NormalTok{ WORLD!}
\NormalTok{$ }\BuiltInTok{echo} \StringTok{'hello world!'} \KeywordTok{|} \FunctionTok{tr} \StringTok{'[:lower:]'} \StringTok{'[:upper:]'}
\ExtensionTok{HELLO}\NormalTok{ WORLD!}
\end{Highlighting}
\end{Shaded}

The latter command is preferable because that also handles non-ASCII characters. If you need to operate on more than individual characters, then you may find \texttt{sed} useful. We've already seen an example of \texttt{sed} with extracting the chapter headings from Alice in Wonderland. Extracting, deleting, and replacing is actually all the same operation in \texttt{sed}. You just specify different regular expressions. For example, to change a word, remove repeated spaces, and remove leading spaces:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{echo} \StringTok{' hello     world!'} \KeywordTok{|} \FunctionTok{sed}\NormalTok{ -re }\StringTok{'s/hello/bye/;s/\textbackslash{}s+/ /g;s/\textbackslash{}s+//'}
\ExtensionTok{bye}\NormalTok{ world!}
\end{Highlighting}
\end{Shaded}

The argument \texttt{-g} stands for global, meaning that the same command can be applied more than once on the same line. We do not need that with the second command, which removes leading spaces. Note that regular expressions of the first and the last command could have been combined into one regular expression.

\hypertarget{working-with-csv}{%
\section{Working with CSV}\label{working-with-csv}}

\hypertarget{bodies-and-headers-and-columns-oh-my}{%
\subsection{Bodies and Headers and Columns, Oh My!}\label{bodies-and-headers-and-columns-oh-my}}

The command-line tools that we've used to scrub plain text, such as \texttt{tr} and \texttt{grep}, cannot always be applied to CSV. The reason is that these command-line tools have no notion of headers, bodies, and columns. What if we wanted to filter lines using \texttt{grep} but always include the header in the output? Or what if we only wanted to uppercase the values of a specific column using \texttt{tr} and leave the other columns untouched? There are multi-step workarounds for this, but they are very cumbersome. We have something better. In order to leverage ordinary command-line tools for CSV, we'd like to introduce you to three command-line tools, aptly named: \texttt{body} \citep{body}, \texttt{header} \citep{header}, and \texttt{cols} \citep{cols}.

Let's start with the first command-line tool, \texttt{body}. With \texttt{body} you can apply any command-line tool to the body of a CSV file, that is, everything excluding the header. For example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{echo}\NormalTok{ -e }\StringTok{"value\textbackslash{}n7\textbackslash{}n2\textbackslash{}n5\textbackslash{}n3"} \KeywordTok{|} \ExtensionTok{body}\NormalTok{ sort -n}
\ExtensionTok{value}
\ExtensionTok{2}
\ExtensionTok{3}
\ExtensionTok{5}
\ExtensionTok{7}
\end{Highlighting}
\end{Shaded}

It assumes that the header of the CSV file only spans one row. Here's the source code for completeness:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#!/usr/bin/env bash}
\VariableTok{IFS=} \BuiltInTok{read}\NormalTok{ -r }\VariableTok{header}        
\BuiltInTok{printf} \StringTok{'%s\textbackslash{}n'} \StringTok{"}\VariableTok{$header}\StringTok{"}    
\VariableTok{$@}                         
\end{Highlighting}
\end{Shaded}

It works like this:

\begin{itemize}
\tightlist
\item
  Take one line from standard in and store it as a variable named \emph{\$header}.
\item
  Print out the header.
\item
  Execute all the command-line arguments passed to \texttt{body} on the remaining data in standard in.
\end{itemize}

Here's another example. Imagine that we count the lines of the following CSV file:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{seq}\NormalTok{ 5 }\KeywordTok{|} \ExtensionTok{header}\NormalTok{ -a count}
\ExtensionTok{count}
\ExtensionTok{1}
\ExtensionTok{2}
\ExtensionTok{3}
\ExtensionTok{4}
\ExtensionTok{5}
\end{Highlighting}
\end{Shaded}

With \texttt{wc\ -l}, we can count the number of all lines:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{seq}\NormalTok{ 5 }\KeywordTok{|} \ExtensionTok{header}\NormalTok{ -a count }\KeywordTok{|} \FunctionTok{wc}\NormalTok{ -l}
\ExtensionTok{6}
\end{Highlighting}
\end{Shaded}

If we only want to consider the lines in the body (so everything except the header), we simply add \texttt{body}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{seq}\NormalTok{ 5 }\KeywordTok{|} \ExtensionTok{header}\NormalTok{ -a count }\KeywordTok{|} \ExtensionTok{body}\NormalTok{ wc -l}
\ExtensionTok{count}
\ExtensionTok{5}
\end{Highlighting}
\end{Shaded}

Note that the header is not used and is also printed again in the output.

The second command-line tool, \texttt{header} allows us, as the name implies, to manipulate the header of a CSV file. The complete source code is as follows:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#!/usr/bin/env bash}
\FunctionTok{get_header ()} \KeywordTok{\{}
        \KeywordTok{for} \ExtensionTok{i}\NormalTok{ in }\VariableTok{$(}\FunctionTok{seq} \VariableTok{$NUMROWS)}\KeywordTok{;} \KeywordTok{do}
                \VariableTok{IFS=} \BuiltInTok{read}\NormalTok{ -r }\VariableTok{LINE}
                \VariableTok{OLDHEADER=}\StringTok{"}\VariableTok{$\{OLDHEADER\}$\{LINE\}}\StringTok{\textbackslash{}n"}
        \KeywordTok{done}
\KeywordTok{\}}

\FunctionTok{print_header ()} \KeywordTok{\{}
        \BuiltInTok{echo}\NormalTok{ -ne }\StringTok{"}\VariableTok{$1}\StringTok{"}
\KeywordTok{\}}

\FunctionTok{print_body ()} \KeywordTok{\{}
        \FunctionTok{cat}
\KeywordTok{\}}

\VariableTok{OLDHEADER=}
\VariableTok{NUMROWS=}\NormalTok{1}

\KeywordTok{while} \BuiltInTok{getopts} \StringTok{"dn:ha:r:e:"}\NormalTok{ OPTION}
\KeywordTok{do}
        \KeywordTok{case} \VariableTok{$OPTION}\KeywordTok{ in}
\NormalTok{                n}\KeywordTok{)}
                        \VariableTok{NUMROWS=$OPTARG}
                        \KeywordTok{;;}
\NormalTok{                a}\KeywordTok{)}
                        \ExtensionTok{print_header} \StringTok{"}\VariableTok{$OPTARG}\StringTok{\textbackslash{}n"}
                        \ExtensionTok{print_body}
                        \BuiltInTok{exit}\NormalTok{ 1}
                        \KeywordTok{;;}
\NormalTok{                d}\KeywordTok{)}
                        \ExtensionTok{get_header}
                        \ExtensionTok{print_body}
                        \BuiltInTok{exit}\NormalTok{ 1}
                        \KeywordTok{;;}
\NormalTok{                r}\KeywordTok{)}
                        \ExtensionTok{get_header}
                        \ExtensionTok{print_header} \StringTok{"}\VariableTok{$OPTARG}\StringTok{\textbackslash{}n"}
                        \ExtensionTok{print_body}
                        \BuiltInTok{exit}\NormalTok{ 1}
                        \KeywordTok{;;}
\NormalTok{                e}\KeywordTok{)}
                        \ExtensionTok{get_header}
                        \ExtensionTok{print_header} \StringTok{"}\VariableTok{$(}\BuiltInTok{echo}\NormalTok{ -ne }\VariableTok{$OLDHEADER} \KeywordTok{|} \BuiltInTok{eval} \VariableTok{$OPTARG)}\StringTok{\textbackslash{}n"}
                        \ExtensionTok{print_body}
                        \BuiltInTok{exit}\NormalTok{ 1}
                        \KeywordTok{;;}
\NormalTok{                h}\KeywordTok{)}
                        \ExtensionTok{usage}
                        \BuiltInTok{exit}\NormalTok{ 1}
                        \KeywordTok{;;}
        \KeywordTok{esac}
\KeywordTok{done}

\ExtensionTok{get_header}
\ExtensionTok{print_header} \VariableTok{$OLDHEADER}
\end{Highlighting}
\end{Shaded}

If no argument are provided, the header of the CSV file is printed:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{tips.csv} \KeywordTok{|} \ExtensionTok{header}
\ExtensionTok{bill}\NormalTok{,tip,sex,smoker,day,time,size}
\end{Highlighting}
\end{Shaded}

This is the same as \texttt{head\ -n\ 1}. If the header spans more than one row, which is not recommended, you can specify \texttt{-n\ 2}. We can also add a header to a CSV file:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{seq}\NormalTok{ 5 }\KeywordTok{|} \ExtensionTok{header}\NormalTok{ -a count}
\ExtensionTok{count}
\ExtensionTok{1}
\ExtensionTok{2}
\ExtensionTok{3}
\ExtensionTok{4}
\ExtensionTok{5}
\end{Highlighting}
\end{Shaded}

This is equivalent to \texttt{echo\ "count"\ \textbar{}\ cat\ -\ \textless{}(seq\ 5)}. Deleting a header is done with the \texttt{-d} command-line argument:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{iris.csv} \KeywordTok{|} \ExtensionTok{header}\NormalTok{ -d }\KeywordTok{|} \FunctionTok{head}
\ExtensionTok{5.1}\NormalTok{,3.5,1.4,0.2,Iris-setosa}
\ExtensionTok{4.9}\NormalTok{,3.0,1.4,0.2,Iris-setosa}
\ExtensionTok{4.7}\NormalTok{,3.2,1.3,0.2,Iris-setosa}
\ExtensionTok{4.6}\NormalTok{,3.1,1.5,0.2,Iris-setosa}
\ExtensionTok{5.0}\NormalTok{,3.6,1.4,0.2,Iris-setosa}
\ExtensionTok{5.4}\NormalTok{,3.9,1.7,0.4,Iris-setosa}
\ExtensionTok{4.6}\NormalTok{,3.4,1.4,0.3,Iris-setosa}
\ExtensionTok{5.0}\NormalTok{,3.4,1.5,0.2,Iris-setosa}
\ExtensionTok{4.4}\NormalTok{,2.9,1.4,0.2,Iris-setosa}
\ExtensionTok{4.9}\NormalTok{,3.1,1.5,0.1,Iris-setosa}
\end{Highlighting}
\end{Shaded}

This is similar to \texttt{tail\ -n\ +2}, but it's a bit easier to remember. Replacing a header, which is basically first deleting a header and then adding one if you look at the above source code, is accomplished with specifying \texttt{-r}. Here, we combine it with \texttt{body}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{seq}\NormalTok{ 5 }\KeywordTok{|} \ExtensionTok{header}\NormalTok{ -a line }\KeywordTok{|} \ExtensionTok{body}\NormalTok{ wc -l }\KeywordTok{|} \ExtensionTok{header}\NormalTok{ -r count}
\ExtensionTok{count}
\ExtensionTok{5}
\end{Highlighting}
\end{Shaded}

And last but not least, we can apply a command to just the header, similar to what the \texttt{body} command-line tool does to the body:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{seq}\NormalTok{ 5 }\KeywordTok{|} \ExtensionTok{header}\NormalTok{ -a line }\KeywordTok{|} \ExtensionTok{header}\NormalTok{ -e }\StringTok{"tr '[a-z]' '[A-Z]'"}
\ExtensionTok{LINE}
\ExtensionTok{1}
\ExtensionTok{2}
\ExtensionTok{3}
\ExtensionTok{4}
\ExtensionTok{5}
\end{Highlighting}
\end{Shaded}

The third command-line tool is called \texttt{cols}, which is similar to \texttt{header} and \texttt{body} in that it allows you to apply a certain command to only a subset of the columns. The code is as follows:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#!/usr/bin/env bash}
\VariableTok{ARG=}\StringTok{"}\VariableTok{$1}\StringTok{"}
\BuiltInTok{shift}
\VariableTok{COLUMNS=}\StringTok{"}\VariableTok{$1}\StringTok{"}
\BuiltInTok{shift}
\VariableTok{EXPR=}\StringTok{"}\VariableTok{$@}\StringTok{"}
\VariableTok{DIRTMP=$(}\FunctionTok{mktemp}\NormalTok{ -d}\VariableTok{)}
\FunctionTok{mkfifo} \VariableTok{$DIRTMP}\NormalTok{/other_columns}
\FunctionTok{tee} \VariableTok{$DIRTMP}\NormalTok{/other_columns }\KeywordTok{|} \ExtensionTok{csvcut} \VariableTok{$ARG} \VariableTok{$COLUMNS} \KeywordTok{|} \VariableTok{$\{EXPR\}} \KeywordTok{|}
\ExtensionTok{paste}\NormalTok{ -d, - }\OperatorTok{<(}\ExtensionTok{csvcut} \VariableTok{$\{ARG}\ErrorTok{~~}\VariableTok{\}} \VariableTok{$COLUMNS} \VariableTok{$DIRTMP}\NormalTok{/other_columns}\OperatorTok{)}
\FunctionTok{rm}\NormalTok{ -rf }\VariableTok{$DIRTMP}
\end{Highlighting}
\end{Shaded}

For example, if we wanted to uppercase the values in the day column in the tips data set (without affecting the other columns and the header), we would use \texttt{cols} in combination with \texttt{body}, as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{tips.csv}\NormalTok{ cols -c day body }\StringTok{"tr '[a-z]' '[A-Z]'"} \KeywordTok{|} \FunctionTok{head}\NormalTok{ -n 5 }\KeywordTok{|} \ExtensionTok{csvlook}\NormalTok{ -I}
\KeywordTok{|}\ExtensionTok{------+-------+------+--------+--------+--------+-------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{day} \KeywordTok{|} \ExtensionTok{bill}  \KeywordTok{|} \ExtensionTok{tip}  \KeywordTok{|} \ExtensionTok{sex}    \KeywordTok{|} \ExtensionTok{smoker} \KeywordTok{|} \BuiltInTok{time}   \KeywordTok{|} \FunctionTok{size}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{------+-------+------+--------+--------+--------+-------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{SUN} \KeywordTok{|} \ExtensionTok{16.99} \KeywordTok{|} \ExtensionTok{1.01} \KeywordTok{|} \ExtensionTok{Female} \KeywordTok{|} \ExtensionTok{No}     \KeywordTok{|} \ExtensionTok{Dinner} \KeywordTok{|} \ExtensionTok{2}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{SUN} \KeywordTok{|} \ExtensionTok{10.34} \KeywordTok{|} \ExtensionTok{1.66} \KeywordTok{|} \ExtensionTok{Male}   \KeywordTok{|} \ExtensionTok{No}     \KeywordTok{|} \ExtensionTok{Dinner} \KeywordTok{|} \ExtensionTok{3}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{SUN} \KeywordTok{|} \ExtensionTok{21.01} \KeywordTok{|} \ExtensionTok{3.5}  \KeywordTok{|} \ExtensionTok{Male}   \KeywordTok{|} \ExtensionTok{No}     \KeywordTok{|} \ExtensionTok{Dinner} \KeywordTok{|} \ExtensionTok{3}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{SUN} \KeywordTok{|} \ExtensionTok{23.68} \KeywordTok{|} \ExtensionTok{3.31} \KeywordTok{|} \ExtensionTok{Male}   \KeywordTok{|} \ExtensionTok{No}     \KeywordTok{|} \ExtensionTok{Dinner} \KeywordTok{|} \ExtensionTok{2}     \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{------+-------+------+--------+--------+--------+-------}\KeywordTok{|}
\end{Highlighting}
\end{Shaded}

Please note that passing multiple command-line tools and arguments as command to \texttt{header\ -e}, \texttt{body}, and \texttt{cols} can lead to tricky quoting citations. If you ever run in such problems, it is best to create a separate command-line tool for this and pass that as command.

In conclusion, while it is generally preferable to use command-line tools which are specifically made for CSV data, \texttt{body}, \texttt{header}, and \texttt{cols} also allow you to apply the classic command-line tools to CSV files if needed.

\hypertarget{performing-sql-queries-on-csv}{%
\subsection{Performing SQL Queries on CSV}\label{performing-sql-queries-on-csv}}

In case the command-line tools mentioned in this chapter do not provide enough flexibility, then there is another approach to scrub your data from the command line. The command-line tool \texttt{csvsql} \citep{csvsql} allows you to execute SQL queries directly on CSV files. As you may know, SQL is a very powerful language to define operations for scrubbing data; it is a very different way than using individual command-line tools.

\begin{rmdnote}
If your data originally comes from a relational database, then, if possible, try to execute SQL queries on that database and subsequently extract the data as CSV. As discussed in Chapter 3, you can use the command-line tool \texttt{sql2csv} for this. When you first export data from the database to a CSV file, and then apply SQL, it is not only slower, but there is also a possibility that the column types are not correctly inferred from the CSV data.
\end{rmdnote}

In the scrubbing tasks below, we'll include several solutions that involve \texttt{csvsql}. The basic command is this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{seq}\NormalTok{ 5 }\KeywordTok{|} \ExtensionTok{header}\NormalTok{ -a value }\KeywordTok{|} \ExtensionTok{csvsql}\NormalTok{ --query }\StringTok{"SELECT SUM(value) AS sum FROM stdin"}
\FunctionTok{sum}
\ExtensionTok{15}
\end{Highlighting}
\end{Shaded}

If you pass standard input to \texttt{csvsql}, then the table is named \emph{stdin}. The types of the column are automatically inferred from the data. As you will see later, in the combining CSV files section, you can also specify multiple CSV files. Please keep in mind that \texttt{csvsql} employs SQLite dialect. While SQL is generally more verbose than the other solutions, it is also much more flexible. If you already know how to tackle a scrubbing problem with SQL, then there's no shame in using it from the command line!

\hypertarget{working-with-xmlhtml-and-json}{%
\section{Working with XML/HTML and JSON}\label{working-with-xmlhtml-and-json}}

As we have seen in Chapter 3, our obtained data can come in a variety of formats. The most common ones are plain text, CSV, JSON, and HTML/XML. In this section we are going to demonstrate a couple of command-line tools that can convert our data from one format to another. There are two reasons to convert data.

First, oftentimes, the data needs to be in tabular form, just like a database table or a spreadsheet, because many visualization and machine learning algorithms depend on it. CSV is inherently in tabular form, but JSON and HTML/XML data can have a deeply nested structure.

Second, many command-line tools, especially the classic ones such as \texttt{cut} and \texttt{grep}, operate on plain text. This is because text is regarded as a universal interface between command-line tools. Moreover, the other formats are simply younger. Each of these formats can be treated as plain text, allowing us to apply such command-line tools to the other formats as well.

Sometimes we can get away with applying the classic tools to structured data. For example, by treating the JSON data below as plain text, we can change the attribute \emph{gender} to \emph{sex} using \texttt{sed}:

\begin{verbatim}
$ sed -e 's/"gender":/"sex":/g' data/users.json | fold | head -n 3
\end{verbatim}

Like many other command-line tools, \texttt{sed} does not make use of the structure of the data. Better is to either use a command-line tool that makes use of the structure of the data (such as \texttt{jq} which we discuss below), or first convert the data to a tabular format such as CSV and then apply the appropriate command-line tool.

We're going to demonstrate converting XML/HTML and JSON to CSV through a real-world use case. The command-line tools that we'll be using here are: \texttt{curl}, \texttt{scrape} \citep{scrape}, \texttt{xml2json} \citep{xml2json}, \texttt{jq} \citep{jq}, and \texttt{json2csv} \citep{json2csv}.

Wikpedia holds a wealth of information. Much of this information is ordered in tables, which can be regarded as data sets. For example, the page \url{http://en.wikipedia.org/wiki/List_of_countries_and_territories_by_border/area_ratio} contains a list of countries and territories together with their border length, their area, and the ration between the two.

Let's imagine that we're interested in analyzing this data. In this section, we'll walk you through all the necessary steps and their corresponding commands. We won't go into every little detail, so it could be that you won't understand everything right away. Don't worry, we're confident that you'll get the gist of it. Remember that the purpose of this section is to demonstrate the command line. All tools and concepts used in this section (and more) will be explained in the subsequent chapters.

The data set that we're interested in, is embedded in HTML. Our goal is to end up with a representation of this data set that we can work with. The very first step is to download the HTML using \texttt{curl}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{curl}\NormalTok{ -sL }\StringTok{'http://en.wikipedia.org/wiki/List_of_countries_and_territories_'}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{'by_border/area_ratio'} \OperatorTok{>}\NormalTok{ data/wiki.html}
\end{Highlighting}
\end{Shaded}

The option \texttt{-s} causes \texttt{curl} to be silent and not output any other information but the actual HTML. The HTML is saved to a file named \texttt{data/wiki.html}. Let's see how the first 10 lines look like:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{head}\NormalTok{ -n 10 data/wiki.html }\KeywordTok{|} \FunctionTok{cut}\NormalTok{ -c1-79}
\OperatorTok{<}\NormalTok{!}\ExtensionTok{DOCTYPE}\NormalTok{ html}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{html}\NormalTok{ lang=}\StringTok{"en"}\NormalTok{ dir=}\StringTok{"ltr"}\NormalTok{ class=}\StringTok{"client-nojs"}\OperatorTok{>}
\OperatorTok{<}\FunctionTok{head}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{meta}\NormalTok{ charset=}\StringTok{"UTF-8"}\NormalTok{ /}\OperatorTok{><}\NormalTok{title}\OperatorTok{>}\NormalTok{List of countries and territories by border/area}
\OperatorTok{<}\ExtensionTok{meta}\NormalTok{ http-equiv=}\StringTok{"X-UA-Compatible"}\NormalTok{ content=}\StringTok{"IE=EDGE"}\NormalTok{ /}\OperatorTok{><}\NormalTok{meta name=}\StringTok{"generator"}\NormalTok{ c}
\OperatorTok{<}\FunctionTok{link}\NormalTok{ rel=}\StringTok{"alternate"}\NormalTok{ type=}\StringTok{"application/x-wiki"}\NormalTok{ title=}\StringTok{"Edit this page"}\NormalTok{ href=}\StringTok{"/w}
\StringTok{<link rel="}\NormalTok{edit}\StringTok{" title="}\NormalTok{Edit this page}\StringTok{" href="}\NormalTok{/w/index.php?title=List_of_countr}
\OperatorTok{<}\FunctionTok{link}\NormalTok{ rel=}\StringTok{"apple-touch-icon"}\NormalTok{ href=}\StringTok{"//bits.wikimedia.org/apple-touch/wikipedia.p}
\StringTok{<link rel="}\NormalTok{shortcut icon}\StringTok{" href="}\NormalTok{//bits.wikimedia.org/favicon/wikipedia.ico}\StringTok{" />}
\StringTok{<link rel="}\NormalTok{search}\StringTok{" type="}\NormalTok{application/opensearchdescription+xml}\StringTok{" href="}\NormalTok{/w/opense}
\end{Highlighting}
\end{Shaded}

That seems to be in order. (Note that we're only showing the first 79 characters of each line so that output fits on the page.)

Using the developer tools of our browser, we were able to determine that the root HTML element that we're interested in is a \emph{\textless{}table\textgreater{}} with the class \emph{wikitable}. This allows us to look at the part that we're interest in using \texttt{grep} (the \texttt{-A} command-line argument specifies the number of lines we want to see after the matching line):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{data/wiki.html}\NormalTok{ grep wikitable -A 21}
\OperatorTok{<}\ExtensionTok{table}\NormalTok{ class=}\StringTok{"wikitable sortable"}\OperatorTok{>}
\OperatorTok{<}\FunctionTok{tr}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{th}\OperatorTok{>}\NormalTok{Rank}\OperatorTok{<}\NormalTok{/th}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{th}\OperatorTok{>}\NormalTok{Country or territory}\OperatorTok{<}\NormalTok{/th}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{th}\OperatorTok{>}\NormalTok{Total length of land borders (km)}\OperatorTok{<}\NormalTok{/}\ExtensionTok{th}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{th}\OperatorTok{>}\NormalTok{Total surface area (kmÂ²)}\OperatorTok{<}\NormalTok{/}\ExtensionTok{th}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{th}\OperatorTok{>}\NormalTok{Border/area ratio (km/kmÂ²)}\OperatorTok{<}\NormalTok{/}\ExtensionTok{th}\OperatorTok{>}
\OperatorTok{<}\NormalTok{/}\ExtensionTok{tr}\OperatorTok{>}
\OperatorTok{<}\FunctionTok{tr}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{td}\OperatorTok{>1<}\NormalTok{/td}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{td}\OperatorTok{>}\NormalTok{Vatican City}\OperatorTok{<}\NormalTok{/td}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{td}\OperatorTok{>}\NormalTok{3.}\OperatorTok{2<}\NormalTok{/td}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{td}\OperatorTok{>}\NormalTok{0.}\OperatorTok{44<}\NormalTok{/td}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{td}\OperatorTok{>}\NormalTok{7.}\OperatorTok{2727273<}\NormalTok{/td}\OperatorTok{>}
\OperatorTok{<}\NormalTok{/}\ExtensionTok{tr}\OperatorTok{>}
\OperatorTok{<}\FunctionTok{tr}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{td}\OperatorTok{>2<}\NormalTok{/td}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{td}\OperatorTok{>}\NormalTok{Monaco}\OperatorTok{<}\NormalTok{/td}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{td}\OperatorTok{>}\NormalTok{4.}\OperatorTok{4<}\NormalTok{/td}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{td}\OperatorTok{>2<}\NormalTok{/td}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{td}\OperatorTok{>}\NormalTok{2.}\OperatorTok{2000000<}\NormalTok{/td}\OperatorTok{>}
\OperatorTok{<}\NormalTok{/}\ExtensionTok{tr}\OperatorTok{>}
\end{Highlighting}
\end{Shaded}

We now actually see the countries and their values that we first saw in the screenshot. The next step is to extract the necessary elements from the HTML file. For this we use the \texttt{scrape} tool:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{data/wiki.html}\NormalTok{ scrape -b -e }\StringTok{'table.wikitable > tr:not(:first-child)'}\NormalTok{ \textbackslash{}}
\OperatorTok{>} \OperatorTok{>}\NormalTok{ data/table.html}
\NormalTok{$ }\FunctionTok{head}\NormalTok{ -n 21 data/table.html}
\OperatorTok{<}\NormalTok{!}\ExtensionTok{DOCTYPE}\NormalTok{ html}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{html}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{body}\OperatorTok{>}
\OperatorTok{<}\FunctionTok{tr}\OperatorTok{><}\NormalTok{td}\OperatorTok{>1<}\NormalTok{/td}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{td}\OperatorTok{>}\NormalTok{Vatican City}\OperatorTok{<}\NormalTok{/td}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{td}\OperatorTok{>}\NormalTok{3.}\OperatorTok{2<}\NormalTok{/td}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{td}\OperatorTok{>}\NormalTok{0.}\OperatorTok{44<}\NormalTok{/td}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{td}\OperatorTok{>}\NormalTok{7.}\OperatorTok{2727273<}\NormalTok{/td}\OperatorTok{>}
\OperatorTok{<}\NormalTok{/}\ExtensionTok{tr}\OperatorTok{>}
\OperatorTok{<}\FunctionTok{tr}\OperatorTok{><}\NormalTok{td}\OperatorTok{>2<}\NormalTok{/td}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{td}\OperatorTok{>}\NormalTok{Monaco}\OperatorTok{<}\NormalTok{/td}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{td}\OperatorTok{>}\NormalTok{4.}\OperatorTok{4<}\NormalTok{/td}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{td}\OperatorTok{>2<}\NormalTok{/td}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{td}\OperatorTok{>}\NormalTok{2.}\OperatorTok{2000000<}\NormalTok{/td}\OperatorTok{>}
\OperatorTok{<}\NormalTok{/}\ExtensionTok{tr}\OperatorTok{>}
\OperatorTok{<}\FunctionTok{tr}\OperatorTok{><}\NormalTok{td}\OperatorTok{>3<}\NormalTok{/td}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{td}\OperatorTok{>}\NormalTok{San Marino}\OperatorTok{<}\NormalTok{/td}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{td}\OperatorTok{>39<}\NormalTok{/td}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{td}\OperatorTok{>61<}\NormalTok{/td}\OperatorTok{>}
\OperatorTok{<}\ExtensionTok{td}\OperatorTok{>}\NormalTok{0.}\OperatorTok{6393443<}\NormalTok{/td}\OperatorTok{>}
\OperatorTok{<}\NormalTok{/}\ExtensionTok{tr}\OperatorTok{>}
\end{Highlighting}
\end{Shaded}

The value passed to argument \texttt{-e}, which stands for \emph{expression} (also with many other command-line tools), is a so-called CSS-selector. The syntax is usually used to style web pages, but we can also use it to select certain elements from our HTML. In this case, we wish to select all \emph{\textless{}tr\textgreater{}} elements or \emph{rows} (except the first) that are part of a table which belongs to the \emph{wikitable} class. This is precisely the table that we're interested in. The reason that we don't want the first row (specified by \emph{:not(first-child)}) is that we don't want the header of the table. This results in a data set where each row represents a country or territory. As you can see, we now have a \emph{\textless{}tr\textgreater{}} elements that we're looking for, encapsulated in \emph{\textless{}html\textgreater{}` and '\textless{}body\textgreater{}} elements (because we specified the \texttt{-b} argument). This ensures that our next tool, \texttt{xml2json}, can work with it.

As its name implies, \texttt{xml2json} converts XML (and HTML) to JSON.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{data/table.html}\NormalTok{ xml2json }\OperatorTok{>}\NormalTok{ data/table.json}
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{data/table.json}\NormalTok{ jq }\StringTok{'.'} \KeywordTok{|} \FunctionTok{head}\NormalTok{ -n 25}
\KeywordTok{\{}
  \StringTok{"html"}\NormalTok{: }\KeywordTok{\{}
    \StringTok{"body"}\NormalTok{: }\KeywordTok{\{}
      \StringTok{"tr"}\NormalTok{:}\BuiltInTok{ [}
\NormalTok{        \{}
          \StringTok{"td"}\NormalTok{: [}
\NormalTok{            \{}
              \StringTok{"}\VariableTok{$t}\StringTok{"}\NormalTok{: }\StringTok{"1"}
\NormalTok{            \},}
\NormalTok{            \{}
              \StringTok{"}\VariableTok{$t}\StringTok{"}\NormalTok{: }\StringTok{"Vatican City"}
\NormalTok{            \},}
\NormalTok{            \{}
              \StringTok{"}\VariableTok{$t}\StringTok{"}\NormalTok{: }\StringTok{"3.2"}
\NormalTok{            \},}
\NormalTok{            \{}
              \StringTok{"}\VariableTok{$t}\StringTok{"}\NormalTok{: }\StringTok{"0.44"}
\NormalTok{            \},}
\NormalTok{            \{}
              \StringTok{"}\VariableTok{$t}\StringTok{"}\NormalTok{: }\StringTok{"7.2727273"}
\NormalTok{            \}}
\NormalTok{          ]}
\NormalTok{        \},}
\NormalTok{        \{}
          \StringTok{"td"}\NormalTok{: [}
\end{Highlighting}
\end{Shaded}

The reason we convert the HTML to JSON is because there is a very powerful tool called \texttt{jq} that operates on JSON data. The following command extracts certain parts of the JSON data and reshapes it into a form that we can work with:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{data/table.json}\NormalTok{ jq -c }\StringTok{'.html.body.tr[] | \{country: .td[1][],border:'}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{'.td[2][], surface: .td[3][]\}'} \OperatorTok{>}\NormalTok{ data/countries.json}
\NormalTok{$ }\FunctionTok{head}\NormalTok{ -n 10 data/countries.json}
\NormalTok{\{}\StringTok{"surface"}\NormalTok{:}\StringTok{"0.44"}\NormalTok{,}\StringTok{"border"}\NormalTok{:}\StringTok{"3.2"}\NormalTok{,}\StringTok{"country"}\NormalTok{:}\StringTok{"Vatican City"}\NormalTok{\}}
\DataTypeTok{\{"surface":"2","border":"4.4","country":"Monaco"\}}
\NormalTok{\{}\StringTok{"surface"}\NormalTok{:}\StringTok{"61"}\NormalTok{,}\StringTok{"border"}\NormalTok{:}\StringTok{"39"}\NormalTok{,}\StringTok{"country"}\NormalTok{:}\StringTok{"San Marino"}\NormalTok{\}}
\DataTypeTok{\{"surface":"160","border":"76","country":"Liechtenstein"\}}
\NormalTok{\{}\StringTok{"surface"}\NormalTok{:}\StringTok{"34"}\NormalTok{,}\StringTok{"border"}\NormalTok{:}\StringTok{"10.2"}\NormalTok{,}\StringTok{"country"}\NormalTok{:}\StringTok{"Sint Maarten (Netherlands)"}\NormalTok{\}}
\DataTypeTok{\{"surface":"468","border":"120.3","country":"Andorra"\}}
\NormalTok{\{}\StringTok{"surface"}\NormalTok{:}\StringTok{"6"}\NormalTok{,}\StringTok{"border"}\NormalTok{:}\StringTok{"1.2"}\NormalTok{,}\StringTok{"country"}\NormalTok{:}\StringTok{"Gibraltar (United Kingdom)"}\NormalTok{\}}
\NormalTok{\{}\StringTok{"surface"}\NormalTok{:}\StringTok{"54"}\NormalTok{,}\StringTok{"border"}\NormalTok{:}\StringTok{"10.2"}\NormalTok{,}\StringTok{"country"}\NormalTok{:}\StringTok{"Saint Martin (France)"}\NormalTok{\}}
\DataTypeTok{\{"surface":"2586","border":"359","country":"Luxembourg"\}}
\NormalTok{\{}\StringTok{"surface"}\NormalTok{:}\StringTok{"6220"}\NormalTok{,}\StringTok{"border"}\NormalTok{:}\StringTok{"466"}\NormalTok{,}\StringTok{"country"}\NormalTok{:}\StringTok{"Palestinian territories"}\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now we're getting somewhere. JSON is a very popular data format, with many advantages, but for our purposes, we're better off with having the data in CSV format. The tool \texttt{json2csv} is able to convert the data from JSON to CSV:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{data/countries.json}\NormalTok{ json2csv -p -k border,surface }\OperatorTok{>}\NormalTok{ data/countries.csv}
\NormalTok{$ }\FunctionTok{head}\NormalTok{ -n 11 data/countries.csv }\KeywordTok{|} \ExtensionTok{csvlook}
\KeywordTok{|}\ExtensionTok{---------+----------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{border} \KeywordTok{|} \ExtensionTok{surface}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{---------+----------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{3.2}    \KeywordTok{|} \ExtensionTok{0.44}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{4.4}    \KeywordTok{|} \ExtensionTok{2}        \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{39}     \KeywordTok{|} \ExtensionTok{61}       \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{76}     \KeywordTok{|} \ExtensionTok{160}      \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{10.2}   \KeywordTok{|} \ExtensionTok{34}       \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{120.3}  \KeywordTok{|} \ExtensionTok{468}      \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{1.2}    \KeywordTok{|} \ExtensionTok{6}        \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{10.2}   \KeywordTok{|} \ExtensionTok{54}       \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{359}    \KeywordTok{|} \ExtensionTok{2586}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{466}    \KeywordTok{|} \ExtensionTok{6220}     \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{---------+----------}\KeywordTok{|}
\end{Highlighting}
\end{Shaded}

The data is now in a form that we can work with. Those were quite a few steps to get from a Wikipedia page to a CSV data set. However, when you combine all of the above commands into one, you will see that it's actually really concise and expressive:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{curl}\NormalTok{ -sL }\StringTok{'http://en.wikipedia.org/wiki/List_of_countries'}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{'_and_territories_by_border/area_ratio'} \KeywordTok{|}
\OperatorTok{>} \ExtensionTok{scrape}\NormalTok{ -be }\StringTok{'table.wikitable > tr:not(:first-child)'} \KeywordTok{|}
\OperatorTok{>} \ExtensionTok{xml2json} \KeywordTok{|} \ExtensionTok{jq}\NormalTok{ -c }\StringTok{'.html.body.tr[] | \{country: .td[1][],'}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{'border: .td[2][], surface: .td[3][], ratio: .td[4][]\}'} \KeywordTok{|}
\OperatorTok{>} \ExtensionTok{json2csv}\NormalTok{ -p -k=border,surface }\KeywordTok{|} \FunctionTok{head}\NormalTok{ -n 11 }\KeywordTok{|} \ExtensionTok{csvlook}
\KeywordTok{|}\ExtensionTok{---------+----------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{border} \KeywordTok{|} \ExtensionTok{surface}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{---------+----------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{3.2}    \KeywordTok{|} \ExtensionTok{0.44}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{4.4}    \KeywordTok{|} \ExtensionTok{2}        \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{39}     \KeywordTok{|} \ExtensionTok{61}       \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{76}     \KeywordTok{|} \ExtensionTok{160}      \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{10.2}   \KeywordTok{|} \ExtensionTok{34}       \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{120.3}  \KeywordTok{|} \ExtensionTok{468}      \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{1.2}    \KeywordTok{|} \ExtensionTok{6}        \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{10.2}   \KeywordTok{|} \ExtensionTok{54}       \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{359}    \KeywordTok{|} \ExtensionTok{2586}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{466}    \KeywordTok{|} \ExtensionTok{6220}     \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{---------+----------}\KeywordTok{|}
\end{Highlighting}
\end{Shaded}

That concludes the demonstration of conversion XML/HTML to JSON to CSV. While \texttt{jq} can perform much more operations, and while there exist specialized tools to work with XML data, in our experience, converting the data to CSV format as quickly as possible tends to work well. This way, you can spend more time becoming proficient at generic command-line tools, rather than very specific tools.

\hypertarget{common-scrub-operations-for-csv}{%
\section{Common Scrub Operations for CSV}\label{common-scrub-operations-for-csv}}

\hypertarget{extracting-and-reordering-columns}{%
\subsection{Extracting and Reordering Columns}\label{extracting-and-reordering-columns}}

Columns can be extracted and reordered using the command-line tool: \texttt{csvcut} \citep{csvcut}. For example, to keep only the columns in the Iris data set that contain numerical values \emph{and} reorder the middle two columns:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{iris.csv}\NormalTok{ csvcut -c sepal_length,petal_length,sepal_width,petal_width }\KeywordTok{|}
\OperatorTok{>} \FunctionTok{head}\NormalTok{ -n 5 }\KeywordTok{|} \ExtensionTok{csvlook}
\KeywordTok{|}\ExtensionTok{---------------+--------------+-------------+--------------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{sepal_length} \KeywordTok{|} \ExtensionTok{petal_length} \KeywordTok{|} \ExtensionTok{sepal_width} \KeywordTok{|} \ExtensionTok{petal_width}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{---------------+--------------+-------------+--------------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{5.1}          \KeywordTok{|} \ExtensionTok{1.4}          \KeywordTok{|} \ExtensionTok{3.5}         \KeywordTok{|} \ExtensionTok{0.2}          \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{4.9}          \KeywordTok{|} \ExtensionTok{1.4}          \KeywordTok{|} \ExtensionTok{3.0}         \KeywordTok{|} \ExtensionTok{0.2}          \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{4.7}          \KeywordTok{|} \ExtensionTok{1.3}          \KeywordTok{|} \ExtensionTok{3.2}         \KeywordTok{|} \ExtensionTok{0.2}          \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{4.6}          \KeywordTok{|} \ExtensionTok{1.5}          \KeywordTok{|} \ExtensionTok{3.1}         \KeywordTok{|} \ExtensionTok{0.2}          \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{---------------+--------------+-------------+--------------}\KeywordTok{|}
\end{Highlighting}
\end{Shaded}

Alternatively, we can also specify the columns we want to leave out with \texttt{-C}, which stands for \emph{complement}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{iris.csv}\NormalTok{ csvcut -C species }\KeywordTok{|} \FunctionTok{head}\NormalTok{ -n 5 }\KeywordTok{|} \ExtensionTok{csvlook}
\KeywordTok{|}\ExtensionTok{---------------+-------------+--------------+--------------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{sepal_length} \KeywordTok{|} \ExtensionTok{sepal_width} \KeywordTok{|} \ExtensionTok{petal_length} \KeywordTok{|} \ExtensionTok{petal_width}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{---------------+-------------+--------------+--------------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{5.1}          \KeywordTok{|} \ExtensionTok{3.5}         \KeywordTok{|} \ExtensionTok{1.4}          \KeywordTok{|} \ExtensionTok{0.2}          \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{4.9}          \KeywordTok{|} \ExtensionTok{3.0}         \KeywordTok{|} \ExtensionTok{1.4}          \KeywordTok{|} \ExtensionTok{0.2}          \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{4.7}          \KeywordTok{|} \ExtensionTok{3.2}         \KeywordTok{|} \ExtensionTok{1.3}          \KeywordTok{|} \ExtensionTok{0.2}          \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{4.6}          \KeywordTok{|} \ExtensionTok{3.1}         \KeywordTok{|} \ExtensionTok{1.5}          \KeywordTok{|} \ExtensionTok{0.2}          \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{---------------+-------------+--------------+--------------}\KeywordTok{|}
\end{Highlighting}
\end{Shaded}

Here, the included columns are kept in the same order. Instead of the column names, you can also specify the indices of the columns, which start at 1. This allows you to, for example, select only the odd columns (should you ever need it!):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{echo} \StringTok{'a,b,c,d,e,f,g,h,i\textbackslash{}n1,2,3,4,5,6,7,8,9'} \KeywordTok{|}
\OperatorTok{>} \ExtensionTok{csvcut}\NormalTok{ -c }\VariableTok{$(}\FunctionTok{seq}\NormalTok{ 1 2 9 }\KeywordTok{|} \ExtensionTok{paste}\NormalTok{ -sd,}\VariableTok{)}
\ExtensionTok{a}\NormalTok{,c,e,g,i}
\ExtensionTok{1}\NormalTok{,3,5,7,9}
\end{Highlighting}
\end{Shaded}

If you're certain that there are no comma's in any of the values, then you can also use \texttt{cut} to extract columns. Be aware that \texttt{cut} does not reorder columns, as is demonstrated with the following command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{echo} \StringTok{'a,b,c,d,e,f,g,h,i\textbackslash{}n1,2,3,4,5,6,7,8,9'} \KeywordTok{|} \FunctionTok{cut}\NormalTok{ -d, -f 5,1,3}
\ExtensionTok{a}\NormalTok{,c,e}
\ExtensionTok{1}\NormalTok{,3,5}
\end{Highlighting}
\end{Shaded}

As you can see, it does not matter in which order we specify the columns with \texttt{-f}, with \texttt{cut} they will always appear in the original order. For completeness, let's also take a look at the SQL approach for extracting and reordering the numerical columns of the Iris data set:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{iris.csv}\NormalTok{ csvsql --query }\StringTok{"SELECT sepal_length, petal_length, "}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{"sepal_width, petal_width FROM stdin"} \KeywordTok{|} \FunctionTok{head}\NormalTok{ -n 5 }\KeywordTok{|} \ExtensionTok{csvlook}
\KeywordTok{|}\ExtensionTok{---------------+--------------+-------------+--------------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{sepal_length} \KeywordTok{|} \ExtensionTok{petal_length} \KeywordTok{|} \ExtensionTok{sepal_width} \KeywordTok{|} \ExtensionTok{petal_width}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{---------------+--------------+-------------+--------------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{5.1}          \KeywordTok{|} \ExtensionTok{1.4}          \KeywordTok{|} \ExtensionTok{3.5}         \KeywordTok{|} \ExtensionTok{0.2}          \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{4.9}          \KeywordTok{|} \ExtensionTok{1.4}          \KeywordTok{|} \ExtensionTok{3.0}         \KeywordTok{|} \ExtensionTok{0.2}          \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{4.7}          \KeywordTok{|} \ExtensionTok{1.3}          \KeywordTok{|} \ExtensionTok{3.2}         \KeywordTok{|} \ExtensionTok{0.2}          \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{4.6}          \KeywordTok{|} \ExtensionTok{1.5}          \KeywordTok{|} \ExtensionTok{3.1}         \KeywordTok{|} \ExtensionTok{0.2}          \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{---------------+--------------+-------------+--------------}\KeywordTok{|}
\end{Highlighting}
\end{Shaded}

\hypertarget{filtering-lines-1}{%
\subsection{Filtering Lines}\label{filtering-lines-1}}

The difference between filtering lines in a CSV file as opposed to a plain text file is that you may want to base this filtering on values in a certain column, only. Filtering on location is essentially the same, but you have to take into account that the first line of a CSV file is usually the header. Remember that you can always use the \texttt{body} command-line tool if you want to keep the header:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{seq}\NormalTok{ 5 }\KeywordTok{|} \FunctionTok{sed}\NormalTok{ -n }\StringTok{'3,5p'}
\ExtensionTok{3}
\ExtensionTok{4}
\ExtensionTok{5}
\NormalTok{$ }\FunctionTok{seq}\NormalTok{ 5 }\KeywordTok{|} \ExtensionTok{header}\NormalTok{ -a count }\KeywordTok{|} \ExtensionTok{body}\NormalTok{ sed -n }\StringTok{'3,5p'}
\ExtensionTok{count}
\ExtensionTok{3}
\ExtensionTok{4}
\ExtensionTok{5}
\end{Highlighting}
\end{Shaded}

When it comes down to filtering on a certain pattern within a certain column, we can use either \texttt{csvgrep}, \texttt{awk}, or, of course, \texttt{csvsql}. For example, to exclude all the bills of which the party size was 4 or less:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{csvgrep}\NormalTok{ -c size -i -r }\StringTok{"[1-4]"}\NormalTok{ tips.csv }\KeywordTok{|} \ExtensionTok{csvlook}
\KeywordTok{|}\ExtensionTok{--------+------+--------+--------+------+--------+-------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{bill}  \KeywordTok{|} \ExtensionTok{tip}  \KeywordTok{|} \ExtensionTok{sex}    \KeywordTok{|} \ExtensionTok{smoker} \KeywordTok{|} \ExtensionTok{day}  \KeywordTok{|} \BuiltInTok{time}   \KeywordTok{|} \FunctionTok{size}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{--------+------+--------+--------+------+--------+-------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{29.8}  \KeywordTok{|} \ExtensionTok{4.2}  \KeywordTok{|} \ExtensionTok{Female} \KeywordTok{|} \ExtensionTok{No}     \KeywordTok{|} \ExtensionTok{Thur} \KeywordTok{|} \ExtensionTok{Lunch}  \KeywordTok{|} \ExtensionTok{6}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{34.3}  \KeywordTok{|} \ExtensionTok{6.7}  \KeywordTok{|} \ExtensionTok{Male}   \KeywordTok{|} \ExtensionTok{No}     \KeywordTok{|} \ExtensionTok{Thur} \KeywordTok{|} \ExtensionTok{Lunch}  \KeywordTok{|} \ExtensionTok{6}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{41.19} \KeywordTok{|} \ExtensionTok{5.0}  \KeywordTok{|} \ExtensionTok{Male}   \KeywordTok{|} \ExtensionTok{No}     \KeywordTok{|} \ExtensionTok{Thur} \KeywordTok{|} \ExtensionTok{Lunch}  \KeywordTok{|} \ExtensionTok{5}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{27.05} \KeywordTok{|} \ExtensionTok{5.0}  \KeywordTok{|} \ExtensionTok{Female} \KeywordTok{|} \ExtensionTok{No}     \KeywordTok{|} \ExtensionTok{Thur} \KeywordTok{|} \ExtensionTok{Lunch}  \KeywordTok{|} \ExtensionTok{6}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{29.85} \KeywordTok{|} \ExtensionTok{5.14} \KeywordTok{|} \ExtensionTok{Female} \KeywordTok{|} \ExtensionTok{No}     \KeywordTok{|} \ExtensionTok{Sun}  \KeywordTok{|} \ExtensionTok{Dinner} \KeywordTok{|} \ExtensionTok{5}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{48.17} \KeywordTok{|} \ExtensionTok{5.0}  \KeywordTok{|} \ExtensionTok{Male}   \KeywordTok{|} \ExtensionTok{No}     \KeywordTok{|} \ExtensionTok{Sun}  \KeywordTok{|} \ExtensionTok{Dinner} \KeywordTok{|} \ExtensionTok{6}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{20.69} \KeywordTok{|} \ExtensionTok{5.0}  \KeywordTok{|} \ExtensionTok{Male}   \KeywordTok{|} \ExtensionTok{No}     \KeywordTok{|} \ExtensionTok{Sun}  \KeywordTok{|} \ExtensionTok{Dinner} \KeywordTok{|} \ExtensionTok{5}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{30.46} \KeywordTok{|} \ExtensionTok{2.0}  \KeywordTok{|} \ExtensionTok{Male}   \KeywordTok{|} \ExtensionTok{Yes}    \KeywordTok{|} \ExtensionTok{Sun}  \KeywordTok{|} \ExtensionTok{Dinner} \KeywordTok{|} \ExtensionTok{5}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{28.15} \KeywordTok{|} \ExtensionTok{3.0}  \KeywordTok{|} \ExtensionTok{Male}   \KeywordTok{|} \ExtensionTok{Yes}    \KeywordTok{|} \ExtensionTok{Sat}  \KeywordTok{|} \ExtensionTok{Dinner} \KeywordTok{|} \ExtensionTok{5}     \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{--------+------+--------+--------+------+--------+-------}\KeywordTok{|}
\end{Highlighting}
\end{Shaded}

Both \texttt{awk} and \texttt{csvsql} can also do numerical comparisons. For example, to get all the bills above 40 USD on a Saturday or a Sunday:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{tips.csv}\NormalTok{ awk -F, }\StringTok{'($1 > 40.0) && ($5 ~ /S/)'} \KeywordTok{|} \ExtensionTok{csvlook}\NormalTok{ -I}
\KeywordTok{|}\ExtensionTok{--------+------+--------+-----+-----+--------+----}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{48.27} \KeywordTok{|} \ExtensionTok{6.73} \KeywordTok{|} \ExtensionTok{Male}   \KeywordTok{|} \ExtensionTok{No}  \KeywordTok{|} \ExtensionTok{Sat} \KeywordTok{|} \ExtensionTok{Dinner} \KeywordTok{|} \ExtensionTok{4}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{--------+------+--------+-----+-----+--------+----}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{44.3}  \KeywordTok{|} \ExtensionTok{2.5}  \KeywordTok{|} \ExtensionTok{Female} \KeywordTok{|} \ExtensionTok{Yes} \KeywordTok{|} \ExtensionTok{Sat} \KeywordTok{|} \ExtensionTok{Dinner} \KeywordTok{|} \ExtensionTok{3}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{48.17} \KeywordTok{|} \ExtensionTok{5.0}  \KeywordTok{|} \ExtensionTok{Male}   \KeywordTok{|} \ExtensionTok{No}  \KeywordTok{|} \ExtensionTok{Sun} \KeywordTok{|} \ExtensionTok{Dinner} \KeywordTok{|} \ExtensionTok{6}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{50.81} \KeywordTok{|} \ExtensionTok{10.0} \KeywordTok{|} \ExtensionTok{Male}   \KeywordTok{|} \ExtensionTok{Yes} \KeywordTok{|} \ExtensionTok{Sat} \KeywordTok{|} \ExtensionTok{Dinner} \KeywordTok{|} \ExtensionTok{3}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{45.35} \KeywordTok{|} \ExtensionTok{3.5}  \KeywordTok{|} \ExtensionTok{Male}   \KeywordTok{|} \ExtensionTok{Yes} \KeywordTok{|} \ExtensionTok{Sun} \KeywordTok{|} \ExtensionTok{Dinner} \KeywordTok{|} \ExtensionTok{3}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{40.55} \KeywordTok{|} \ExtensionTok{3.0}  \KeywordTok{|} \ExtensionTok{Male}   \KeywordTok{|} \ExtensionTok{Yes} \KeywordTok{|} \ExtensionTok{Sun} \KeywordTok{|} \ExtensionTok{Dinner} \KeywordTok{|} \ExtensionTok{2}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{48.33} \KeywordTok{|} \ExtensionTok{9.0}  \KeywordTok{|} \ExtensionTok{Male}   \KeywordTok{|} \ExtensionTok{No}  \KeywordTok{|} \ExtensionTok{Sat} \KeywordTok{|} \ExtensionTok{Dinner} \KeywordTok{|} \ExtensionTok{4}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{--------+------+--------+-----+-----+--------+----}\KeywordTok{|}
\end{Highlighting}
\end{Shaded}

The \texttt{csvsql} solution is more verbose but is also more robust as it uses the names of the columns instead of their indexes:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{tips.csv}\NormalTok{ csvsql --query }\StringTok{"SELECT * FROM stdin "}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{"WHERE bill > 40 AND day LIKE '%S%'"} \KeywordTok{|} \ExtensionTok{csvlook}\NormalTok{ -I}
\KeywordTok{|}\ExtensionTok{--------+------+--------+--------+-----+--------+-------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{bill}  \KeywordTok{|} \ExtensionTok{tip}  \KeywordTok{|} \ExtensionTok{sex}    \KeywordTok{|} \ExtensionTok{smoker} \KeywordTok{|} \ExtensionTok{day} \KeywordTok{|} \BuiltInTok{time}   \KeywordTok{|} \FunctionTok{size}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{--------+------+--------+--------+-----+--------+-------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{48.27} \KeywordTok{|} \ExtensionTok{6.73} \KeywordTok{|} \ExtensionTok{Male}   \KeywordTok{|} \ExtensionTok{0}      \KeywordTok{|} \ExtensionTok{Sat} \KeywordTok{|} \ExtensionTok{Dinner} \KeywordTok{|} \ExtensionTok{4}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{44.3}  \KeywordTok{|} \ExtensionTok{2.5}  \KeywordTok{|} \ExtensionTok{Female} \KeywordTok{|} \ExtensionTok{1}      \KeywordTok{|} \ExtensionTok{Sat} \KeywordTok{|} \ExtensionTok{Dinner} \KeywordTok{|} \ExtensionTok{3}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{48.17} \KeywordTok{|} \ExtensionTok{5.0}  \KeywordTok{|} \ExtensionTok{Male}   \KeywordTok{|} \ExtensionTok{0}      \KeywordTok{|} \ExtensionTok{Sun} \KeywordTok{|} \ExtensionTok{Dinner} \KeywordTok{|} \ExtensionTok{6}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{50.81} \KeywordTok{|} \ExtensionTok{10.0} \KeywordTok{|} \ExtensionTok{Male}   \KeywordTok{|} \ExtensionTok{1}      \KeywordTok{|} \ExtensionTok{Sat} \KeywordTok{|} \ExtensionTok{Dinner} \KeywordTok{|} \ExtensionTok{3}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{45.35} \KeywordTok{|} \ExtensionTok{3.5}  \KeywordTok{|} \ExtensionTok{Male}   \KeywordTok{|} \ExtensionTok{1}      \KeywordTok{|} \ExtensionTok{Sun} \KeywordTok{|} \ExtensionTok{Dinner} \KeywordTok{|} \ExtensionTok{3}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{40.55} \KeywordTok{|} \ExtensionTok{3.0}  \KeywordTok{|} \ExtensionTok{Male}   \KeywordTok{|} \ExtensionTok{1}      \KeywordTok{|} \ExtensionTok{Sun} \KeywordTok{|} \ExtensionTok{Dinner} \KeywordTok{|} \ExtensionTok{2}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{48.33} \KeywordTok{|} \ExtensionTok{9.0}  \KeywordTok{|} \ExtensionTok{Male}   \KeywordTok{|} \ExtensionTok{0}      \KeywordTok{|} \ExtensionTok{Sat} \KeywordTok{|} \ExtensionTok{Dinner} \KeywordTok{|} \ExtensionTok{4}     \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{--------+------+--------+--------+-----+--------+-------}\KeywordTok{|}
\end{Highlighting}
\end{Shaded}

It should be noted that the flexibility of the \emph{WHERE} clause in an SQL query cannot be easily matched with other command-line tools, as SQL can operate on dates and sets, and form complex combinations of clauses.

\hypertarget{merging-columns}{%
\subsection{Merging Columns}\label{merging-columns}}

Merging columns is useful for when the values of interest are spread over multiple columns. This may happen with dates (where year, month, and day could be separate columns) or names (where the first name and last name are separate columns). Let's consider the second situation.

The input CSV is a list of contemporary composers. Imagine our task is to combine the first name and the last name into a full name. We'll present four different approaches for this task: \texttt{sed}, \texttt{awk}, \texttt{cols} + \texttt{tr}, and \texttt{csvsql}. Let's have a look at the input CSV:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{names.csv}\NormalTok{ csvlook -I}
\KeywordTok{|}\ExtensionTok{-----+-----------+------------+-------}\KeywordTok{|}
\KeywordTok{|}  \FunctionTok{id} \KeywordTok{|} \ExtensionTok{last_name} \KeywordTok{|} \ExtensionTok{first_name} \KeywordTok{|} \ExtensionTok{born}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{-----+-----------+------------+-------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{1}  \KeywordTok{|} \ExtensionTok{Williams}  \KeywordTok{|} \ExtensionTok{John}       \KeywordTok{|} \ExtensionTok{1932}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{2}  \KeywordTok{|} \ExtensionTok{Elfman}    \KeywordTok{|} \ExtensionTok{Danny}      \KeywordTok{|} \ExtensionTok{1953}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{3}  \KeywordTok{|} \ExtensionTok{Horner}    \KeywordTok{|} \ExtensionTok{James}      \KeywordTok{|} \ExtensionTok{1953}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{4}  \KeywordTok{|} \ExtensionTok{Shore}     \KeywordTok{|} \ExtensionTok{Howard}     \KeywordTok{|} \ExtensionTok{1946}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{5}  \KeywordTok{|} \ExtensionTok{Zimmer}    \KeywordTok{|} \ExtensionTok{Hans}       \KeywordTok{|} \ExtensionTok{1957}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{-----+-----------+------------+-------}\KeywordTok{|}
\end{Highlighting}
\end{Shaded}

The first approach, \texttt{sed}, uses two statements. The first is to replace the header and the second is a regular expression with back references applied to the second row onwards:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{names.csv}\NormalTok{ sed -re }\StringTok{'1s/.*/id,full_name,born/g;'}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{'2,$s/(.*),(.*),(.*),(.*)/\textbackslash{}1,\textbackslash{}3 \textbackslash{}2,\textbackslash{}4/g'} \KeywordTok{|} \ExtensionTok{csvlook}\NormalTok{ -I}
\KeywordTok{|}\ExtensionTok{-----+---------------+-------}\KeywordTok{|}
\KeywordTok{|}  \FunctionTok{id} \KeywordTok{|} \ExtensionTok{full_name}     \KeywordTok{|} \ExtensionTok{born}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{-----+---------------+-------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{1}  \KeywordTok{|} \ExtensionTok{John}\NormalTok{ Williams }\KeywordTok{|} \ExtensionTok{1932}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{2}  \KeywordTok{|} \ExtensionTok{Danny}\NormalTok{ Elfman  }\KeywordTok{|} \ExtensionTok{1953}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{3}  \KeywordTok{|} \ExtensionTok{James}\NormalTok{ Horner  }\KeywordTok{|} \ExtensionTok{1953}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{4}  \KeywordTok{|} \ExtensionTok{Howard}\NormalTok{ Shore  }\KeywordTok{|} \ExtensionTok{1946}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{5}  \KeywordTok{|} \ExtensionTok{Hans}\NormalTok{ Zimmer   }\KeywordTok{|} \ExtensionTok{1957}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{-----+---------------+-------}\KeywordTok{|}
\end{Highlighting}
\end{Shaded}

The \texttt{awk} approach looks as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{names.csv}\NormalTok{ awk -F, }\StringTok{'BEGIN\{OFS=","; print "id,full_name,born"\}'}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{'\{if(NR > 1) \{print $1,$3" "$2,$4\}\}'} \KeywordTok{|} \ExtensionTok{csvlook}\NormalTok{ -I}
\KeywordTok{|}\ExtensionTok{-----+---------------+-------}\KeywordTok{|}
\KeywordTok{|}  \FunctionTok{id} \KeywordTok{|} \ExtensionTok{full_name}     \KeywordTok{|} \ExtensionTok{born}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{-----+---------------+-------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{1}  \KeywordTok{|} \ExtensionTok{John}\NormalTok{ Williams }\KeywordTok{|} \ExtensionTok{1932}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{2}  \KeywordTok{|} \ExtensionTok{Danny}\NormalTok{ Elfman  }\KeywordTok{|} \ExtensionTok{1953}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{3}  \KeywordTok{|} \ExtensionTok{James}\NormalTok{ Horner  }\KeywordTok{|} \ExtensionTok{1953}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{4}  \KeywordTok{|} \ExtensionTok{Howard}\NormalTok{ Shore  }\KeywordTok{|} \ExtensionTok{1946}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{5}  \KeywordTok{|} \ExtensionTok{Hans}\NormalTok{ Zimmer   }\KeywordTok{|} \ExtensionTok{1957}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{-----+---------------+-------}\KeywordTok{|}
\end{Highlighting}
\end{Shaded}

The \texttt{cols} approach in combination with \texttt{tr}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{names.csv} \KeywordTok{|} \ExtensionTok{cols}\NormalTok{ -c first_name,last_name tr }\DataTypeTok{\textbackslash{}"}\NormalTok{,}\DataTypeTok{\textbackslash{}"} \DataTypeTok{\textbackslash{}"} \DataTypeTok{\textbackslash{}"} \KeywordTok{|}
\OperatorTok{>} \ExtensionTok{header}\NormalTok{ -r full_name,id,born }\KeywordTok{|} \ExtensionTok{csvcut}\NormalTok{ -c id,full_name,born }\KeywordTok{|} \ExtensionTok{csvlook}\NormalTok{ -I}
\KeywordTok{|}\ExtensionTok{-----+---------------+-------}\KeywordTok{|}
\KeywordTok{|}  \FunctionTok{id} \KeywordTok{|} \ExtensionTok{full_name}     \KeywordTok{|} \ExtensionTok{born}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{-----+---------------+-------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{1}  \KeywordTok{|} \ExtensionTok{John}\NormalTok{ Williams }\KeywordTok{|} \ExtensionTok{1932}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{2}  \KeywordTok{|} \ExtensionTok{Danny}\NormalTok{ Elfman  }\KeywordTok{|} \ExtensionTok{1953}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{3}  \KeywordTok{|} \ExtensionTok{James}\NormalTok{ Horner  }\KeywordTok{|} \ExtensionTok{1953}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{4}  \KeywordTok{|} \ExtensionTok{Howard}\NormalTok{ Shore  }\KeywordTok{|} \ExtensionTok{1946}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{5}  \KeywordTok{|} \ExtensionTok{Hans}\NormalTok{ Zimmer   }\KeywordTok{|} \ExtensionTok{1957}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{-----+---------------+-------}\KeywordTok{|}
\end{Highlighting}
\end{Shaded}

Please note that \texttt{csvsql} employ SQLite as the database to execute the query and that \texttt{\textbar{}\textbar{}} stands for concatenation:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{names.csv}\NormalTok{ csvsql --query }\StringTok{"SELECT id, first_name || ' ' || last_name "}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{"AS full_name, born FROM stdin"} \KeywordTok{|} \ExtensionTok{csvlook}\NormalTok{ -I}
\KeywordTok{|}\ExtensionTok{-----+-----------------------+-------}\KeywordTok{|}
\KeywordTok{|}  \FunctionTok{id} \KeywordTok{|} \ExtensionTok{full_name}             \KeywordTok{|} \ExtensionTok{born}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{-----+-----------------------+-------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{1}  \KeywordTok{|} \ExtensionTok{John}\NormalTok{ Williams         }\KeywordTok{|} \ExtensionTok{1932}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{2}  \KeywordTok{|} \ExtensionTok{Danny}\NormalTok{ Elfman          }\KeywordTok{|} \ExtensionTok{1953}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{3}  \KeywordTok{|} \ExtensionTok{James}\NormalTok{ Horner          }\KeywordTok{|} \ExtensionTok{1953}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{4}  \KeywordTok{|} \ExtensionTok{Howard}\NormalTok{ Shore          }\KeywordTok{|} \ExtensionTok{1946}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{5}  \KeywordTok{|} \ExtensionTok{Hans}\NormalTok{ Zimmer           }\KeywordTok{|} \ExtensionTok{1957}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{-----+-----------------------+-------}\KeywordTok{|}
\end{Highlighting}
\end{Shaded}

What if \emph{last\_name} would contain a comma? Let's have a look at the raw input CSV for clarity sake:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{cat}\NormalTok{ names-comma.csv}
\FunctionTok{id}\NormalTok{,last_name,first_name,born}
\ExtensionTok{1}\NormalTok{,Williams,John,1932}
\ExtensionTok{2}\NormalTok{,Elfman,Danny,1953}
\ExtensionTok{3}\NormalTok{,Horner,James,1953}
\ExtensionTok{4}\NormalTok{,Shore,Howard,1946}
\ExtensionTok{5}\NormalTok{,Zimmer,Hans,1957}
\ExtensionTok{6}\NormalTok{,}\StringTok{"Beethoven, van"}\NormalTok{,Ludwig,1770}
\end{Highlighting}
\end{Shaded}

Well, it appears that the first three approaches fail; all in different ways. Only \texttt{csvsql} is able to combine first\_name and full\_name:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{names-comma.csv}\NormalTok{ sed -re }\StringTok{'1s/.*/id,full_name,born/g;'}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{'2,$s/(.*),(.*),(.*),(.*)/\textbackslash{}1,\textbackslash{}3 \textbackslash{}2,\textbackslash{}4/g'} \KeywordTok{|} \FunctionTok{tail}\NormalTok{ -n 1}
\ExtensionTok{6}\NormalTok{,}\StringTok{"Beethoven,Ludwig  van"}\NormalTok{,1770}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{names-comma.csv}\NormalTok{ awk -F, }\StringTok{'BEGIN\{OFS=","; print "id,full_name,born"\}'}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{'\{if(NR > 1) \{print $1,$3" "$2,$4\}\}'} \KeywordTok{|} \FunctionTok{tail}\NormalTok{ -n 1}
\ExtensionTok{6}\NormalTok{, van}\StringTok{" "}\NormalTok{Beethoven,Ludwig}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{names-comma.csv} \KeywordTok{|} \ExtensionTok{cols}\NormalTok{ -c first_name,last_name tr }\DataTypeTok{\textbackslash{}"}\NormalTok{,}\DataTypeTok{\textbackslash{}"} \DataTypeTok{\textbackslash{}"} \DataTypeTok{\textbackslash{}"} \KeywordTok{|}
\OperatorTok{>} \ExtensionTok{header}\NormalTok{ -r full_name,id,born }\KeywordTok{|} \ExtensionTok{csvcut}\NormalTok{ -c id,full_name,born }\KeywordTok{|} \FunctionTok{tail}\NormalTok{ -n 1}
\ExtensionTok{6}\NormalTok{,}\StringTok{"Ludwig ""Beethoven  van"""}\NormalTok{,1770}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{names-comma.csv}\NormalTok{ csvsql --query }\StringTok{"SELECT id, first_name || ' ' || last_name"}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{" AS full_name, born FROM stdin"} \KeywordTok{|} \FunctionTok{tail}\NormalTok{ -n 1}
\ExtensionTok{6}\NormalTok{,}\StringTok{"Ludwig Beethoven, van"}\NormalTok{,1770}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{names-comma.csv}\NormalTok{ Rio -e }\StringTok{'df$full_name <- paste(df$first_name, df$last_name);'}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{'df[c("id","full_name","born")]'} \KeywordTok{|} \FunctionTok{tail}\NormalTok{ -n 1}
\ExtensionTok{6}\NormalTok{,}\StringTok{"Ludwig Beethoven, van"}\NormalTok{,1770}
\end{Highlighting}
\end{Shaded}

Wait a minute! What's that last command? Is that R? Well, as a matter of fact, it is. It's R code evaluated through a command-line tool called \texttt{Rio} \citep{Rio}. All that we can say at this moment, is that also this approach succeeds at merging the two columns. We'll discuss this nifty command-line tool later.

\hypertarget{combining-multiple-csv-files}{%
\subsection{Combining Multiple CSV Files}\label{combining-multiple-csv-files}}

\hypertarget{concatenate-vertically}{%
\subsubsection{Concatenate Vertically}\label{concatenate-vertically}}

Vertical concatenation may be necessary in cases where you have, for example, a data set which is generated on a daily basis, or where each data set represents a different, say, market or product. Let's simulate the latter by splitting up our beloved Iris data set into three CSV files, so that we have something to combine again. We'll use \texttt{fieldsplit} \citep{fieldsplit}, which is part of the \texttt{CRUSH} suite of command-line tools:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{iris.csv}\NormalTok{ fieldsplit -d, -k -F species -p . -s .csv}
\end{Highlighting}
\end{Shaded}

Here, the command-line arguments specify: the delimiter (\texttt{-d}), that we want to keep the header in each file (\texttt{-k}), the column whose values dictate the possible output files (\texttt{-F}), the relative output path (\texttt{-p}), and the filename suffix (\texttt{-s}), respectively. Because the \emph{species} column in the Iris data set contains three different values, we end up with three CSV files, each with 50 data points and a header:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{wc}\NormalTok{ -l Iris-*.csv}
  \ExtensionTok{51}\NormalTok{ Iris-setosa.csv}
  \ExtensionTok{51}\NormalTok{ Iris-versicolor.csv}
  \ExtensionTok{51}\NormalTok{ Iris-virginica.csv}
 \ExtensionTok{153}\NormalTok{ total}
\end{Highlighting}
\end{Shaded}

You could just concatenate the files back using \texttt{cat} and removing the headers of all but the first file using \texttt{header\ -d} as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{cat}\NormalTok{ Iris-setosa.csv }\OperatorTok{<(<} \ExtensionTok{Iris-versicolor.csv}\NormalTok{ header -d}\OperatorTok{)}\NormalTok{ \textbackslash{}}
\OperatorTok{>} \OperatorTok{<(<} \ExtensionTok{Iris-virginica.csv}\NormalTok{ header -d}\OperatorTok{)} \KeywordTok{|} \FunctionTok{sed}\NormalTok{ -n }\StringTok{'1p;49,54p'} \KeywordTok{|} \ExtensionTok{csvlook}
\KeywordTok{|}\ExtensionTok{---------------+-------------+--------------+-------------+------------------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{sepal_length} \KeywordTok{|} \ExtensionTok{sepal_width} \KeywordTok{|} \ExtensionTok{petal_length} \KeywordTok{|} \ExtensionTok{petal_width} \KeywordTok{|} \ExtensionTok{species}          \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{---------------+-------------+--------------+-------------+------------------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{4.6}          \KeywordTok{|} \ExtensionTok{3.2}         \KeywordTok{|} \ExtensionTok{1.4}          \KeywordTok{|} \ExtensionTok{0.2}         \KeywordTok{|} \ExtensionTok{Iris-setosa}      \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{5.3}          \KeywordTok{|} \ExtensionTok{3.7}         \KeywordTok{|} \ExtensionTok{1.5}          \KeywordTok{|} \ExtensionTok{0.2}         \KeywordTok{|} \ExtensionTok{Iris-setosa}      \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{5.0}          \KeywordTok{|} \ExtensionTok{3.3}         \KeywordTok{|} \ExtensionTok{1.4}          \KeywordTok{|} \ExtensionTok{0.2}         \KeywordTok{|} \ExtensionTok{Iris-setosa}      \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{7.0}          \KeywordTok{|} \ExtensionTok{3.2}         \KeywordTok{|} \ExtensionTok{4.7}          \KeywordTok{|} \ExtensionTok{1.4}         \KeywordTok{|} \ExtensionTok{Iris-versicolor}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{6.4}          \KeywordTok{|} \ExtensionTok{3.2}         \KeywordTok{|} \ExtensionTok{4.5}          \KeywordTok{|} \ExtensionTok{1.5}         \KeywordTok{|} \ExtensionTok{Iris-versicolor}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{6.9}          \KeywordTok{|} \ExtensionTok{3.1}         \KeywordTok{|} \ExtensionTok{4.9}          \KeywordTok{|} \ExtensionTok{1.5}         \KeywordTok{|} \ExtensionTok{Iris-versicolor}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{---------------+-------------+--------------+-------------+------------------}\KeywordTok{|}
\end{Highlighting}
\end{Shaded}

Note that we're merely using \texttt{sed} to only print the header and the first three body rows that belonged to the second file in order to illustrate success. While this method works, it's easier (and less prone to errors) to use \texttt{csvstack} \citep{csvstack}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{csvstack}\NormalTok{ Iris-*.csv }\KeywordTok{|} \FunctionTok{sed}\NormalTok{ -n }\StringTok{'1p;49,54p'} \KeywordTok{|} \ExtensionTok{csvlook}
\KeywordTok{|}\ExtensionTok{---------------+-------------+--------------+-------------+------------------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{sepal_length} \KeywordTok{|} \ExtensionTok{sepal_width} \KeywordTok{|} \ExtensionTok{petal_length} \KeywordTok{|} \ExtensionTok{petal_width} \KeywordTok{|} \ExtensionTok{species}          \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{---------------+-------------+--------------+-------------+------------------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{4.6}          \KeywordTok{|} \ExtensionTok{3.2}         \KeywordTok{|} \ExtensionTok{1.4}          \KeywordTok{|} \ExtensionTok{0.2}         \KeywordTok{|} \ExtensionTok{Iris-setosa}      \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{5.3}          \KeywordTok{|} \ExtensionTok{3.7}         \KeywordTok{|} \ExtensionTok{1.5}          \KeywordTok{|} \ExtensionTok{0.2}         \KeywordTok{|} \ExtensionTok{Iris-setosa}      \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{5.0}          \KeywordTok{|} \ExtensionTok{3.3}         \KeywordTok{|} \ExtensionTok{1.4}          \KeywordTok{|} \ExtensionTok{0.2}         \KeywordTok{|} \ExtensionTok{Iris-setosa}      \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{7.0}          \KeywordTok{|} \ExtensionTok{3.2}         \KeywordTok{|} \ExtensionTok{4.7}          \KeywordTok{|} \ExtensionTok{1.4}         \KeywordTok{|} \ExtensionTok{Iris-versicolor}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{6.4}          \KeywordTok{|} \ExtensionTok{3.2}         \KeywordTok{|} \ExtensionTok{4.5}          \KeywordTok{|} \ExtensionTok{1.5}         \KeywordTok{|} \ExtensionTok{Iris-versicolor}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{6.9}          \KeywordTok{|} \ExtensionTok{3.1}         \KeywordTok{|} \ExtensionTok{4.9}          \KeywordTok{|} \ExtensionTok{1.5}         \KeywordTok{|} \ExtensionTok{Iris-versicolor}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{---------------+-------------+--------------+-------------+------------------}\KeywordTok{|}
\end{Highlighting}
\end{Shaded}

If the species column did not exist, you can create a new column based on the filename using \texttt{csvstack}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{csvstack}\NormalTok{ Iris-*.csv -n species --filenames}
\end{Highlighting}
\end{Shaded}

Alternatively, you could specify the group names using \texttt{-g}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{csvstack}\NormalTok{ Iris-*.csv -n class -g a,b,c }\KeywordTok{|} \ExtensionTok{csvcut}\NormalTok{ -C species }\KeywordTok{|}
\OperatorTok{>} \FunctionTok{sed}\NormalTok{ -n }\StringTok{'1p;49,54p'} \KeywordTok{|} \ExtensionTok{csvlook}
\KeywordTok{|}\ExtensionTok{--------+--------------+-------------+--------------+--------------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{class} \KeywordTok{|} \ExtensionTok{sepal_length} \KeywordTok{|} \ExtensionTok{sepal_width} \KeywordTok{|} \ExtensionTok{petal_length} \KeywordTok{|} \ExtensionTok{petal_width}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{--------+--------------+-------------+--------------+--------------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{a}     \KeywordTok{|} \ExtensionTok{4.6}          \KeywordTok{|} \ExtensionTok{3.2}         \KeywordTok{|} \ExtensionTok{1.4}          \KeywordTok{|} \ExtensionTok{0.2}          \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{a}     \KeywordTok{|} \ExtensionTok{5.3}          \KeywordTok{|} \ExtensionTok{3.7}         \KeywordTok{|} \ExtensionTok{1.5}          \KeywordTok{|} \ExtensionTok{0.2}          \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{a}     \KeywordTok{|} \ExtensionTok{5.0}          \KeywordTok{|} \ExtensionTok{3.3}         \KeywordTok{|} \ExtensionTok{1.4}          \KeywordTok{|} \ExtensionTok{0.2}          \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{b}     \KeywordTok{|} \ExtensionTok{7.0}          \KeywordTok{|} \ExtensionTok{3.2}         \KeywordTok{|} \ExtensionTok{4.7}          \KeywordTok{|} \ExtensionTok{1.4}          \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{b}     \KeywordTok{|} \ExtensionTok{6.4}          \KeywordTok{|} \ExtensionTok{3.2}         \KeywordTok{|} \ExtensionTok{4.5}          \KeywordTok{|} \ExtensionTok{1.5}          \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{b}     \KeywordTok{|} \ExtensionTok{6.9}          \KeywordTok{|} \ExtensionTok{3.1}         \KeywordTok{|} \ExtensionTok{4.9}          \KeywordTok{|} \ExtensionTok{1.5}          \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{--------+--------------+-------------+--------------+--------------}\KeywordTok{|}
\end{Highlighting}
\end{Shaded}

The new column \emph{class} is added at the front. If you'd like to change the order you can use \texttt{csvcut} as discussed earlier in this section.

\hypertarget{concatenate-horizontally}{%
\subsubsection{Concatenate Horizontally}\label{concatenate-horizontally}}

Let's say you have three CSV files that want to put side by side. We use \texttt{tee} \citep{tee} to save the result of \texttt{csvcut} in the middle of the pipeline:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{data/tips.csv}\NormalTok{ csvcut -c bill,tip }\KeywordTok{|} \FunctionTok{tee}\NormalTok{ data/bills.csv }\KeywordTok{|} \FunctionTok{head}\NormalTok{ -n 3 }\KeywordTok{|} \ExtensionTok{csvlook}
\KeywordTok{|}\ExtensionTok{--------+-------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{bill}  \KeywordTok{|} \ExtensionTok{tip}   \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{--------+-------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{16.99} \KeywordTok{|} \ExtensionTok{1.01}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{10.34} \KeywordTok{|} \ExtensionTok{1.66}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{--------+-------}\KeywordTok{|}
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{data/tips.csv}\NormalTok{ csvcut -c day,time }\KeywordTok{|} \FunctionTok{tee}\NormalTok{ data/datetime.csv }\KeywordTok{|}
\OperatorTok{>} \FunctionTok{head}\NormalTok{ -n 3 }\KeywordTok{|} \ExtensionTok{csvlook}\NormalTok{ -I}
\KeywordTok{|}\ExtensionTok{------+---------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{day} \KeywordTok{|} \BuiltInTok{time}    \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{------+---------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{Sun} \KeywordTok{|} \ExtensionTok{Dinner}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{Sun} \KeywordTok{|} \ExtensionTok{Dinner}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{------+---------}\KeywordTok{|}
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{data/tips.csv}\NormalTok{ csvcut -c sex,smoker,size }\KeywordTok{|} \FunctionTok{tee}\NormalTok{ data/customers.csv }\KeywordTok{|}
\OperatorTok{>} \FunctionTok{head}\NormalTok{ -n 3 }\KeywordTok{|} \ExtensionTok{csvlook}
\KeywordTok{|}\ExtensionTok{---------+--------+-------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{sex}    \KeywordTok{|} \ExtensionTok{smoker} \KeywordTok{|} \FunctionTok{size}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{---------+--------+-------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{Female} \KeywordTok{|} \ExtensionTok{No}     \KeywordTok{|} \ExtensionTok{2}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{Male}   \KeywordTok{|} \ExtensionTok{No}     \KeywordTok{|} \ExtensionTok{3}     \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{---------+--------+-------}\KeywordTok{|}
\end{Highlighting}
\end{Shaded}

Assuming that the rows line up, you can simply \texttt{paste} \citep{paste} the files together:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{paste}\NormalTok{ -d, data/}\DataTypeTok{\{bills,customers,datetime\}}\NormalTok{.csv }\KeywordTok{|} \FunctionTok{head}\NormalTok{ -n 3 }\KeywordTok{|} \ExtensionTok{csvlook}\NormalTok{ -I}
\KeywordTok{|}\ExtensionTok{--------+------+--------+--------+------+-----+---------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{bill}  \KeywordTok{|} \ExtensionTok{tip}  \KeywordTok{|} \ExtensionTok{sex}    \KeywordTok{|} \ExtensionTok{smoker} \KeywordTok{|} \FunctionTok{size} \KeywordTok{|} \ExtensionTok{day} \KeywordTok{|} \BuiltInTok{time}    \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{--------+------+--------+--------+------+-----+---------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{16.99} \KeywordTok{|} \ExtensionTok{1.01} \KeywordTok{|} \ExtensionTok{Female} \KeywordTok{|} \ExtensionTok{No}     \KeywordTok{|} \ExtensionTok{2}    \KeywordTok{|} \ExtensionTok{Sun} \KeywordTok{|} \ExtensionTok{Dinner}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{10.34} \KeywordTok{|} \ExtensionTok{1.66} \KeywordTok{|} \ExtensionTok{Male}   \KeywordTok{|} \ExtensionTok{No}     \KeywordTok{|} \ExtensionTok{3}    \KeywordTok{|} \ExtensionTok{Sun} \KeywordTok{|} \ExtensionTok{Dinner}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{--------+------+--------+--------+------+-----+---------}\KeywordTok{|}
\end{Highlighting}
\end{Shaded}

Here, the command-line argument \texttt{-d} instructs \texttt{paste} to use a comma as the delimiter.

\hypertarget{joining}{%
\subsubsection{Joining}\label{joining}}

Sometimes data cannot simply by combined by vertical or horizontal concatenation. In some cases, especially in relational databases, the data is spread over multiple tables (or files) in order to minimize redundancy. Imagine we wanted to extend the Iris data set with more information about the three types of Iris flowers, namely the USDA identifier. It so happens that we have separate CSV file with these identifiers:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{csvlook}\NormalTok{ irismeta.csv}
\KeywordTok{|}\ExtensionTok{------------------+----------------------------------------------+----------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{species}         \KeywordTok{|} \ExtensionTok{wikipedia_url}                                \KeywordTok{|} \ExtensionTok{usda_id}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{------------------+----------------------------------------------+----------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{Iris-versicolor} \KeywordTok{|} \ExtensionTok{http}\NormalTok{://en.wikipedia.org/wiki/Iris_versicolor }\KeywordTok{|} \ExtensionTok{IRVE2}    \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{Iris-virginica}  \KeywordTok{|} \ExtensionTok{http}\NormalTok{://en.wikipedia.org/wiki/Iris_virginica  }\KeywordTok{|} \ExtensionTok{IRVI}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{Iris-setosa}     \KeywordTok{|}                                              \KeywordTok{|} \ExtensionTok{IRSE}     \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{------------------+----------------------------------------------+----------}\KeywordTok{|}
\end{Highlighting}
\end{Shaded}

What this data set and the Iris data set have in common is the species column. We can use \texttt{csvjoin} \citep{csvjoin} to join the two data sets:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{csvjoin}\NormalTok{ -c species iris.csv irismeta.csv }\KeywordTok{|} \ExtensionTok{csvcut}\NormalTok{ -c sepal_length,\textbackslash{}}
\OperatorTok{>}\NormalTok{ sepal_width,species,usda_id }\KeywordTok{|} \FunctionTok{sed}\NormalTok{ -n }\StringTok{'1p;49,54p'} \KeywordTok{|} \ExtensionTok{csvlook}
\KeywordTok{|}\ExtensionTok{---------------+-------------+-----------------+----------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{sepal_length} \KeywordTok{|} \ExtensionTok{sepal_width} \KeywordTok{|} \ExtensionTok{species}         \KeywordTok{|} \ExtensionTok{usda_id}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{---------------+-------------+-----------------+----------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{4.6}          \KeywordTok{|} \ExtensionTok{3.2}         \KeywordTok{|} \ExtensionTok{Iris-setosa}     \KeywordTok{|} \ExtensionTok{IRSE}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{5.3}          \KeywordTok{|} \ExtensionTok{3.7}         \KeywordTok{|} \ExtensionTok{Iris-setosa}     \KeywordTok{|} \ExtensionTok{IRSE}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{5.0}          \KeywordTok{|} \ExtensionTok{3.3}         \KeywordTok{|} \ExtensionTok{Iris-setosa}     \KeywordTok{|} \ExtensionTok{IRSE}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{7.0}          \KeywordTok{|} \ExtensionTok{3.2}         \KeywordTok{|} \ExtensionTok{Iris-versicolor} \KeywordTok{|} \ExtensionTok{IRVE2}    \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{6.4}          \KeywordTok{|} \ExtensionTok{3.2}         \KeywordTok{|} \ExtensionTok{Iris-versicolor} \KeywordTok{|} \ExtensionTok{IRVE2}    \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{6.9}          \KeywordTok{|} \ExtensionTok{3.1}         \KeywordTok{|} \ExtensionTok{Iris-versicolor} \KeywordTok{|} \ExtensionTok{IRVE2}    \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{---------------+-------------+-----------------+----------}\KeywordTok{|}
\end{Highlighting}
\end{Shaded}

Of course we can also use the SQL approach using \texttt{csvsql}, which is, as per usual, a bit longer (but potentially much more flexible):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{csvsql}\NormalTok{ --query }\StringTok{'SELECT i.sepal_length, i.sepal_width, i.species, m.usda_id '}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{'FROM iris i JOIN irismeta m ON (i.species = m.species)'}\NormalTok{ \textbackslash{}}
\OperatorTok{>}\NormalTok{ iris.csv irismeta.csv }\KeywordTok{|} \FunctionTok{sed}\NormalTok{ -n }\StringTok{'1p;49,54p'} \KeywordTok{|} \ExtensionTok{csvlook}
\KeywordTok{|}\ExtensionTok{---------------+-------------+-----------------+----------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{sepal_length} \KeywordTok{|} \ExtensionTok{sepal_width} \KeywordTok{|} \ExtensionTok{species}         \KeywordTok{|} \ExtensionTok{usda_id}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{---------------+-------------+-----------------+----------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{4.6}          \KeywordTok{|} \ExtensionTok{3.2}         \KeywordTok{|} \ExtensionTok{Iris-setosa}     \KeywordTok{|} \ExtensionTok{IRSE}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{5.3}          \KeywordTok{|} \ExtensionTok{3.7}         \KeywordTok{|} \ExtensionTok{Iris-setosa}     \KeywordTok{|} \ExtensionTok{IRSE}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{5.0}          \KeywordTok{|} \ExtensionTok{3.3}         \KeywordTok{|} \ExtensionTok{Iris-setosa}     \KeywordTok{|} \ExtensionTok{IRSE}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{7.0}          \KeywordTok{|} \ExtensionTok{3.2}         \KeywordTok{|} \ExtensionTok{Iris-versicolor} \KeywordTok{|} \ExtensionTok{IRVE2}    \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{6.4}          \KeywordTok{|} \ExtensionTok{3.2}         \KeywordTok{|} \ExtensionTok{Iris-versicolor} \KeywordTok{|} \ExtensionTok{IRVE2}    \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{6.9}          \KeywordTok{|} \ExtensionTok{3.1}         \KeywordTok{|} \ExtensionTok{Iris-versicolor} \KeywordTok{|} \ExtensionTok{IRVE2}    \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{---------------+-------------+-----------------+----------}\KeywordTok{|}
\end{Highlighting}
\end{Shaded}

\hypertarget{further-reading}{%
\section{Further Reading}\label{further-reading}}

\begin{itemize}
\tightlist
\item
  Molinaro, Anthony. 2005. SQL Cookbook. O'Reilly Media.
\item
  Goyvaerts, Jan, and Steven Levithan. 2012. Regular Expressions Cookbook. 2nd Ed. O'Reilly Media.
\item
  Dougherty, Dale, and Arnold Robbins. 1997. Sed \& Awk. 2nd Ed. O'Reilly Media.
\end{itemize}

\hypertarget{chapter-6-managing-your-data-workflow}{%
\chapter{Managing Your Data Workflow}\label{chapter-6-managing-your-data-workflow}}

We hope that by now, you have come to appreciate that the command line is a very convenient environment for exploratory data analysis. You may have noticed that, as a consequence of working with the command line, we:

\begin{itemize}
\tightlist
\item
  Invoke many different commands.
\item
  Create custom command-line tools.
\item
  Obtain and generate many (intermediate) files.
\end{itemize}

As this process is of an exploratory nature, our workflow tends to be rather chaotic, which makes it difficult to keep track of what we've done. It is very important that our steps can be reproduced, whether that is by ourselves or by others. When we, for example, continue with a project from a few weeks earlier, chances are that we have forgotten which commands we have ran, on which files, in which order, and with which parameters. Imagine the difficulty passing on your analysis to a collaborator.

You may recover some lost commands by digging into your Bash history, but this is, of course, not a good approach. A better approach would be to save your commands to a shell script \emph{run.sh}. This allows you and your collaborators to at least reproduce the analysis. A shell script is, however, a sub-optimal approach because:

\begin{itemize}
\tightlist
\item
  It is difficult to read and to maintain.
\item
  Dependencies between steps are unclear.
\item
  Every step gets executed every time, which is inefficient and sometimes undesirable.
\end{itemize}

This is where Drake comes in handy \citep{drake}. Drake is command-line tool created by Factual that allows you to:

\begin{itemize}
\tightlist
\item
  Formalize your data workflow steps in terms of input and output dependencies.
\item
  Run specific steps of your workflow from the command line.
\item
  Use inline code.
\item
  Store and retrieve data from external sources.
\end{itemize}

\hypertarget{overview}{%
\section{Overview}\label{overview}}

Managing your data workflow with Drake is the main topic of this chapter. As such, you'll learn about:

\begin{itemize}
\tightlist
\item
  Defining your workflow with a so-called Drakefile.
\item
  Thinking about your workflow in terms of input and output dependencies.
\item
  Build specific targets.
\end{itemize}

\hypertarget{introducing-drake}{%
\section{Introducing Drake}\label{introducing-drake}}

Drake organizes command execution around data and its dependencies. Your data processing steps are formalized in a separate text file (a workflow). Each step usually has one or more inputs and outputs. Drake automatically resolves their dependencies and determines which commands need to be run and in which order.

This means that when you have, say, an SQL query that takes ten minutes, it only has to be executed when the result is missing or when the query has changed afterwards. Also, if want to (re-)run a specific step, Drake only considers to (re-)run the steps on which it depends. This can save you a lot of time.

The benefit of having a formalized workflow allows you to pick up easily your project after a few weeks and to collaborate with others. We strongly advise you to do this, even when you think this will be a one-off project, because you'll never know when to run certain steps again, or when you want to reuse certain steps in another project.

\hypertarget{installing-drake}{%
\section{Installing Drake}\label{installing-drake}}

Drake has quite a few dependencies, which makes its installation process rather involved. For the following instructions we assume that you are on Ubuntu.

\begin{rmdtip}
If you're using the Data Science Toolbox, then you already have Drake installed, and you may safely skip this section.
\end{rmdtip}

Drake is written in the programming language Clojure which means that it runs on the Java Virtual Machine (JVM). There are pre-built jars available but because Drake is in active development, we will build it from source. For this, you will need to install Leiningen.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{sudo}\NormalTok{ apt-get install openjdk-6-jdk}
\NormalTok{$ }\FunctionTok{sudo}\NormalTok{ apt-get install leiningen}
\end{Highlighting}
\end{Shaded}

Then, clone the Drake repository from Factual:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{git}\NormalTok{ clone https://github.com/Factual/drake.git}
\end{Highlighting}
\end{Shaded}

And build the uberjar using Leiningen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{cd}\NormalTok{ drake}
\NormalTok{$ }\ExtensionTok{lein}\NormalTok{ uberjar}
\end{Highlighting}
\end{Shaded}

This creates \emph{drake.jar}. Copy this file to a directory which is on your \emph{\$PATH}, for example, \emph{\textasciitilde{}/bin}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{mv}\NormalTok{ drake.jar ~/bin/}
\end{Highlighting}
\end{Shaded}

At this point you should already be able to run Drake:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{cd}\NormalTok{ ~/bin/}
\NormalTok{$ }\ExtensionTok{java}\NormalTok{ -jar drake.jar}
\end{Highlighting}
\end{Shaded}

This is not really convenient for two reasons: (1) the Java Virtual Machine (JVM) takes a long time to start and (2) you can only run it from that directory. We advise you to install Drip, which is a launcher for the JVM that provides much faster startup times than the \texttt{java} command. First, clone the Drip repository from Flatland:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{git}\NormalTok{ clone https://github.com/flatland/drip.git}
\NormalTok{$ }\BuiltInTok{cd}\NormalTok{ drip}
\NormalTok{$ }\FunctionTok{make}\NormalTok{ prefix=~/bin install}
\end{Highlighting}
\end{Shaded}

Then, create a Bash script that allows you to run Drake from everywhere:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{cd}\NormalTok{ ~/bin}
\NormalTok{$ }\FunctionTok{cat} \OperatorTok{<< 'EOF'} \OperatorTok{>} \ExtensionTok{drake}
\NormalTok{> #!/bin/bash}
\NormalTok{> drip -cp $(dirname $0)/drake.jar drake.core "$@"}
\NormalTok{> EOF}
\NormalTok{$ chmod +x drake}
\end{Highlighting}
\end{Shaded}

To verify that you have correctly installed both Drake and Drip, you can run the following command, preferably from a different directory:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{drake}\NormalTok{ --version}
\ExtensionTok{Drake}\NormalTok{ Version 0.1.6}
\end{Highlighting}
\end{Shaded}

\begin{rmdnote}
Drip speeds up Java because it reserves an instance of the JVM, after it has been run once. Because of this, you will only notice the speed up from the second time onwards.
\end{rmdnote}

\hypertarget{obtain-top-e-books-from-project-gutenberg}{%
\section{Obtain Top E-books from Project Gutenberg}\label{obtain-top-e-books-from-project-gutenberg}}

For the remainder of this chapter, we'll use the following task as a running example. Our goal is to turn the command that we use to solve this task into a Drake workflow. We start out simple, and work our way towards an advanced workflow in order to explain to you the various concepts and syntax of Drake.

Project Gutenberg is an ambitious project that, since 1971, has archived and digitized over 42,000 books and offers these as free e-books. On its website you can find the top hundred most downloaded books. Let's assume that we are interested in the top five downloads of Project Gutenberg. Because this list is available in HTML it is straightforward to obtain the top five downloads:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{curl}\NormalTok{ -s }\StringTok{'http://www.gutenberg.org/browse/scores/top'} \KeywordTok{|}  
\OperatorTok{>} \FunctionTok{grep}\NormalTok{ -E }\StringTok{'^<li>'} \KeywordTok{|}                                       
\OperatorTok{>} \FunctionTok{head}\NormalTok{ -n 5 }\KeywordTok{|}                                             
\OperatorTok{>} \FunctionTok{sed}\NormalTok{ -E }\StringTok{"s/.*ebooks\textbackslash{}/([0-9]+).*/}\DataTypeTok{\textbackslash{}\textbackslash{}}\StringTok{1/"} \OperatorTok{>}\NormalTok{ data/top-5       }
\end{Highlighting}
\end{Shaded}

This command:

\begin{itemize}
\tightlist
\item
  Downloads the HTML.
\item
  Extracts the list items.
\item
  Keeps only the top five items.
\item
  Saves e-book IDs to \emph{data/top-5}.
\end{itemize}

The output of the command is:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{cat}\NormalTok{ data/top-5}
\ExtensionTok{1342}
\ExtensionTok{76}
\ExtensionTok{11}
\ExtensionTok{1661}
\ExtensionTok{1952}
\end{Highlighting}
\end{Shaded}

If you want to be able to reproduce this, that is, once again at a later time, the easiest thing you can do is put this command in a script as we've seen in \protect\hyperlink{chapter-4-creating-reusable-command-line-tools}{Chapter 4}. If you execute this script again, the HTML will be downloaded again as well. There are three common reasons why you might want to be able to control whether certain steps are run. First, because this step may take a very long time. Second, because you want to continue with the same data. Third, the data may come from an API which has certain rate limits. It would be a good idea to let one step save the data to a file and then let subsequent steps operate on that file so that you don't have to make any redundant computations or API calls. Now, the first reason is not really a problem in our example because the HTML can be downloaded fast enough. However, in some cases the data may come from other sources and may comprise of gigabytes of data.

\hypertarget{every-workflow-starts-with-a-single-step}{%
\section{Every Workflow Starts with a Single Step}\label{every-workflow-starts-with-a-single-step}}

In this section we'll convert the above command into a Drake workflow. A workflow is just a text file. You'd usually name this file \emph{Drakefile} because Drake uses that file if no other file is specified at the command line. A workflow with just a single step would look like Example \ref{exm:drakefile}.

\begin{example}[A workflow with just a single stip]
\protect\hypertarget{exm:drakefile}{}{\label{exm:drakefile} \iffalse (A workflow with just a single stip) \fi{} }
\end{example}

\begin{verbatim}
top-5 <-                                                    
    curl -s 'http://www.gutenberg.org/browse/scores/top' |  
    grep -E '^<li>' |                                       
    head -n 5 |                                             
    sed -E "s/.*ebooks\/([0-9]+).*/\\1/" > top-5            
\end{verbatim}

Let's go through this file. The first line, which contains the arrow pointing to the left, is our step definition. The left side of this arrow, which says \emph{top-5}, is the name or output of this step. Any inputs to this step would appear on the right side of this arrow, but since this step has no input, it's empty. Defining inputs and outputs is what allows Drake to recognize the dependencies between steps, and to figure out whether and when which steps need to be executed in order to fulfill a certain output. This output is also known as a target. As you can see, the body of this step is literally our command from before but then indented.

\begin{itemize}
\tightlist
\item
  The arrow (\emph{←}) denotes the name of the step and its dependencies. More on this later.
\item
  The body is indented.
\item
  Select only list items.
\item
  Get the first five items.
\item
  Extract the id, and save to file \emph{top-5}. Note that \emph{top-5} was already specified in the step definition and that \emph{5} has now been used three times. We are going to address that later.
\end{itemize}

This workflow is as simple as it gets. It doesn't offer any advantages over having our command in a Bash script. But don't worry, we promise you that it will get more exciting. For now, let's run Drake and see what it does with our first workflow:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{drake}
\ExtensionTok{The}\NormalTok{ following steps will be run, in order:}
  \ExtensionTok{1}\NormalTok{: top-5 }\OperatorTok{<}\NormalTok{-  [missing output]}
\ExtensionTok{Confirm?}\NormalTok{ [y/n] y}
\ExtensionTok{Running}\NormalTok{ 1 steps with concurrence of 1...}

\ExtensionTok{---}\NormalTok{ 0. Running (missing output)}\BuiltInTok{:}\NormalTok{ top-5 }\OperatorTok{<}\NormalTok{-}
\ExtensionTok{---}\NormalTok{ 0: top-5 }\OperatorTok{<}\NormalTok{-  -}\OperatorTok{>}\NormalTok{ done in 0.35s}
\ExtensionTok{Done}\NormalTok{ (1 steps run)}\ExtensionTok{.}
\end{Highlighting}
\end{Shaded}

If we do not specify any specific workflow file, then Drake will use \emph{./Drakefile}. Drake first determines which steps need to be run. In our case, the one and only step will be run because it's missing the output. This means that there's no file named \emph{data/top-5}. Drake asks for confirmation before it will execute these steps. We press \texttt{y}, and very soon thereafter we see that Drake is done. Drake did not complain about any errors in our steps. Let's verify that we have the top five books by looking at the output file \emph{data/top-5}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{cat}\NormalTok{ data/top-5}
\ExtensionTok{1342}
\ExtensionTok{76}
\ExtensionTok{11}
\ExtensionTok{1661}
\ExtensionTok{1952}
\end{Highlighting}
\end{Shaded}

Now we do have the output file. Let's run Drake again:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{drake}
\ExtensionTok{The}\NormalTok{ following steps will be run, in order:}
  \ExtensionTok{1}\NormalTok{: top-5 }\OperatorTok{<}\NormalTok{-  [no-input step]}
\ExtensionTok{Confirm?}\NormalTok{ [y/n] n}
\ExtensionTok{Aborted.}
\end{Highlighting}
\end{Shaded}

As you can see, Drake wants to execute the step again! However, now mentions a different reason, namely, that there is no input step \emph{\[no-input-step\]}. Its default behavior is to check whether the input has changed by looking at the timestamp of the input. However, since we didn't specify any input, Drake doesn't know whether or not this step should be run again. We can disable this default behavior to check timestamps as follows:

\begin{verbatim}
top-5 <- [-timecheck]
    curl -s 'http://www.gutenberg.org/browse/scores/top' |
    grep -E '^<li>' |
    head -n 5 |
    sed -E "s/.*ebooks\/([0-9]+)\">([^<]+)<.*/\\1,\\2/" > top-5
\end{verbatim}

The square brackets around \emph{\[-timecheck\]} indicate that this is an option to the step. The minus (\emph{-}) means that we wish to disable checking timestamps. Now, this step is only run when the output is missing.

We're going to use different filenames so that we keep old versions. We can specify a different workflow name (other than \emph{Drakefile}) with the \texttt{-w} option. Let's run Drake once more:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{mv}\NormalTok{ Drakefile 01.drake}
\NormalTok{$ }\ExtensionTok{drake}\NormalTok{ -w 01.drake}
\ExtensionTok{Nothing}\NormalTok{ to do.}
\end{Highlighting}
\end{Shaded}

Our very first workflow is already saving us time because Drake detects that the step was not required to be executed again. However, we can do much better than this. This workflow has three shortcomings that we're going to address in the next section.

\hypertarget{well-that-depends}{%
\section{Well, That Depends}\label{well-that-depends}}

Our workflow contains just a single step, which means that, just like having a simple Bash script, everything will be executed all the time. So the first thing we are going to do is to split up this single step into two steps, where the first step downloads the HTML, and the second step processes this HTML. The second step obviously depends on the first step. We can define this dependency in our workflow.

You may have noticed that the number 5 is specified three times. If you ever wanted to get the top, say, top 10 e-books from Project Gutenberg, we would have to change our workflow in three places. This is inefficient and needs to be addressed. Luckily, Drake supports variables.

It may not be immediately obvious from our workflow, but our data resides in the same location as the script. It is better to have the data live in a separate location and have it separated from any code that generates this data. Not only does it keep our project cleaner, it also allows us to delete the generated data files easier, and we can easily specify that we do not like the data files to be included in any version control system such as \texttt{git} \citep{git}. Let's have a look:

\begin{verbatim}
NUM:=5                                                              
BASE=data/                                                          

top.html <- [-timecheck]
    curl -s 'http://www.gutenberg.org/browse/scores/top' > $OUTPUT  

top-$[NUM] <- top.html                                              
    < $INPUT grep -E '^<li>' |
    head -n $[NUM] |
    sed -E "s/.*ebooks\/([0-9]+)\">([^<]+)<.*/\\1,\\2/" > $OUTPUT
\end{verbatim}

\begin{itemize}
\tightlist
\item
  You can specify variables in Drake, preferably at the beginning of the file, by specifying the variable name, then an equal sign, and then the value. The name of the variable doesn't have to be in all capitals, but it does make them stand out more. As you can see, we have used for the variable \emph{NUM} the \emph{:=} instead of \emph{=}. This means that if the variable \emph{NUM} is already set, it will not be overridden. This allows us to specify the value of \emph{NUM} from the command line before we run Drake.
\item
  The \emph{BASE} variable is a special variable. Drake will treat every file specified in the workflow as if it were in this base directory.
\item
  We now have two steps. The first step has the same input as before, but now the output is a different file, namely, \emph{top.html}. This output is defined again as the input of step two. This is how Drake knows that the second step depends on the first step.
\item
  We have used two more special variables: \emph{INPUT} and \emph{OUTPUT}. Values of these two special variables are set to what we have defined as the input and output of that step, respectively. This way, we don't have to specify the input and output of a certain step twice. Furthermore, it allows us to easily reuse certain steps in any future workflows.
\end{itemize}

Let's execute this new workflow using Drake:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{drake}\NormalTok{ -w 02.drake}
\ExtensionTok{The}\NormalTok{ following steps will be run, in order:}
  \ExtensionTok{1}\NormalTok{: ../../data/top.html }\OperatorTok{<}\NormalTok{-  [missing output]}
  \ExtensionTok{2}\NormalTok{: ../../data/top-5 }\OperatorTok{<}\NormalTok{- ../../data/top.html [projected timestamped]}
\ExtensionTok{Confirm?}\NormalTok{ [y/n] y}
\ExtensionTok{Running}\NormalTok{ 2 steps with concurrence of 1...}

\ExtensionTok{---}\NormalTok{ 0. Running (missing output)}\BuiltInTok{:}\NormalTok{ ../../data/top.html }\OperatorTok{<}\NormalTok{-}
\ExtensionTok{---}\NormalTok{ 0: ../../data/top.html }\OperatorTok{<}\NormalTok{-  -}\OperatorTok{>}\NormalTok{ done in 0.89s}
\ExtensionTok{---}\NormalTok{ 1. Running (missing output)}\BuiltInTok{:}\NormalTok{ ../../data/top-5 }\OperatorTok{<}\NormalTok{- ../../data/top.html}
\ExtensionTok{---}\NormalTok{ 1: ../../data/top-5 }\OperatorTok{<}\NormalTok{- ../../data/top.html -}\OperatorTok{>}\NormalTok{ done in 0.02s}
\ExtensionTok{Done}\NormalTok{ (2 steps run)}\ExtensionTok{.}
\end{Highlighting}
\end{Shaded}

Now, let's assume that we want instead of the top 5 e-books, the top 10 e-books. We can set the \emph{NUM} variable from the command line and run Drake again:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\VariableTok{NUM=}\NormalTok{10 }\ExtensionTok{drake}\NormalTok{ -w 02.drake}
\ExtensionTok{The}\NormalTok{ following steps will be run, in order:}
  \ExtensionTok{1}\NormalTok{: ../../data/top-10 }\OperatorTok{<}\NormalTok{- ../../data/top.html [missing output]}
\ExtensionTok{Confirm?}\NormalTok{ [y/n] y}
\ExtensionTok{Running}\NormalTok{ 1 steps with concurrence of 1...}

\ExtensionTok{---}\NormalTok{ 1. Running (missing output)}\BuiltInTok{:}\NormalTok{ ../../data/top-10 }\OperatorTok{<}\NormalTok{- ../../data/top.html}
\ExtensionTok{---}\NormalTok{ 1: ../../data/top-10 }\OperatorTok{<}\NormalTok{- ../../data/top.html -}\OperatorTok{>}\NormalTok{ done in 0.02s}
\ExtensionTok{Done}\NormalTok{ (1 steps run)}\ExtensionTok{.}
\end{Highlighting}
\end{Shaded}

As you can see, Drake now only needs to execute the second step, because the output of the first step has already been satisfied. Again, downloading an HTML file is not such a big deal, but can you imagine the implications if you were dealing with 10 GB worth of data?

\hypertarget{rebuilding-certain-targets}{%
\section{Rebuilding Certain Targets}\label{rebuilding-certain-targets}}

The list of the top 100 e-books on project Gutenberg changes daily. We've seen that if we run the Drake workflow again then the HTML containing this list is not being downloaded again. Luckily, Drake allows us to run certain steps again, so that we can update this HTML file:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{drake}\NormalTok{ -w 02.drake }\StringTok{'=top.html'}
\end{Highlighting}
\end{Shaded}

There is a more convenient way than using the output filename to specify which step you want to execute again. We can add so-called \emph{tags} to both the input and output of steps. A tag starts with a ``\%''. It is a good idea to choose a short and descriptive tag name so that you can easily specify this at the command line. Let's add the tag \emph{\%html} to the first step and \emph{\%filter} to the second step:

\begin{verbatim}
NUM:=5
BASE=data/

top.html, %html <- [-timecheck]
    curl -s 'http://www.gutenberg.org/browse/scores/top' > $OUTPUT

top-$[NUM], %filter <- top.html
    < $INPUT grep -E '^<li>' |
    head -n $[NUM] |
    sed -E "s/.*ebooks\/([0-9]+)\">([^<]+)<.*/\\1,\\2/" > $OUTPUT
\end{verbatim}

We can now rebuild the first step by specifying the \emph{\%html} tag:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{drake}\NormalTok{ -w 03.drake }\StringTok{'=%html'}
\end{Highlighting}
\end{Shaded}

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

One of the beauties of the command line is that allows you to play with your data. You can easily execute different commands and process different data files. It is a very interactive and iterative process. After a while, it is easy to forget which steps you have taken to get the desired result. It is therefore very important to document your steps every once in a while. This way, if you or one of your colleagues picks up your project after some time, the same result can be produced again by executing the same steps.

We have shown you that just putting every command in one bash script is suboptimal. We have proposed to use Drake as a command-line tool to manage your data workflow. By using a running example, we have shown you how to define steps and the dependencies between them. We've also discussed how to use variables and tags.

There's nothing more fun than just playing with your data and forget everything else. But you have to trust us when we say that it's worthwhile to keep a record of what you have done by means of a Drake workflow. Not only will it make your life easier, but you will also start thinking about your data workflow in terms of steps. Just as with your command-line toolbox, which you expand over time. It makes you more efficient over time, the same holds for Drake workflows. The more steps you have defined, The easier it gets to keep doing it, because very often you can reuse certain steps. We hope that you will get used to Drake, and that it will make your life easier.

We've only been able to scratch the surface with Drake. Some of its more advanced features are:

\begin{itemize}
\tightlist
\item
  Asynchronous execution of steps.
\item
  Support for inline Python and R code.
\item
  Upload and download data from HDFS S3.
\end{itemize}

\hypertarget{further-reading}{%
\section{Further Reading}\label{further-reading}}

\begin{itemize}
\tightlist
\item
  Factual. 2014. ``Drake.'' https://github.com/Factual/drake.
\end{itemize}

\hypertarget{chapter-7-exploring-data}{%
\chapter{Exploring Data}\label{chapter-7-exploring-data}}

Now that we have obtained and scrubbed our data, we can continue with the third step of the OSEMN model, which is to explore it. After all that hard work, (unless you already had clean data lying around!), it is time for some fun.

Exploring is the step where you familiarize yourself with the data. Being familiar with the data is essential when you want to extract any value from it. For example, knowing what kind of features the data has, means you know which ones are worth further exploring and which ones you can use to answer any questions that you have.

Exploring your data can be done from three perspectives. The first perspective is to inspect the data and its properties. Here, we want to know, for example, what the raw data looks like, how many data points the data set has, and what kind of features the data set has.

The second perspective from which we can explore out data is to compute descriptive statistics. This perspective is useful for learning more about the individual features. One advantage of this perspective is that the output is often brief and textual and can therefore be printed on the command line.

The third perspective is to create visualizations of the data. From this perspective we can gain insight into how multiple features interact. We'll discuss a way of creating visualizations that can be printed on the command line. However, visualizations are best suited to be displayed on a graphical user interfaces. An advantage of visualizations over descriptive statistics is that they are more flexible and that they can convey much more information.

\hypertarget{overview}{%
\section{Overview}\label{overview}}

In this chapter, you'll learn how to:

\begin{itemize}
\tightlist
\item
  Inspect the data and its properties.
\item
  Compute descriptive statistics.
\item
  Create data visualizations inside and outside the command line.
\end{itemize}

\hypertarget{inspecting-data-and-its-properties}{%
\section{Inspecting Data and its Properties}\label{inspecting-data-and-its-properties}}

In this section we'll demonstrate how to inspect your data set and its properties. Because the upcoming visualization and modeling techniques expect the data to be in tabular format, we'll assume that the data is in CSV format. You can use the techniques described in \protect\hyperlink{chapter-5-scrubbing-data}{Chapter 5} to convert your data to CSV if necessary.

For simplicity sake, we'll also assume that your data has a header. In the first subsection we are going to determine whether that is the case. Once we know we have a header, we can continue answering the following questions:

\begin{itemize}
\tightlist
\item
  How many data points and features does the data set have?
\item
  What does the raw data look like?
\item
  What kind of features does the data set have?
\item
  Can some of these features be treated as categorical or as factors?
\end{itemize}

\hypertarget{header-or-not-here-i-come}{%
\subsection{Header Or Not, Here I Come}\label{header-or-not-here-i-come}}

You can check whether your file has a header by printing the first few lines:

\begin{verbatim}
$ #? [echo]
$ head file.csv | csvlook
\end{verbatim}

It is then up to you to decide whether the first line is indeed a header or already the first data point. When the data set contains no header or when its header contains newlines, you're best off going back to \protect\hyperlink{chapter-5-scrubbing-data}{Chapter 5} and correct that.

\hypertarget{inspect-all-the-data}{%
\subsection{Inspect All The Data}\label{inspect-all-the-data}}

If you want to inspect the raw data, then it's best not to use the \texttt{cat} command-line tool, since \texttt{cat} prints all the data to the screen in one go. In order to inspect the raw data at your own pace, we recommend to use \texttt{less} \citep{less} with the \texttt{-S} command-line argument:

\begin{verbatim}
$ #? [echo]
$ less -S file.csv
\end{verbatim}

The \texttt{-S} command-line argument ensures that long lines are not being wrapped when they don't fit in the terminal. Instead, \texttt{less} allows you to scroll horizontally to see the rest of the lines. The advantage of \texttt{less} is that it does not load the entire file into memory, which is good for viewing large files. Once you're in \texttt{less}, you can scroll down a full screen by pressing \texttt{\textless{}Space\textgreater{}}. Scrolling horizontally is done by pressing \texttt{\textless{}Left\textgreater{}} and \texttt{\textless{}Right\textgreater{}}. Press \texttt{g} and \texttt{G} to go to start and the end of the file, respectively. Quiting \texttt{less} is done by pressing \texttt{q}. Read the man page for more key bindings.

If you want the data set to be nicely formatted, you can add in \texttt{csvlook}:

\begin{verbatim}
$ #? [echo]
$ < file.csv csvlook | less -S
\end{verbatim}

Unfortunately, \texttt{csvlook} needs to read the entire file into memory in order to determine the width of the columns. So, when you want to inspect a very large file, then either you may want to get a subset (using \texttt{sample}, for example) or you may need to be patient.

\hypertarget{feature-names-and-data-types}{%
\subsection{Feature Names and Data Types}\label{feature-names-and-data-types}}

In order to gain insight into the data set, it is useful to print the feature names and study them. After all, the feature names may indicate the meaning of the feature. You can use the following \texttt{sed} expression for this:

\begin{verbatim}
$ < data/iris.csv sed -e 's/,/\n/g;q'
\end{verbatim}

Note that this basic command assumes that the file is delimited by commas. Just as reminder: if you intend to use this command often, you could define a function in your \emph{.bashrc} file called, say, \emph{names}:

\begin{example}[]
\protect\hypertarget{exm:unnamed-chunk-1}{}{\label{exm:unnamed-chunk-1} \iffalse () \fi{} }
\end{example}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names ()} \KeywordTok{\{} \FunctionTok{sed}\NormalTok{ -e }\StringTok{'s/,/\textbackslash{}n/g;q'}\KeywordTok{;} \KeywordTok{\}}
\end{Highlighting}
\end{Shaded}

Which you can then you use like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{data/investments.csv}\NormalTok{ names}
\ExtensionTok{company_permalink}
\ExtensionTok{company_name}
\ExtensionTok{company_category_list}
\ExtensionTok{company_market}
\ExtensionTok{company_country_code}
\ExtensionTok{company_state_code}
\ExtensionTok{company_region}
\ExtensionTok{company_city}
\ExtensionTok{investor_permalink}
\ExtensionTok{investor_name}
\ExtensionTok{investor_category_list}
\ExtensionTok{investor_market}
\ExtensionTok{investor_country_code}
\ExtensionTok{investor_state_code}
\ExtensionTok{investor_region}
\ExtensionTok{investor_city}
\ExtensionTok{funding_round_permalink}
\ExtensionTok{funding_round_type}
\ExtensionTok{funding_round_code}
\ExtensionTok{funded_at}
\ExtensionTok{funded_month}
\ExtensionTok{funded_quarter}
\ExtensionTok{funded_year}
\ExtensionTok{raised_amount_usd}
\end{Highlighting}
\end{Shaded}

We can go a step further than just printing the column names. Besides the names of the columns, it would be very useful to know what type of values each column contains. Examples of data types are a string of characters, a numerical value, or a date. Assume that we have the following toy data set:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{data/datatypes.csv}\NormalTok{ csvlook}
\KeywordTok{|}\ExtensionTok{-----+--------+-------+----------+------------------+------------+----------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{a}  \KeywordTok{|} \ExtensionTok{b}      \KeywordTok{|} \ExtensionTok{c}     \KeywordTok{|} \ExtensionTok{d}        \KeywordTok{|} \ExtensionTok{e}                \KeywordTok{|} \ExtensionTok{f}          \KeywordTok{|} \ExtensionTok{g}        \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{-----+--------+-------+----------+------------------+------------+----------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{2}  \KeywordTok{|} \ExtensionTok{0.0}    \KeywordTok{|} \ExtensionTok{FALSE} \KeywordTok{|} \StringTok{"Yes!"}   \KeywordTok{|} \ExtensionTok{2011-11-11}\NormalTok{ 11:00 }\KeywordTok{|} \ExtensionTok{2012-09-08} \KeywordTok{|} \ExtensionTok{12}\NormalTok{:34    }\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{42} \KeywordTok{|} \ExtensionTok{3.1415} \KeywordTok{|} \ExtensionTok{TRUE}  \KeywordTok{|} \ExtensionTok{Oh}\NormalTok{, good }\KeywordTok{|} \ExtensionTok{2014-09-15}       \KeywordTok{|} \ExtensionTok{12/6/70}    \KeywordTok{|} \ExtensionTok{0}\NormalTok{:07 PM  }\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{66} \KeywordTok{|}        \KeywordTok{|} \ExtensionTok{False} \KeywordTok{|} \ExtensionTok{2198}     \KeywordTok{|}                  \KeywordTok{|}            \KeywordTok{|}          \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{-----+--------+-------+----------+------------------+------------+----------}\KeywordTok{|}
\end{Highlighting}
\end{Shaded}

We've already used \texttt{csvsql} in \protect\hyperlink{chapter-5-scrubbing-data}{Chapter 5} to execute SQL queries directly on CSV data. When no command-line arguments are passed, it generates the necessary SQL statement that would be needed if we were to insert this data into an actual database. We can use the output also for ourselves to inspect what the inferred column types are:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{csvsql}\NormalTok{ data/datatypes.csv}
\ExtensionTok{CREATE}\NormalTok{ TABLE datatypes (}
        \ExtensionTok{a}\NormalTok{ INTEGER NOT NULL,}
        \ExtensionTok{b}\NormalTok{ FLOAT,}
        \ExtensionTok{c}\NormalTok{ BOOLEAN NOT NULL,}
        \ExtensionTok{d}\NormalTok{ VARCHAR(8) }\ExtensionTok{NOT}\NormalTok{ NULL,}
        \ExtensionTok{e}\NormalTok{ DATETIME,}
        \ExtensionTok{f}\NormalTok{ DATE,}
        \ExtensionTok{g}\NormalTok{ TIME,}
        \ExtensionTok{CHECK}\NormalTok{ (c IN (0, 1))}
\NormalTok{);}
\end{Highlighting}
\end{Shaded}

provides on overview of what the various SQL data types mean. If a column has the \emph{NOT NULL} string printed after the data type, then that column contains no missing values.

\begin{longtable}[]{@{}lll@{}}
\caption{Python versus SQL data types}\tabularnewline
\toprule
\begin{minipage}[b]{0.30\columnwidth}\raggedright
Type\strut
\end{minipage} & \begin{minipage}[b]{0.30\columnwidth}\raggedright
Python\strut
\end{minipage} & \begin{minipage}[b]{0.30\columnwidth}\raggedright
SQL\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.30\columnwidth}\raggedright
Type\strut
\end{minipage} & \begin{minipage}[b]{0.30\columnwidth}\raggedright
Python\strut
\end{minipage} & \begin{minipage}[b]{0.30\columnwidth}\raggedright
SQL\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.30\columnwidth}\raggedright
Character string\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
unicode\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
VARCHAR\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright
Boolean\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
bool\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
BOOLEAN\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright
Integer\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
int\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
INTEGER\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright
Real number\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
float\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
FLOAT\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright
Date\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
datetime.date\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
DATE\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright
Time\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
datetime.time\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
TIME\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright
Date and time\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
datetime.datetime\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
DATETIME\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{unique-identifiers-continuous-variables-and-factors}{%
\subsection{Unique Identifiers, Continuous Variables, and Factors}\label{unique-identifiers-continuous-variables-and-factors}}

Knowing the data type of each feature is not enough. It is also essential to know what each feature represents. Having knowledge about the domain is very useful here, however we may also get some ideas from the data itself.

Both a string and an integer could be a unique identifier or could represent a category. In the latter case, this could be used to assign a color to your visualization. If an integer denotes, say, the ZIP Code, then it doesn't make sense to compute the average.

To determine whether a feature should be treated as a unique identifier or categorical variable (or factor in R terms), you could count the number of unique values for a specific column:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{cat}\NormalTok{ data/iris.csv }\KeywordTok{|} \ExtensionTok{csvcut}\NormalTok{ -c species }\KeywordTok{|} \ExtensionTok{body} \StringTok{"sort | uniq | wc -l"}
\ExtensionTok{species}
\ExtensionTok{3}
\end{Highlighting}
\end{Shaded}

Or we can use \texttt{csvstat} \citep{csvstat}, which is part of \texttt{csvkit}, to get the number of unique values for each column:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{csvstat}\NormalTok{ data/investments2.csv --unique}
  \ExtensionTok{1.}\NormalTok{ company_permalink: 27342}
  \ExtensionTok{2.}\NormalTok{ company_name: 27324}
  \ExtensionTok{3.}\NormalTok{ company_category_list: 8759}
  \ExtensionTok{4.}\NormalTok{ company_market: 443}
  \ExtensionTok{5.}\NormalTok{ company_country_code: 150}
  \ExtensionTok{6.}\NormalTok{ company_state_code: 147}
  \ExtensionTok{7.}\NormalTok{ company_region: 1079}
  \ExtensionTok{8.}\NormalTok{ company_city: 3305}
  \ExtensionTok{9.}\NormalTok{ investor_permalink: 11176}
 \ExtensionTok{10.}\NormalTok{ investor_name: 11135}
 \ExtensionTok{11.}\NormalTok{ investor_category_list: 468}
 \ExtensionTok{12.}\NormalTok{ investor_market: 134}
 \ExtensionTok{13.}\NormalTok{ investor_country_code: 111}
 \ExtensionTok{14.}\NormalTok{ investor_state_code: 80}
 \ExtensionTok{15.}\NormalTok{ investor_region: 549}
 \ExtensionTok{16.}\NormalTok{ investor_city: 1198}
 \ExtensionTok{17.}\NormalTok{ funding_round_permalink: 41790}
 \ExtensionTok{18.}\NormalTok{ funding_round_type: 13}
 \ExtensionTok{19.}\NormalTok{ funding_round_code: 15}
 \ExtensionTok{20.}\NormalTok{ funded_at: 3595}
 \ExtensionTok{21.}\NormalTok{ funded_month: 295}
 \ExtensionTok{22.}\NormalTok{ funded_quarter: 121}
 \ExtensionTok{23.}\NormalTok{ funded_year: 34}
 \ExtensionTok{24.}\NormalTok{ raised_amount_usd: 6143}
\end{Highlighting}
\end{Shaded}

If the number of unique values is low compared to the number of rows, then that feature may indeed be treated as a categorical one (such as \emph{funding\_round\_type}). If the number is equal to the number of rows, it may be a unique identifier (such as \emph{company\_permalink}).

\hypertarget{computing-descriptive-statistics}{%
\section{Computing Descriptive Statistics}\label{computing-descriptive-statistics}}

\hypertarget{csvstat}{%
\subsection{csvstat}\label{csvstat}}

The command-line tool \texttt{csvstat} gives a lot of information. For each feature (column), it shows:

\begin{itemize}
\tightlist
\item
  The data type in Python terminology (see Table 7-1 for a comparison between Python and SQL data types).
\item
  Whether it has any missing values (nulls).
\item
  The number of unique values.
\item
  Various descriptive statistics (maximum, minimum, sum, mean, standard deviation, and median) for those features for which it is appropriate.
\end{itemize}

We invoke \texttt{csvstat} as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{csvstat}\NormalTok{ data/datatypes.csv}
  \ExtensionTok{1.}\NormalTok{ a}
        \OperatorTok{<}\BuiltInTok{type} \StringTok{'int'}\OperatorTok{>}
        \ExtensionTok{Nulls}\NormalTok{: False}
        \ExtensionTok{Values}\NormalTok{: 2, 66, 42}
  \ExtensionTok{2.}\NormalTok{ b}
        \OperatorTok{<}\BuiltInTok{type} \StringTok{'float'}\OperatorTok{>}
        \ExtensionTok{Nulls}\NormalTok{: True}
        \ExtensionTok{Values}\NormalTok{: 0.0, 3.1415}
  \ExtensionTok{3.}\NormalTok{ c}
        \OperatorTok{<}\BuiltInTok{type} \StringTok{'bool'}\OperatorTok{>}
        \ExtensionTok{Nulls}\NormalTok{: False}
        \ExtensionTok{Unique}\NormalTok{ values: 2}
        \ExtensionTok{5}\NormalTok{ most frequent values:}
                \ExtensionTok{False}\NormalTok{:  2}
                \ExtensionTok{True}\NormalTok{:   1}
  \ExtensionTok{4.}\NormalTok{ d}
        \OperatorTok{<}\BuiltInTok{type} \StringTok{'unicode'}\OperatorTok{>}
        \ExtensionTok{Nulls}\NormalTok{: False}
        \ExtensionTok{Values}\NormalTok{: 2198, }\StringTok{"Yes!"}\NormalTok{, Oh, good}
  \ExtensionTok{5.}\NormalTok{ e}
        \OperatorTok{<}\BuiltInTok{type} \StringTok{'datetime.datetime'}\OperatorTok{>}
        \ExtensionTok{Nulls}\NormalTok{: True}
        \ExtensionTok{Values}\NormalTok{: 2011-11-11 11:00:00, 2014-09-15 00:00:00}
  \ExtensionTok{6.}\NormalTok{ f}
        \OperatorTok{<}\BuiltInTok{type} \StringTok{'datetime.date'}\OperatorTok{>}
        \ExtensionTok{Nulls}\NormalTok{: True}
        \ExtensionTok{Values}\NormalTok{: 2012-09-08, 1970-12-06}
  \ExtensionTok{7.}\NormalTok{ g}
        \OperatorTok{<}\BuiltInTok{type} \StringTok{'datetime.time'}\OperatorTok{>}
        \ExtensionTok{Nulls}\NormalTok{: True}
        \ExtensionTok{Values}\NormalTok{: 12:34:00, 12:07:00}

\ExtensionTok{Row}\NormalTok{ count: 3}
\end{Highlighting}
\end{Shaded}

This gives a very verbose output. For a more concise output specify one of the statistics arguments:

\begin{itemize}
\item
  \texttt{-\/-max} (maximum)
\item
  \texttt{-\/-min} (minimum)
\item
  \texttt{-\/-sum} (sum)
\item
  \texttt{-\/-mean} (mean)
\item
  \texttt{-\/-median} (median)
\item
  \texttt{-\/-stdev} (standard deviation)
\item
  \texttt{-\/-nulls} (whether column contains nulls)
\item
  \texttt{-\/-unique} (unique values)
\item
  \texttt{-\/-freq} (frequent values)
\item
  \texttt{-\/-len} (max value length)
\end{itemize}

For example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{csvstat}\NormalTok{ data/datatypes.csv --null}
  \ExtensionTok{1.}\NormalTok{ a: False}
  \ExtensionTok{2.}\NormalTok{ b: True}
  \ExtensionTok{3.}\NormalTok{ c: False}
  \ExtensionTok{4.}\NormalTok{ d: False}
  \ExtensionTok{5.}\NormalTok{ e: True}
  \ExtensionTok{6.}\NormalTok{ f: True}
  \ExtensionTok{7.}\NormalTok{ g: True}
\end{Highlighting}
\end{Shaded}

You can select a subset of features with the \texttt{-c} command-line argument. This accepts both integers and column names:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{csvstat}\NormalTok{ data/investments2.csv -c 2,13,19,24}
  \ExtensionTok{2.}\NormalTok{ company_name}
        \OperatorTok{<}\BuiltInTok{type} \StringTok{'unicode'}\OperatorTok{>}
        \ExtensionTok{Nulls}\NormalTok{: True}
        \ExtensionTok{Unique}\NormalTok{ values: 27324}
        \ExtensionTok{5}\NormalTok{ most frequent values:}
                \ExtensionTok{Aviir}\NormalTok{:  13}
                \ExtensionTok{Galectin}\NormalTok{ Therapeutics:  12}
                \ExtensionTok{Rostima}\NormalTok{:        12}
                \ExtensionTok{Facebook}\NormalTok{:       11}
                \ExtensionTok{Lending}\NormalTok{ Club:   11}
        \ExtensionTok{Max}\NormalTok{ length: 66}
 \ExtensionTok{13.}\NormalTok{ investor_country_code}
        \OperatorTok{<}\BuiltInTok{type} \StringTok{'unicode'}\OperatorTok{>}
        \ExtensionTok{Nulls}\NormalTok{: True}
        \ExtensionTok{Unique}\NormalTok{ values: 111}
        \ExtensionTok{5}\NormalTok{ most frequent values:}
                \ExtensionTok{USA}\NormalTok{:    20806}
                \ExtensionTok{GBR}\NormalTok{:    2357}
                \ExtensionTok{DEU}\NormalTok{:    946}
                \ExtensionTok{CAN}\NormalTok{:    893}
                \ExtensionTok{FRA}\NormalTok{:    737}
        \ExtensionTok{Max}\NormalTok{ length: 15}
 \ExtensionTok{19.}\NormalTok{ funding_round_code}
        \OperatorTok{<}\BuiltInTok{type} \StringTok{'unicode'}\OperatorTok{>}
        \ExtensionTok{Nulls}\NormalTok{: True}
        \ExtensionTok{Unique}\NormalTok{ values: 15}
        \ExtensionTok{5}\NormalTok{ most frequent values:}
                \ExtensionTok{a}\NormalTok{:      7529}
                \ExtensionTok{b}\NormalTok{:      4776}
                \ExtensionTok{c}\NormalTok{:      2452}
                \ExtensionTok{d}\NormalTok{:      1042}
                \ExtensionTok{e}\NormalTok{:      384}
        \ExtensionTok{Max}\NormalTok{ length: 10}
 \ExtensionTok{24.}\NormalTok{ raised_amount_usd}
        \OperatorTok{<}\BuiltInTok{type} \StringTok{'int'}\OperatorTok{>}
        \ExtensionTok{Nulls}\NormalTok{: True}
        \ExtensionTok{Min}\NormalTok{: 0}
        \ExtensionTok{Max}\NormalTok{: 3200000000}
        \ExtensionTok{Sum}\NormalTok{: 359891203117}
        \ExtensionTok{Mean}\NormalTok{: 10370010.1748}
        \ExtensionTok{Median}\NormalTok{: 3250000}
        \ExtensionTok{Standard}\NormalTok{ Deviation: 38513119.1802}
        \ExtensionTok{Unique}\NormalTok{ values: 6143}
        \ExtensionTok{5}\NormalTok{ most frequent values:}
                \ExtensionTok{10000000}\NormalTok{:       1159}
                \ExtensionTok{1000000}\NormalTok{:        1074}
                \ExtensionTok{5000000}\NormalTok{:        1066}
                \ExtensionTok{2000000}\NormalTok{:        875}
                \ExtensionTok{3000000}\NormalTok{:        820}

\ExtensionTok{Row}\NormalTok{ count: 41799}
\end{Highlighting}
\end{Shaded}

Please note that \texttt{csvstat}, just like \texttt{csvsql}, employs heuristics to determine the data type, and therefore may not always get it right. We encourage you to always do a manual inspection as discussed in the previous subsection. Moreover, the type may be a character string or integer that doesn't say anything about how it should be used.

As a nice extra, \texttt{csvstat} outputs, at the very end, the number of data points (rows). Newlines and commas inside values are handles correctly. To only see the relevant line, we can use \texttt{tail}:

\begin{verbatim}
$ csvstat data/iris.csv | tail -n 1
\end{verbatim}

If you only want to see the actual number number of data points, you can use, for example, the following \texttt{sed} expression to extract the number:

\begin{verbatim}
$ csvstat data/iris.csv | sed -rne '${s/^([^:]+): ([0-9]+)$/\2/;p}'
\end{verbatim}

\hypertarget{using-r-from-the-command-line-using-rio}{%
\subsection{Using R from the Command Line using Rio}\label{using-r-from-the-command-line-using-rio}}

In this section we would like to introduce you to a command-line tool called \texttt{Rio}, which is essentially a small, nifty wrapper around the statistical programming environment R. Before we explain what Rio does and why it exists, lets talk a bit about R itself.

R is a very powerful statistical software package to analyze data and create visualizations. It's an interpreted programming language, has an extensive collection of packages, and offers its own REPL (Read-Eval-Print-Loop), which allows you, similar to the command line, to play with your data. Unfortunately, R is quite separated from the command line. Once you start it, you're in a separate environment. R doesn't really play well with the command line because you cannot pipe any data into it and it also doesn't support any one-liners that you can specify.

For example, imagine that you have a CSV file called \emph{tips.csv}, and you would like compute the tip percentage, and save the result. To accomplish this in R you would first startup R:

\begin{verbatim}
$ #? [echo]
$ R
\end{verbatim}

And then run the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{>}\StringTok{ }\NormalTok{tips <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{'tips.csv'}\NormalTok{, }\DataTypeTok{header =}\NormalTok{ T, }\DataTypeTok{sep =} \StringTok{','}\NormalTok{, }\DataTypeTok{stringsAsFactors =}\NormalTok{ F)}
\OperatorTok{>}\StringTok{ }\NormalTok{tips.percent <-}\StringTok{ }\NormalTok{tips}\OperatorTok{$}\NormalTok{tip }\OperatorTok{/}\StringTok{ }\NormalTok{tips}\OperatorTok{$}\NormalTok{bill }\OperatorTok{*}\StringTok{ }\DecValTok{100}
\OperatorTok{>}\StringTok{ }\KeywordTok{cat}\NormalTok{(tips.percent, }\DataTypeTok{sep =} \StringTok{'}\CharTok{\textbackslash{}n}\StringTok{'}\NormalTok{, }\DataTypeTok{file =} \StringTok{'percent.csv'}\NormalTok{)}
\OperatorTok{>}\StringTok{ }\KeywordTok{q}\NormalTok{(}\StringTok{"no"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Afterwards, you can continue with the saved file \emph{percent.csv} on the command line. Note that there is only one command that is associated with what we want to accomplish specifically. The other commands are necessary boilerplate. Typing in this boilerplate in order to accomplish something simple is cumbersome and breaks your workflow. Sometimes, you only want to do one or two things at a time to your data. Wouldn't it be great if we could harness the power of R and be able to use it from the command line?

This is where \texttt{Rio} comes in. The name Rio stands for \emph{R input/output}, because it enables you to use R as a filter on the command line. You simply pipe CSV data into \texttt{Rio} and you specify the R commands that you want to run on it. Let's perform the same task as before, but now using \texttt{Rio}:

\begin{verbatim}
$ < data/tips.csv Rio -e 'df$tip / df$bill * 100' | head -n 10
\end{verbatim}

\texttt{Rio} can execute multiple R command that are separated by semicolons. So, if you wanted to add a column called \emph{percent} to the input data, you could do the following:

\begin{verbatim}
$ < data/tips.csv Rio -e 'df$percent <- df$tip / df$bill * 100; df' | head
\end{verbatim}

These small one-liners are possible because \texttt{Rio} takes care of all the boilerplate. Being able to use the command line for this and capture the power of R into a one-liner is fantastic, especially if you want to keep on working on the command line. \texttt{Rio} assumes that the input data is in CSV format with a header. (By specifying the \texttt{-n} command-line argument \texttt{Rio} does not consider the first row to be the header and creates default column names.) Behind the scenes, \texttt{Rio} writes the piped data to a temporary CSV file and creates a script that:

\begin{itemize}
\item
  Import required libraries.
\item
  Loads the CSV file as a data frame.
\item
  Generates a \texttt{ggplot2} object if needed (more on this in the next section).
\item
  Runs the specified commands.
\item
  Prints the result of the last command to standard output.
\end{itemize}

So now, if you wanted to do one or two things to your data set with R, you can specify it as a one-liner, and keep on working on the command line. All the knowledge that you already have about R can now be used from the command line. With \texttt{Rio}, you can even create sophisticated visualizations, as you will see later in this chapter.

Rio doesn't have to be used as a filter, meaning the output doesn't have to be a in CSV format per se. You can compute

\begin{verbatim}
$ < data/iris.csv Rio -e 'mean(df$sepal_length)'
$ < data/iris.csv Rio -e 'sd(df$sepal_length)'
$ < data/iris.csv Rio -e 'sum(df$sepal_length)'
\end{verbatim}

If we wanted to compute the five summary statistics, we would do:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{iris.csv}\NormalTok{ Rio -e }\StringTok{'summary(df$sepal_length)'}
   \ExtensionTok{Min.}\NormalTok{ 1st Qu.  Median    Mean 3rd Qu.    Max.}
  \ExtensionTok{4.300}\NormalTok{   5.100   5.800   5.843   6.400   7.900}
\end{Highlighting}
\end{Shaded}

You can also compute the skewness (symmetry of the distribution) and kurtosis (peakedness of the distribution), but then you need to have the \texttt{moments} package installed:

\begin{verbatim}
$ #? [echo]
$ < data/iris.csv Rio -e 'skewness(df$sepal_length)'
$ < data/iris.csv Rio -e 'kurtosis(df$petal_width)'
\end{verbatim}

Correlation between two features:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{tips.csv}\NormalTok{ Rio -e }\StringTok{'cor(df$bill, df$tip)'}
\ExtensionTok{0.6757341}
\end{Highlighting}
\end{Shaded}

Or a correlation matrix:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{data/tips.csv}\NormalTok{ csvcut -c bill,tip }\KeywordTok{|} \ExtensionTok{Rio}\NormalTok{ -f cor }\KeywordTok{|} \ExtensionTok{csvlook}
\KeywordTok{|}\ExtensionTok{--------------------+--------------------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{bill}              \KeywordTok{|} \ExtensionTok{tip}                \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{--------------------+--------------------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{1}                 \KeywordTok{|} \ExtensionTok{0.675734109211365}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{0.675734109211365} \KeywordTok{|} \ExtensionTok{1}                  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{--------------------+--------------------}\KeywordTok{|}
\end{Highlighting}
\end{Shaded}

Note that with the command-line argument \texttt{-f}, we can specify the function to apply to the data frame \texttt{df}. In this case, it is the same as \texttt{-e\ cor(df)}.

You can even create a stem plot \citep{Tukey1977} using \texttt{Rio}:

\begin{verbatim}
$ < data/iris.csv Rio -e 'stem(df$sepal_length)'
\end{verbatim}

\hypertarget{creating-visualizations}{%
\section{Creating Visualizations}\label{creating-visualizations}}

In this section we're going to discuss how to create visualizations at the command line. We'll be looking at two different software packages: gnuplot and ggplot. First, we'll introduce both packages. Then, we'll demonstrate how to create several different types of visualizations using both of them.

\hypertarget{introducing-gnuplot-and-feedgnuplot}{%
\subsection{Introducing Gnuplot and Feedgnuplot}\label{introducing-gnuplot-and-feedgnuplot}}

The first software package to create visualizations that we're discussing in this chapter is Gnuplot. Gnuplot has been around since 1986. Despite being rather old, its visualization capabilities are quite extensive. As such, it's impossible to do it justice. There are other good resources available, including \emph{Gnuplot in Action} by \citet{Janert2009}.

To demonstrate the flexibility (and its archaic notation), consider Example \ref{exm:script-gnuplot}, which is copied from the Gnuplot website (\url{http://gnuplot.sourceforge.net/demo/histograms.6.gnu}).

\begin{example}[Creating a histogram using Gnuplot]
\protect\hypertarget{exm:script-gnuplot}{}{\label{exm:script-gnuplot} \iffalse (Creating a histogram using Gnuplot) \fi{} }
\end{example}

\begin{verbatim}
# set terminal pngcairo  transparent enhanced font "arial,10" fontscale 1.0 size
# set output 'histograms.6.png'
set border 3 front linetype -1 linewidth 1.000
set boxwidth 0.75 absolute
set style fill   solid 1.00 border lt -1
set grid nopolar
set grid noxtics nomxtics ytics nomytics noztics nomztics \
 nox2tics nomx2tics noy2tics nomy2tics nocbtics nomcbtics
set grid layerdefault   linetype 0 linewidth 1.000,  linetype 0 linewidth 1.000
set key outside right top vertical Left reverse noenhanced autotitles columnhead
set style histogram columnstacked title  offset character 0, 0, 0
set datafile missing '-'
set style data histograms
set xtics border in scale 1,0.5 nomirror norotate  offset character 0, 0, 0 auto
set xtics  norangelimit
set xtics   ()
set ytics border in scale 0,0 mirror norotate  offset character 0, 0, 0 autojust
set ztics border in scale 0,0 nomirror norotate  offset character 0, 0, 0 autoju
set cbtics border in scale 0,0 mirror norotate  offset character 0, 0, 0 autojus
set rtics axis in scale 0,0 nomirror norotate  offset character 0, 0, 0 autojust
set title "Immigration from Northern Europe\n(columstacked histogram)"
set xlabel "Country of Origin"
set ylabel "Immigration by decade"
set yrange [ 0.00000 : * ] noreverse nowriteback
i = 23
plot 'immigration.dat' using 6 ti col, '' using 12 ti col,      '' using 13 ti c
\end{verbatim}

Please note that this is trimmed to 80 characters wide. The above script generates the following image:

\begin{figure}

{\centering \includegraphics[width=6.94in]{images/histograms.6} 

}

\caption{Immigration Plot by Gnuplot}\label{fig:unnamed-chunk-14}
\end{figure}

Gnuplot is different from most command-line tools we've been using for two reasons. First, it uses a script instead of command-line arguments. Second, the output is always written to a file and not printed to standard output.

One great advantage of Gnuplot being around for so long, and the main reason we've included it in this book, is that it's able to produce visualizations \textbf{for} the command line. That is, it's able to print its output to the terminal without the need for a graphical user interface (GUI). Even then, you would need to set up a script.

Luckily, there is a command-line tool called \texttt{feedgnuplot} \citep{feedgnuplot}, which can help us with setting up a script for Gnuplot. \texttt{feedgnuplot} is entirely configurable through command-line arguments. Plus, it reads from standard input. After we have introduced \texttt{ggplot2}, we're going to create a few visualizations using \texttt{feedgnuplot}.

One great feature of \texttt{feedgnuplot} that we would like to mention here, is that it allows you to plot streaming data. The following is a snapshot of a continuously updated plot based on random input data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\KeywordTok{while} \FunctionTok{true}\KeywordTok{;} \KeywordTok{do} \BuiltInTok{echo} \VariableTok{$RANDOM}\KeywordTok{;} \KeywordTok{done} \KeywordTok{|} \ExtensionTok{sample}\NormalTok{ -d 10 }\KeywordTok{|} \ExtensionTok{feedgnuplot}\NormalTok{ --stream \textbackslash{}}
\OperatorTok{>}\NormalTok{ --terminal }\StringTok{'dumb 80,25'}\NormalTok{ --lines --xlen 10}

  \ExtensionTok{30000}\NormalTok{ ++-----+------------+-------------+-------------+------------+-----++}
        \KeywordTok{|}      \ExtensionTok{+}\NormalTok{            *             +             +            +      }\KeywordTok{|}
        \KeywordTok{|}      \BuiltInTok{:}\NormalTok{            **            :             *******      :      *}
  \ExtensionTok{25000}\NormalTok{ ++.................*.*..........................*.....*............+*}
        \KeywordTok{|}      \BuiltInTok{:}\NormalTok{           *: *           :            *:      *     :     *}\KeywordTok{|}
        \KeywordTok{|}      \BuiltInTok{:}\NormalTok{           *: *           :            *:      *     :     *}\KeywordTok{|}
        \KeywordTok{|}      \BuiltInTok{:}\NormalTok{          * :  *          :           * :       *    :    * }\KeywordTok{|}
  \ExtensionTok{20000}\NormalTok{ ++................*....*......................*.........*.........*++}
        \KeywordTok{|}      \BuiltInTok{:}\NormalTok{          * :   *         :          *  :       *    :    * }\KeywordTok{|}
        \KeywordTok{|}      \BuiltInTok{:}\NormalTok{         *  :   *         :          *  :        *   :   *  }\KeywordTok{|}
  \ExtensionTok{15000}\NormalTok{ ++....**.........*.......*..................*............*.......*.++}
        \KeywordTok{|} \ExtensionTok{****}\NormalTok{ :*        *  :    *        :         *   :         *  :   *  }\KeywordTok{|}
        \ExtensionTok{**}\NormalTok{     :*        *  :     *      ****      *    :         *  :  *   }\KeywordTok{|}
  \ExtensionTok{10000}\NormalTok{ ++.......*......*.........*....**....*.....*..............*.....*..++}
        \KeywordTok{|}      \BuiltInTok{:}\NormalTok{  *     *   :      * **   :   *   *     :          * : *    }\KeywordTok{|}
        \KeywordTok{|}      \BuiltInTok{:}\NormalTok{   *    *   :      **     :    ** *     :          * : *    }\KeywordTok{|}
        \KeywordTok{|}      \BuiltInTok{:}\NormalTok{   *   *    :             :      *      :          * : *    }\KeywordTok{|}
   \ExtensionTok{5000}\NormalTok{ ++..........*..*.........................*..................*.*....++}
        \KeywordTok{|}      \BuiltInTok{:}\NormalTok{     * *    :             :             :           *:*     }\KeywordTok{|}
        \KeywordTok{|}      \ExtensionTok{+}\NormalTok{     **     +             +             +            *      }\KeywordTok{|}
      \ExtensionTok{0}\NormalTok{ ++-----+------*-----+-------------+-------------+------------*-----++}
              \ExtensionTok{2350}\NormalTok{         2352          2354          2356         2358}
\end{Highlighting}
\end{Shaded}

\hypertarget{introducing-ggplot2}{%
\subsection{Introducing ggplot2}\label{introducing-ggplot2}}

A more modern software package for creating visualizations is ggplot, which is an implementation of the grammar of graphics in R \citep{Wickham2009}.

Thanks to the grammar of graphics and using sensible defaults, \texttt{ggplot2} commands tend to be very short and expressive. When used through \texttt{Rio}, this is a very convenient way of creating visualizations from the command line.

To demonstrate it's expressiveness, we'll recreate the histogram plot generated above by gnuplot, with the help of \texttt{Rio}. Because \texttt{Rio} expects the data set to be comma-delimited, and because \texttt{ggplot2} expects the data in \emph{long} format, we first need to scrub and transform the data a little bit:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{data/immigration.dat}\NormalTok{ sed -re }\StringTok{'/^#/d;s/\textbackslash{}t/,/g;s/,-,/,0,/g;s/Region/'}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{'Period/'} \KeywordTok{|} \FunctionTok{tee}\NormalTok{ data/immigration.csv }\KeywordTok{|} \FunctionTok{head} \KeywordTok{|} \FunctionTok{cut}\NormalTok{ -c1-80}
\ExtensionTok{Period}\NormalTok{,Austria,Hungary,Belgium,Czechoslovakia,Denmark,France,Germany,Greece,Irel}
\ExtensionTok{1891-1900}\NormalTok{,234081,181288,18167,0,50231,30770,505152,15979,388416,651893,26758,950}
\ExtensionTok{1901-1910}\NormalTok{,668209,808511,41635,0,65285,73379,341498,167519,339065,2045877,48262,1}
\ExtensionTok{1911-1920}\NormalTok{,453649,442693,33746,3426,41983,61897,143945,184201,146181,1109524,4371}
\ExtensionTok{1921-1930}\NormalTok{,32868,30680,15846,102194,32430,49610,412202,51084,211234,455315,26948,}
\ExtensionTok{1931-1940}\NormalTok{,3563,7861,4817,14393,2559,12623,144058,9119,10973,68028,7150,4740,3960}
\ExtensionTok{1941-1950}\NormalTok{,24860,3469,12189,8347,5393,38809,226578,8973,19789,57661,14860,10100,1}
\ExtensionTok{1951-1960}\NormalTok{,67106,36637,18575,918,10984,51121,477765,47608,43362,185491,52277,2293}
\ExtensionTok{1961-1970}\NormalTok{,20621,5401,9192,3273,9201,45237,190796,85969,32966,214111,30606,15484,}
\end{Highlighting}
\end{Shaded}

The \texttt{sed} expression consists of four parts, delimited by semicolons:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Remove lines that start with \emph{\#}.
\item
  Convert tabs to commas.
\item
  Change dashes (missing values) into zero's.
\item
  Change the feature name \emph{Region} into \emph{Period}.
\end{enumerate}

We then select only the columns that matter using \texttt{csvcut} and subsequently convert the data from a wide format to a long one using the \texttt{Rio} and the \texttt{melt} function which part of the R package \texttt{reshape2}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{data/immigration.csv}\NormalTok{ csvcut -c Period,Denmark,Netherlands,Norway,\textbackslash{}}
\OperatorTok{>}\NormalTok{ Sweden }\KeywordTok{|} \ExtensionTok{Rio}\NormalTok{ -re }\StringTok{'melt(df, id="Period", variable.name="Country", '}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{'value.name="Count")'} \KeywordTok{|} \FunctionTok{tee}\NormalTok{ data/immigration-long.csv }\KeywordTok{|} \FunctionTok{head} \KeywordTok{|} \ExtensionTok{csvlook}
\KeywordTok{|}\ExtensionTok{------------+-------------+--------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{Period}    \KeywordTok{|} \ExtensionTok{Country}     \KeywordTok{|} \ExtensionTok{Count}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{------------+-------------+--------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{1891-1900} \KeywordTok{|} \ExtensionTok{Denmark}     \KeywordTok{|} \ExtensionTok{50231}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{1901-1910} \KeywordTok{|} \ExtensionTok{Denmark}     \KeywordTok{|} \ExtensionTok{65285}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{1911-1920} \KeywordTok{|} \ExtensionTok{Denmark}     \KeywordTok{|} \ExtensionTok{41983}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{1921-1930} \KeywordTok{|} \ExtensionTok{Denmark}     \KeywordTok{|} \ExtensionTok{32430}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{1931-1940} \KeywordTok{|} \ExtensionTok{Denmark}     \KeywordTok{|} \ExtensionTok{2559}   \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{1941-1950} \KeywordTok{|} \ExtensionTok{Denmark}     \KeywordTok{|} \ExtensionTok{5393}   \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{1951-1960} \KeywordTok{|} \ExtensionTok{Denmark}     \KeywordTok{|} \ExtensionTok{10984}  \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{1961-1970} \KeywordTok{|} \ExtensionTok{Denmark}     \KeywordTok{|} \ExtensionTok{9201}   \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{1891-1900} \KeywordTok{|} \ExtensionTok{Netherlands} \KeywordTok{|} \ExtensionTok{26758}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{------------+-------------+--------}\KeywordTok{|}
\end{Highlighting}
\end{Shaded}

Now, we can use \texttt{Rio} again, but then with an expression that builds up a \texttt{ggplot2} visualization:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{data/immigration-long.csv}\NormalTok{ Rio -ge }\StringTok{'g + geom_bar(aes(Country, Count,'}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{' fill=Period), stat="identity") + scale_fill_brewer(palette="Set1") '}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{'+ labs(x="Country of origin", y="Immigration by decade", title='}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{'"Immigration from Northern Europe\textbackslash{}n(columstacked histogram)")'} \KeywordTok{|} \ExtensionTok{display}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=32.81in]{images/rio-immigration} 

}

\caption{Immigration plot by Rio and ggplot2}\label{fig:unnamed-chunk-19}
\end{figure}

The \texttt{-g} command-line argument indicates that Rio should load the \texttt{ggplot2} package. The output is an image in PNG format. You can either view the PNG image via \texttt{display}, which is part of ImageMagick \citep{display} or you can redirect the output to a PNG file. If you're on a remote terminal then you probably won't be able to see any graphics. A workaround for this is to start a webserver from a particular directory:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{python}\NormalTok{ -m SimpleHTTPServer 8000}
\end{Highlighting}
\end{Shaded}

Make sure that you have access to the port (8000 in this case). If you save the PNG image to the directory from which the webserver was launched, then you can access the image from your browser at \url{http://localhost:8000/file.png}.

\hypertarget{histograms}{%
\subsection{Histograms}\label{histograms}}

Using \texttt{Rio}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{data/tips.csv}\NormalTok{ Rio -ge }\StringTok{'g+geom_histogram(aes(bill))'} \KeywordTok{|} \ExtensionTok{display}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=32.81in]{images/rio-histogram} 

}

\caption{Histogram}\label{fig:unnamed-chunk-22}
\end{figure}

Using \texttt{feedgnuplot}:

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{<} \ExtensionTok{data/tips.csv}\NormalTok{ csvcut -c bill }\KeywordTok{|} \ExtensionTok{feedgnuplot}\NormalTok{ --terminal }\StringTok{'dumb 80,25'}\NormalTok{ \textbackslash{}}
\NormalTok{--histogram 0 --with boxes --ymin 0 --binwidth 1.5 --unset grid --exit}



  \ExtensionTok{25}\NormalTok{ ++----+------+-----+--***-+-----+------+-----+------+-----+------+----++}
     \ExtensionTok{+}\NormalTok{     +      +     +*** * +     +      +     +      +     +      +     +}
     \KeywordTok{|}                   \ExtensionTok{*}\NormalTok{ * *                                              }\KeywordTok{|}
     \KeywordTok{|}               \ExtensionTok{***}\NormalTok{ * * *                                              }\KeywordTok{|}
  \ExtensionTok{20}\NormalTok{ ++              * * * * *                                             ++}
     \KeywordTok{|}            \ExtensionTok{****}\NormalTok{ * * * *                                              }\KeywordTok{|}
     \KeywordTok{|}            \ExtensionTok{*}\NormalTok{ ** *** * * ***                                          }\KeywordTok{|}
     \KeywordTok{|}            \ExtensionTok{*}\NormalTok{ ** * * * * * *                                          }\KeywordTok{|}
  \ExtensionTok{15}\NormalTok{ ++           * ** * * * * * *                                         ++}
     \KeywordTok{|}            \ExtensionTok{*}\NormalTok{ ** * * * * * *                                          }\KeywordTok{|}
     \KeywordTok{|}            \ExtensionTok{*}\NormalTok{ ** * * * * * *                                          }\KeywordTok{|}
     \KeywordTok{|}            \ExtensionTok{*}\NormalTok{ ** * * * * * * ***                                      }\KeywordTok{|}
  \ExtensionTok{10}\NormalTok{ ++           * ** * * * *** *** *                                     ++}
     \KeywordTok{|}            \ExtensionTok{*}\NormalTok{ ** * * * * * * * *                                      }\KeywordTok{|}
     \KeywordTok{|}          \ExtensionTok{***}\NormalTok{ ** * * * * * * * ***** ***                              }\KeywordTok{|}
     \KeywordTok{|}          \ExtensionTok{*}\NormalTok{ * ** * * * * * * * * * *** *                              }\KeywordTok{|}
   \ExtensionTok{5}\NormalTok{ ++       *** * ** * * * * * * * * * * * *   ***                       ++}
     \KeywordTok{|}        \ExtensionTok{*}\NormalTok{ * * ** * * * * * * * * * * * * *** *                        }\KeywordTok{|}
     \KeywordTok{|}        \ExtensionTok{*}\NormalTok{ * * ** * * * * * * * * * * * *** * ********   *** ***       }\KeywordTok{|}
     \ExtensionTok{+}\NormalTok{  ***+*** * * ** *+* * * * * * * * * *+* * *+** * *+* ***+* * * ***   +}
   \ExtensionTok{0}\NormalTok{ ++-***+***********************************************-*****-***-***--++}
     \ExtensionTok{0}\NormalTok{     5      10    15     20    25     30    35     40    45     50    55}
\end{Highlighting}
\end{Shaded}

\hypertarget{bar-plots}{%
\subsection{Bar Plots}\label{bar-plots}}

Using \texttt{Rio}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{data/tips.csv}\NormalTok{ Rio -ge }\StringTok{'g+geom_bar(aes(factor(size)))'} \KeywordTok{|} \ExtensionTok{display}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=32.81in]{images/rio-barplot} 

}

\caption{Bar Plot}\label{fig:unnamed-chunk-25}
\end{figure}

Using \texttt{feedgnuplot}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{data/tips.csv} \KeywordTok{|} \ExtensionTok{csvcut}\NormalTok{ -c size }\KeywordTok{|} \ExtensionTok{header}\NormalTok{ -d }\KeywordTok{|} \ExtensionTok{feedgnuplot}\NormalTok{ --terminal \textbackslash{}}
\OperatorTok{>} \StringTok{'dumb 80,25'}\NormalTok{ --histogram 0  --with boxes --unset grid --exit}



  \ExtensionTok{160}\NormalTok{ ++--------+----***********----+---------+---------+---------+--------++}
      \ExtensionTok{+}\NormalTok{         +    *    +    *    +         +         +         +         +}
  \ExtensionTok{140}\NormalTok{ ++             *         *                                           ++}
      \KeywordTok{|}              \ExtensionTok{*}\NormalTok{         *                                            }\KeywordTok{|}
      \KeywordTok{|}              \ExtensionTok{*}\NormalTok{         *                                            }\KeywordTok{|}
  \ExtensionTok{120}\NormalTok{ ++             *         *                                           ++}
      \KeywordTok{|}              \ExtensionTok{*}\NormalTok{         *                                            }\KeywordTok{|}
  \ExtensionTok{100}\NormalTok{ ++             *         *                                           ++}
      \KeywordTok{|}              \ExtensionTok{*}\NormalTok{         *                                            }\KeywordTok{|}
      \KeywordTok{|}              \ExtensionTok{*}\NormalTok{         *                                            }\KeywordTok{|}
   \ExtensionTok{80}\NormalTok{ ++             *         *                                           ++}
      \KeywordTok{|}              \ExtensionTok{*}\NormalTok{         *                                            }\KeywordTok{|}
   \ExtensionTok{60}\NormalTok{ ++             *         *                                           ++}
      \KeywordTok{|}              \ExtensionTok{*}\NormalTok{         *                                            }\KeywordTok{|}
      \KeywordTok{|}              \ExtensionTok{*}\NormalTok{         *                                            }\KeywordTok{|}
   \ExtensionTok{40}\NormalTok{ ++             *         *********************                       ++}
      \KeywordTok{|}              \ExtensionTok{*}\NormalTok{         *         *         *                        }\KeywordTok{|}
   \ExtensionTok{20}\NormalTok{ ++             *         *         *         *                       ++}
      \KeywordTok{|}              \ExtensionTok{*}\NormalTok{         *         *         *                        }\KeywordTok{|}
      \ExtensionTok{+}\NormalTok{    ***********    +    *    +    *    +    *********************    +}
    \ExtensionTok{0}\NormalTok{ ++---*************************************************************---++}
      \ExtensionTok{0}\NormalTok{         1         2         3         4         5         6         7}
\end{Highlighting}
\end{Shaded}

\hypertarget{density-plots}{%
\subsection{Density Plots}\label{density-plots}}

Using \texttt{Rio}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{data/tips.csv}\NormalTok{ Rio -ge }\StringTok{'g+geom_density(aes(tip / bill * 100, fill=sex), '}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{'alpha=0.3) + xlab("percent")'} \KeywordTok{|} \ExtensionTok{display}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=32.81in]{images/rio-densityplot} 

}

\caption{Density Plot}\label{fig:unnamed-chunk-28}
\end{figure}

Since \texttt{feedgnuplot} cannot generate density plots, it's best to just generate a histogram.

\hypertarget{box-plots}{%
\subsection{Box Plots}\label{box-plots}}

Using \texttt{Rio}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{data/tips.csv}\NormalTok{ Rio -ge }\StringTok{'g+geom_boxplot(aes(time, bill))'} \KeywordTok{|} \ExtensionTok{display}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=32.81in]{images/rio-boxplot} 

}

\caption{Box Plot}\label{fig:unnamed-chunk-30}
\end{figure}

Drawing a box plot is unfortunately not possible with \texttt{feedgnuplot}.

\hypertarget{scatter-plots}{%
\subsection{Scatter Plots}\label{scatter-plots}}

Using \texttt{Rio}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{data/tips.csv}\NormalTok{ Rio -ge }\StringTok{'g+geom_point(aes(bill, tip, color=time))'} \KeywordTok{|} \ExtensionTok{display}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=32.81in]{images/rio-scatterplot} 

}

\caption{Scatter Plot}\label{fig:unnamed-chunk-32}
\end{figure}

Using \texttt{feedgnuplot}:

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{<} \ExtensionTok{data/tips.csv}\NormalTok{ csvcut -c bill,tip }\KeywordTok{|} \FunctionTok{tr}\NormalTok{ , }\StringTok{' '} \KeywordTok{|} \ExtensionTok{header}\NormalTok{ -d }\KeywordTok{|} \ExtensionTok{feedgnuplot}\NormalTok{ \textbackslash{}}
\NormalTok{--terminal }\StringTok{'dumb 80,25'}\NormalTok{ --points --domain --unset grid --exit --style }\StringTok{'pt'} \StringTok{'14'}



  \ExtensionTok{10}\NormalTok{ ++----+------+-----+------+-----+------+-----+------+-----+------+A---++}
     \ExtensionTok{+}\NormalTok{     +      +     +      +     +      +     +      +     +      +     +}
   \ExtensionTok{9}\NormalTok{ ++                                                            A       ++}
     \KeywordTok{|}                                                                      \KeywordTok{|}
   \ExtensionTok{8}\NormalTok{ ++                                                                    ++}
     \KeywordTok{|}                                                  \ExtensionTok{A}                   \KeywordTok{|}
     \KeywordTok{|}                                                                      \KeywordTok{|}
   \ExtensionTok{7}\NormalTok{ ++                                          A                 A       ++}
     \KeywordTok{|}                             \ExtensionTok{A}\NormalTok{     A                                  }\KeywordTok{|}
   \ExtensionTok{6}\NormalTok{ ++                              A    A    A                           ++}
     \KeywordTok{|}                             \ExtensionTok{A}\NormalTok{        A                               }\KeywordTok{|}
   \ExtensionTok{5}\NormalTok{ ++       A                 A A   A A   AA A  AA      A  A     A       ++}
     \KeywordTok{|}                                \ExtensionTok{A}\NormalTok{       A    A     A                  }\KeywordTok{|}
   \ExtensionTok{4}\NormalTok{ ++          A     A  AAAA AAA A  A A  A          A                    ++}
     \KeywordTok{|}                \ExtensionTok{A}\NormalTok{   AAAAA AAA AA            A             A           }\KeywordTok{|}
     \KeywordTok{|}              \ExtensionTok{A}\NormalTok{  AAAAAAA AA A A  AA   A AA                            }\KeywordTok{|}
   \ExtensionTok{3}\NormalTok{ ++           A   AAAAAAAAAAA A A    AA           AA A                 ++}
     \KeywordTok{|}              \ExtensionTok{AAAAAAA}\NormalTok{ AA  A A A     A                   A             }\KeywordTok{|}
   \ExtensionTok{2}\NormalTok{ ++        AA AAAAAAAAA A  A  A AA  A A A                              ++}
     \ExtensionTok{+}\NormalTok{     +   AAAAAAAA +A   AA+     + A    +     +      +     +      +     +}
   \ExtensionTok{1}\NormalTok{ ++--A-+A-A---+--AA-+--A---+-----+------+--A--+------+-----+------+----++}
     \ExtensionTok{0}\NormalTok{     5      10    15     20    25     30    35     40    45     50    55}
\end{Highlighting}
\end{Shaded}

\hypertarget{line-graphs}{%
\subsection{Line Graphs}\label{line-graphs}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{data/immigration-long.csv}\NormalTok{ Rio -ge }\StringTok{'g+geom_line(aes(x=Period, '}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{'y=Count, group=Country, color=Country)) + theme(axis.text.x = '}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{'element_text(angle = -45, hjust = 0))'} \KeywordTok{|} \ExtensionTok{display}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=32.81in]{images/rio-linegraph} 

}

\caption{Line Graph}\label{fig:unnamed-chunk-35}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{data/immigration.csv} \KeywordTok{|} \ExtensionTok{csvcut}\NormalTok{ -c Period,Denmark,Netherlands,Norway,Sweden }\KeywordTok{|}
\OperatorTok{>} \ExtensionTok{header}\NormalTok{ -d }\KeywordTok{|} \FunctionTok{tr}\NormalTok{ , }\StringTok{' '} \KeywordTok{|} \ExtensionTok{feedgnuplot}\NormalTok{ --terminal }\StringTok{'dumb 80,25'}\NormalTok{ --lines \textbackslash{}}
\OperatorTok{>}\NormalTok{ --autolegend --domain --legend 0 }\StringTok{"Denmark"}\NormalTok{ --legend 1 }\StringTok{"Netherlands"}\NormalTok{ \textbackslash{}}
\OperatorTok{>}\NormalTok{ --legend 2 }\StringTok{"Norway"}\NormalTok{ --legend 3 }\StringTok{"Sweden"}\NormalTok{ --xlabel }\StringTok{"Period"}\NormalTok{ --unset grid --exit}



  \ExtensionTok{250000}\NormalTok{ ++-----%%%-------+-------+--------+-------+-------+--------+------++}
         \ExtensionTok{+}\NormalTok{  %%%% + %      +       +        +       +       + Denmark+****** +}
         \KeywordTok{|}\ExtensionTok{%%}\NormalTok{        %                                    Netherlands }\CommentTok{###### |}
         \KeywordTok{|}          \ExtensionTok{%}\NormalTok{                                         Norway }\VariableTok{$$$$$$} \KeywordTok{|}
  \ExtensionTok{200000}\NormalTok{ ++          %                                        Sweden %%%%%%++}
         \KeywordTok{|}\NormalTok{        $   }\ExtensionTok{%}                                                     \KeywordTok{|}
         \KeywordTok{|}\NormalTok{       $ $   }\ExtensionTok{%}                                                    \KeywordTok{|}
         \KeywordTok{|}\NormalTok{      $   $  }\ExtensionTok{%}                                                    \KeywordTok{|}
  \ExtensionTok{150000}\NormalTok{ ++   }\VariableTok{$$}\NormalTok{     $  %                                                  ++}
         \KeywordTok{|}\NormalTok{   $        $  }\ExtensionTok{%}                                                  \KeywordTok{|}
         \KeywordTok{|}\NormalTok{  $          $  }\ExtensionTok{%}                                                 \KeywordTok{|}
  \ExtensionTok{100000}\NormalTok{ ++$            $ %                                                ++}
         \KeywordTok{|}\NormalTok{$              $ }\ExtensionTok{%%%%%%%%%%}                                       \KeywordTok{|}
         \KeywordTok{|}\NormalTok{                $          }\ExtensionTok{%}                                      \KeywordTok{|}
         \KeywordTok{|}    \ExtensionTok{***********}  \VariableTok{$$$$$$$$$$}\NormalTok{$%                                     }\KeywordTok{|}
   \ExtensionTok{50000}\NormalTok{ +****  }\CommentTok{#########**           $%%                  #######         ++}
         \KeywordTok{|}  \CommentTok{####           ********    $$%              }\AlertTok{###}\CommentTok{       ##        |}
         \KeywordTok{|}\NormalTok{##                       }\ExtensionTok{******}\NormalTok{##           }\CommentTok{##$$$$$$$$$$$$#       |}
         \ExtensionTok{+}\NormalTok{       +        +       +      **###########}\VariableTok{$$}\NormalTok{*************       +}
       \ExtensionTok{0}\NormalTok{ ++------+--------+-------+--------*************---+--------+------++}
        \ExtensionTok{1890}\NormalTok{    1900     1910    1920     1930    1940    1950     1960    1970}
                                       \ExtensionTok{Period}
\end{Highlighting}
\end{Shaded}

\hypertarget{summary}{%
\subsection{Summary}\label{summary}}

Both \texttt{Rio} with \texttt{ggplot2} and \texttt{feedgnuplot} with Gnuplot have their advantages. The plots generated by \texttt{Rio} are obviously of much higher quality. It offers a consistent syntax that lends itself well for the command line. The only down-side would be that the output is not viewable from the command line. This is where \texttt{feedgnuplot} may come in handy. Each plot has roughly the same command-line arguments. As such, it would be straightforward to create a small Bash script that would make generating plots from and for the command line even easier. After all, with the command line having such a low resolution, we don't need a lot of flexibility.

\hypertarget{further-reading}{%
\section{Further Reading}\label{further-reading}}

\begin{itemize}
\tightlist
\item
  Wickham, Hadley. 2009. ggplot2: Elegant Graphics for Data Analysis. Springer.
\item
  Janert, Philipp K. 2009. Gnuplot in Action. Manning Publications.
\item
  Tukey, John W. 1977. Exploratory Data Analysis. Pearson.
\end{itemize}

\hypertarget{chapter-8-parallel-pipelines}{%
\chapter{Parallel Pipelines}\label{chapter-8-parallel-pipelines}}

In the previous chapters, we have been dealing with commands and pipelines that take care of an entire task at once. In practice, however, you may find yourself facing a task which requires the same command or pipeline to run multiple times. For, example, you may need to:

\begin{itemize}
\tightlist
\item
  Scrape hundreds of web pages.
\item
  Make dozens of API calls and transform their output.
\item
  Train a classifier for a range of parameter values.
\item
  Generate scatter plots for every pair of features in your dataset.
\end{itemize}

In any of the above examples, there is a certain form of repetition involved. With your favorite scripting or programming language, you take care of this with a for loop or a while loop. On the command line, the first thing you might be inclined to do is to press \texttt{\textless{}Up\textgreater{}} (which brings back the previous command), modify it if necessary, and press \texttt{\textless{}Enter\textgreater{}} (which runs the command again). This is fine for two or three times, but imagine doing this for, say, dozens of files. Such an approach quickly becomes cumbersome and time-inefficient. The good news is that we can write such loops on the command line as well. This chapter is all about repetition.

Sometimes, repeating a fast command on after the other (in serial) is sufficient. When you have multiple cores (and perhaps even multiple machines) it would be nice if you could make use of those, especially when you're faced with a data-intensive task. When using multiple cores or machines, the total running time of may be reduced significantly. In this chapter we will introduce a very powerful tool called GNU Parallel that can take care of exactly this. GNU Parallel allows us to apply a command or pipeline with a range of arguments such as numbers, lines, and files. Plus, it allows us to run our commands in parallel.

\hypertarget{overview}{%
\section{Overview}\label{overview}}

This intermezzo chapter discusses several approaches to speed up tasks that require commands and pipelines to be run many times. The main goal of this chapter is to demonstrate to you the flexibility and power of a tool called GNU Parallel. Because this tool can be combined with any other tool discussed in this book, it will positively change the way you use the command line for data science. In this chapter, you'll learn about:

\begin{itemize}
\tightlist
\item
  Running commands in serial to a range of numbers, lines, and files.
\item
  Breaking a large task into several smaller tasks.
\item
  Running pipelines in parallel using GNU Parallel.
\item
  Distributing pipelines on multiple machines.
\end{itemize}

\hypertarget{serial-processing}{%
\section{Serial Processing}\label{serial-processing}}

Before we dive into parallelization, we will look at looping in a serial fashion. It's worthwhile to know how to do this because this functionality is always available, the syntax closely resembles looping in other programming languages, and it will really make you appreciate the tool GNU Parallel.

From the examples provided in the introduction of this chapter, we can distill three types of items to loop over: (1) numbers, (2) lines, and (3) files. These three types of items will be discussed in the next three subsections, respectively.

\hypertarget{looping-over-numbers}{%
\subsection{Looping Over Numbers}\label{looping-over-numbers}}

Imagine that we need to compute the square of every even integer between 0 and 100. There's a tool called \texttt{bc}, which is basically a calculator on the command line where you can pipe an equation to. The command to compute the square of 4 looks as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{echo} \StringTok{"4^2"} \KeywordTok{|} \FunctionTok{bc}
\ExtensionTok{16}
\end{Highlighting}
\end{Shaded}

For a one-off calculation, this is perfect. However, as mentioned in the introduction, we would be creazy to press \texttt{\textless{}Up\textgreater{}}, change the number, and press \texttt{\textless{}Enter\textgreater{}} 51 times! In this case it is better to let Bash do the hard work for us by using a for loop:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\KeywordTok{for} \ExtensionTok{i}\NormalTok{ in }\DataTypeTok{\{0..100..2\}}  
\OperatorTok{>} \KeywordTok{do}
\OperatorTok{>} \BuiltInTok{echo} \StringTok{"}\VariableTok{$i}\StringTok{^2"} \KeywordTok{|} \FunctionTok{bc}      
\OperatorTok{>} \KeywordTok{done} \KeywordTok{|} \FunctionTok{tail}           
\ExtensionTok{6724}
\ExtensionTok{7056}
\ExtensionTok{7396}
\ExtensionTok{7744}
\ExtensionTok{8100}
\ExtensionTok{8464}
\ExtensionTok{8836}
\ExtensionTok{9216}
\ExtensionTok{9604}
\ExtensionTok{10000}
\end{Highlighting}
\end{Shaded}

There are a number of things going on here:

\begin{itemize}
\tightlist
\item
  Bash has a feature called brace expansion, which transforms \emph{\{0..100..2\}} into a list separated by spaces: \emph{0 2 4 \ldots{} 98 100}.
\item
  The variable \emph{i} is assigned the value \emph{1} in the first iteration, \emph{2} in the second iteration, and so forth. The value of this variable can be employed in commands by prefixing it with a dollar sign \emph{\$}. The shell will replace \emph{\$i} with its value before \texttt{echo} is being executed. Note that there can be more than one command between \texttt{do} and \texttt{done}.
\item
  We pipe the output of the for loop to \texttt{tail} so that we see the last ten values, only.
\end{itemize}

Although the syntax may appear a bit odd compared to your favorite programming language, it is worth remembering this because it is always available in the bash shell. We will shortly introduce a better and more flexible way of repeating commands.

\hypertarget{looping-over-lines}{%
\subsection{Looping Over Lines}\label{looping-over-lines}}

The second type of items we can loop over are lines. These lines can come from either a file or from standard input. This is a very generic approach because the lines can contain anything, including: numbers, dates, and email adresses.

Imagine that we want to send an email to our customers. Let's generate some fake users using the \url{https://randomuser.me/} API:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{curl}\NormalTok{ -s }\StringTok{"https://randomuser.me/api/1.2/?results=5"} \OperatorTok{>}\NormalTok{ data/users.json}
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{data/users.json}\NormalTok{ jq -r }\StringTok{'.results[].email'} \OperatorTok{>}\NormalTok{ data/emails.txt}
\NormalTok{$ }\FunctionTok{cat}\NormalTok{ data/emails.txt}
\ExtensionTok{kaylee.anderson64@example.com}
\ExtensionTok{arthur.baker92@example.com}
\ExtensionTok{chloe.graham66@example.com}
\ExtensionTok{wyatt.nelson80@example.com}
\ExtensionTok{peter.coleman75@example.com}
\end{Highlighting}
\end{Shaded}

We can loop over the lines from \emph{emails.txt} with a while-loop:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\KeywordTok{while} \BuiltInTok{read} \VariableTok{line}                                       
\OperatorTok{>} \KeywordTok{do}
\OperatorTok{>} \BuiltInTok{echo} \StringTok{"Sending invitation to }\VariableTok{$\{line\}}\StringTok{."}                 
\OperatorTok{>} \KeywordTok{done} \OperatorTok{<} \ExtensionTok{data/emails.txt}                                
\ExtensionTok{Sending}\NormalTok{ invitation to kaylee.anderson64@example.com.}
\ExtensionTok{Sending}\NormalTok{ invitation to arthur.baker92@example.com.}
\ExtensionTok{Sending}\NormalTok{ invitation to chloe.graham66@example.com.}
\ExtensionTok{Sending}\NormalTok{ invitation to wyatt.nelson80@example.com.}
\ExtensionTok{Sending}\NormalTok{ invitation to peter.coleman75@example.com.}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  In this case we need to use a while loop because Bash does not know beforehand how many lines the input consists of.
\item
  Although the curly braces around the \emph{line} variable are not necessary in this case (since variable names cannot contain periods), it's still good practice.
\item
  This redirection can also be placed before \texttt{while}.
\end{itemize}

You can also provide input to the while loop interactively by specifying the special file standard input \emph{/dev/stdin}. Press \texttt{\textless{}Ctrl-D\textgreater{}} when you are done.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\KeywordTok{while} \BuiltInTok{read} \VariableTok{i}\NormalTok{; }\KeywordTok{do} \BuiltInTok{echo} \StringTok{"You typed: }\VariableTok{$i}\StringTok{."}\KeywordTok{;} \KeywordTok{done} \OperatorTok{<} \ExtensionTok{/dev/stdin}
\ExtensionTok{one}
\ExtensionTok{You}\NormalTok{ typed: one.}
\ExtensionTok{two}
\ExtensionTok{You}\NormalTok{ typed: two.}
\ExtensionTok{three}
\ExtensionTok{You}\NormalTok{ typed: three.}
\end{Highlighting}
\end{Shaded}

This method, however, has the disadvantage that, once you press \texttt{\textless{}Enter\textgreater{}}, the command(s) between \texttt{do} and \texttt{done} are run immediately for that line of input.

\hypertarget{looping-over-files}{%
\subsection{Looping Over Files}\label{looping-over-files}}

In this section we discuss the third type of item that we often need to loop over: files.

To handle special characters, use globbing (i.e., pathname expansion) instead of \texttt{ls}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\KeywordTok{for} \ExtensionTok{filename}\NormalTok{ in *.csv}
\OperatorTok{>} \KeywordTok{do}
\OperatorTok{>} \BuiltInTok{echo} \StringTok{"Processing }\VariableTok{$\{filename\}}\StringTok{."}
\OperatorTok{>} \KeywordTok{done}
\ExtensionTok{Processing}\NormalTok{ countries.csv.}
\end{Highlighting}
\end{Shaded}

Just as with brace expansion with numbers, the \emph{*.csv} is first expanded into a list before it is being processed by the for loop.

A more elaborate alternative to finding files is \texttt{find} \citep{find}, which:

\begin{itemize}
\tightlist
\item
  Allows for elaborate searching on properties such as size, access time, and permissions.
\item
  Handles dashes.
\item
  Handles special characters such as spaces and newlines.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{find}\NormalTok{ data -name }\StringTok{'*.csv'}\NormalTok{ -exec echo }\StringTok{"Processing \{\}"} \DataTypeTok{\textbackslash{};}
\ExtensionTok{Processing}\NormalTok{ data/countries.csv}
\ExtensionTok{Processing}\NormalTok{ data/movies.csv}
\ExtensionTok{Processing}\NormalTok{ data/top250.csv}
\end{Highlighting}
\end{Shaded}

Here's the same but then using \texttt{parallel}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{find}\NormalTok{ data -name }\StringTok{'*.csv'}\NormalTok{ -print0 }\KeywordTok{|} \ExtensionTok{parallel}\NormalTok{ -0 echo }\StringTok{"Processing \{\}"}
\ExtensionTok{Processing}\NormalTok{ data/countries.csv}
\ExtensionTok{Processing}\NormalTok{ data/movies.csv}
\ExtensionTok{Processing}\NormalTok{ data/top250.csv}
\end{Highlighting}
\end{Shaded}

The \texttt{-print0} option allows file names that contain newlines or other types of white space to be correctly interpreted by programs that process the find output. If you are absolutely certain that the filenames contain no special characters such as spaces and newlines, then you can omit \texttt{-print0} and \texttt{-0} options.

\begin{rmdtip}
If the list to process becomes too complex, you can always store the result into a temporary file and then use the method to loop over lines from a file.
\end{rmdtip}

\hypertarget{parallel-processing}{%
\section{Parallel Processing}\label{parallel-processing}}

Assume that we have a very long running command, such as the one shown in Example \ref{exm:slow-sh}.

\begin{example}[~/book/ch08/slow.sh]
\protect\hypertarget{exm:slow-sh}{}{\label{exm:slow-sh} \iffalse (\textasciitilde{}/book/ch08/slow.sh) \fi{} }
\end{example}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#!/bin/bash}
\BuiltInTok{echo} \StringTok{"Starting job }\VariableTok{$1}\StringTok{"}
\VariableTok{duration=$((}\NormalTok{1+RANDOM%5}\VariableTok{))}                
\FunctionTok{sleep} \VariableTok{$duration}
\BuiltInTok{echo} \StringTok{"Job }\VariableTok{$1}\StringTok{ took }\VariableTok{$\{duration\}}\StringTok{ seconds"}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \emph{\$RANDOM} is an internal Bash function that returns a pseudorandom integer between 0 and 32767. Taking the remainder of the division of that number by 5 and adding 1 ensures that the number is between 1 and 5.
\end{itemize}

This process does not take up all the resources we have available. And it so happens that we need to run this command a lot of times. For example, we need to download a whole sequence of files.

A naive way to parallelize is to run the commands in the background:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{cd}\NormalTok{ ~/book/ch08}
\NormalTok{$ }\KeywordTok{for} \ExtensionTok{i}\NormalTok{ in }\DataTypeTok{\{1..4\}}\KeywordTok{;} \KeywordTok{do}
\OperatorTok{>} \KeywordTok{(}\ExtensionTok{slow.sh} \VariableTok{$i}\KeywordTok{;} \BuiltInTok{echo}\NormalTok{ Processed }\VariableTok{$i}\KeywordTok{)} \KeywordTok{&}  
\OperatorTok{>} \KeywordTok{done}
\NormalTok{[}\ExtensionTok{1}\NormalTok{] 3334}
\NormalTok{[}\ExtensionTok{2}\NormalTok{] 3335}
\NormalTok{[}\ExtensionTok{3}\NormalTok{] 3336}
\NormalTok{[}\ExtensionTok{4}\NormalTok{] 3338}
\NormalTok{$ }\ExtensionTok{Starting}\NormalTok{ job 2}
\ExtensionTok{Starting}\NormalTok{ job 1}
\ExtensionTok{Starting}\NormalTok{ job 3}
\ExtensionTok{Starting}\NormalTok{ job 4}
\ExtensionTok{Job}\NormalTok{ 4 took 1 seconds}
\ExtensionTok{Processed}\NormalTok{ 4}
\ExtensionTok{Job}\NormalTok{ 3 took 4 seconds}
\ExtensionTok{Job}\NormalTok{ 2 took 4 seconds}
\ExtensionTok{Processed}\NormalTok{ 3}
\ExtensionTok{Processed}\NormalTok{ 2}
\ExtensionTok{Job}\NormalTok{ 1 took 4 seconds}
\ExtensionTok{Processed}\NormalTok{ 1}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Parentheses create a subshell. The ampersand ensures that it will be executed in the background.
\end{itemize}

The problem with subshells is that they are executed all at once. There is no mechanism to control the maximum number of processes. You are not advised to use this.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\KeywordTok{while} \BuiltInTok{read} \VariableTok{i}\NormalTok{; }\KeywordTok{do}
\OperatorTok{>} \KeywordTok{(}\ExtensionTok{slow.sh} \StringTok{"}\VariableTok{$i}\StringTok{"}\KeywordTok{;} \KeywordTok{)} \KeywordTok{&}
\OperatorTok{>} \KeywordTok{done} \OperatorTok{<} \ExtensionTok{data/movies.txt}
\NormalTok{[}\ExtensionTok{1}\NormalTok{] 3404}
\NormalTok{[}\ExtensionTok{2}\NormalTok{] 3405}
\NormalTok{[}\ExtensionTok{3}\NormalTok{] 3406}
\ExtensionTok{Starting}\NormalTok{ job Star Wars}
\ExtensionTok{Starting}\NormalTok{ job Matrix}
\ExtensionTok{Starting}\NormalTok{ job Home Alone}
\NormalTok{[}\ExtensionTok{4}\NormalTok{] 3407}
\NormalTok{[}\ExtensionTok{5}\NormalTok{] 3410}
\NormalTok{$ }\ExtensionTok{Starting}\NormalTok{ job Back to the Future}
\ExtensionTok{Starting}\NormalTok{ job Indiana Jones}
\ExtensionTok{Job}\NormalTok{ Home Alone took 2 seconds}
\ExtensionTok{Job}\NormalTok{ Matrix took 2 seconds}
\ExtensionTok{Job}\NormalTok{ Star Wars took 2 seconds}
\ExtensionTok{Job}\NormalTok{ Back to the Future took 3 seconds}
\ExtensionTok{Job}\NormalTok{ Indiana Jones took 4 seconds}
\end{Highlighting}
\end{Shaded}

\begin{rmdnote}
Not everything can be parallelized: API calls may be limited to a certain number, or some commands can only have one instance.
\end{rmdnote}

\begin{rmdimportant}
Quoting is important. If we did not quote \emph{\$i}, then only the first word of each movie would have been passed to the script \emph{slow.sh}.
\end{rmdimportant}

There are two problems with this naive approach. First, there's no way to control how many processes you are running concurrently. Second, logging: which output belongs to which input.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{data/movies}\NormalTok{ parallel -j3 slow.sh }\StringTok{"\{\}"}
\ExtensionTok{Starting}\NormalTok{ job Star Wars}
\ExtensionTok{Job}\NormalTok{ Star Wars took 3 seconds}
\ExtensionTok{Starting}\NormalTok{ job Home Alone}
\ExtensionTok{Job}\NormalTok{ Home Alone took 3 seconds}
\ExtensionTok{Starting}\NormalTok{ job Matrix}
\ExtensionTok{Job}\NormalTok{ Matrix took 4 seconds}
\ExtensionTok{Starting}\NormalTok{ job Indiana Jones}
\ExtensionTok{Job}\NormalTok{ Indiana Jones took 1 seconds}
\ExtensionTok{Starting}\NormalTok{ job Back to the Future}
\ExtensionTok{Job}\NormalTok{ Back to the Future took 5 seconds}
\end{Highlighting}
\end{Shaded}

\hypertarget{introducing-gnu-parallel}{%
\subsection{Introducing GNU Parallel}\label{introducing-gnu-parallel}}

GNU Parallel is a command-line tool written by Ole Tange. This tool allows us to parallelize commands and pipelines. The beauty of this tool is that existing tools can be used as they are; they do not need to be modified.

\begin{rmdcaution}
You may have noticed that we keep writing GNU Parallel. That's because there are two tools with the name ``parallel''. If you make use of the Data Science Toolbox then you already have the correct one installed. Otherwise, please double check that you have installed the correct tool installed by running \texttt{parallel\ -\/-version}.
\end{rmdcaution}

Before we go into the details of GNU Parallel, here's a little teaser to show you how easy it is to parallelize the for loop stated above:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{seq}\NormalTok{ 5 }\KeywordTok{|} \ExtensionTok{parallel} \StringTok{"echo \{\}^2 | bc"}
\ExtensionTok{1}
\ExtensionTok{4}
\ExtensionTok{9}
\ExtensionTok{16}
\ExtensionTok{25}
\end{Highlighting}
\end{Shaded}

This is \texttt{parallel} in its simplest form: without any arguments. As you can see it basically acts as a for loop. (We'll explain later what is going on exactly.) With no less than 110 command-line arguments (!), GNU Parallel offers a lot of additional functionality. Don't worry, by the end of this chapter, you'll have a solid understanding of the most important ones.

Install GNU Parallel by running the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{wget}\NormalTok{ http://ftp.gnu.org/gnu/parallel/parallel-latest.tar.bz2}
\NormalTok{$ }\FunctionTok{tar}\NormalTok{ -xvjf parallel-latest.tar.bz2 }\OperatorTok{>}\NormalTok{ extracted-files}
\NormalTok{$ }\BuiltInTok{cd} \VariableTok{$(}\FunctionTok{head}\NormalTok{ -n 1 extracted-files}\VariableTok{)}
\NormalTok{$ }\ExtensionTok{./configure} \KeywordTok{&&} \FunctionTok{make} \KeywordTok{&&} \FunctionTok{sudo}\NormalTok{ make install}
\end{Highlighting}
\end{Shaded}

You can verify that you have correctly installed GNU Parallel:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{parallel}\NormalTok{ --version }\KeywordTok{|} \FunctionTok{head}\NormalTok{ -n 1}
\ExtensionTok{GNU}\NormalTok{ parallel 20140622}
\end{Highlighting}
\end{Shaded}

You can safely delete the created files and directories.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{cd}\NormalTok{ ..}
\NormalTok{$ }\FunctionTok{rm}\NormalTok{ -r }\VariableTok{$(}\FunctionTok{head}\NormalTok{ -n 1 extracted-files}\VariableTok{)}
\NormalTok{$ }\FunctionTok{rm}\NormalTok{ parallel-latest.tar.bz2 extracted-files}
\end{Highlighting}
\end{Shaded}

\begin{rmdtip}
If you use \texttt{parallel} as often as us then you may want to create an alias (for example \texttt{p}) by adding \emph{alias p=parallel} to your \emph{.bashrc}. (In this chapter we'll just use \texttt{parallel} for clarity.)
\end{rmdtip}

\hypertarget{specifying-input}{%
\subsection{Specifying Input}\label{specifying-input}}

The most important argument to GNU Parallel, is the command that you would like to run for every input. The question is: where should the input item be inserted in the command line? If you do not specify anything, then the input item will be appended to the command. While this is usually what you want, we advise you to be explicit about where the input item should be inserted in the command using one or more placeholders.

\begin{rmdnote}
There are many ways to provide input to GNU Parallel. We prefer piping the input (as we do throughout this chapter) because that is generally applicable and allows us to construct a pipeline from left to right. Please consult the man page of parallel to read about other ways to provide input.
\end{rmdnote}

In most cases, you probably want to use the entire input item as it is. For this, you only need one placeholder. You specify the placeholder, in other words, where to put the input item, with two curly braces:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{seq}\NormalTok{ 5 }\KeywordTok{|} \ExtensionTok{parallel}\NormalTok{ echo \{\}}
\end{Highlighting}
\end{Shaded}

When the input item is a file, there are a couple of special placeholders you can use to modify the file name. For example, with \emph{\{./\}}, only the base name of the file name will be used.

If the input line has multiple parts separated by a delimiter you can add numbers to the placeholders. For example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{input.csv} \KeywordTok{|} \ExtensionTok{parallel}\NormalTok{ -C, }\StringTok{"mv \{1\} \{2\}"}
\end{Highlighting}
\end{Shaded}

Here, you can apply the same placeholder modifiers. It is also possible to reuse the same input item. If the input to parallel is a CSV file with a header, then you can use the column names as placeholders:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{input.csv} \KeywordTok{|} \ExtensionTok{parallel}\NormalTok{ -C, --header : }\StringTok{"invite \{name\} \{email\}"}
\end{Highlighting}
\end{Shaded}

Sometimes you just want to run the same command without any changing inputs. This is also possible in parallel. We just have to specify the \texttt{-N0} parameter and give as input as many lines as you want to execute:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{seq}\NormalTok{ 5 }\KeywordTok{|} \ExtensionTok{parallel}\NormalTok{ -N0 }\StringTok{"echo The command line rules"}
\ExtensionTok{The}\NormalTok{ command line rules}
\ExtensionTok{The}\NormalTok{ command line rules}
\ExtensionTok{The}\NormalTok{ command line rules}
\ExtensionTok{The}\NormalTok{ command line rules}
\end{Highlighting}
\end{Shaded}

\begin{rmdtip}
If you ever wonder whether your GNU Parallel command is set up correctly, you can add the \texttt{-\/-dryrun} option. Instead of actually executing the command, GNU Parallel will print out all the commands exactly as if they would have been executed.
\end{rmdtip}

\hypertarget{controlling-the-number-of-concurrent-jobs}{%
\subsection{Controlling the Number of Concurrent Jobs}\label{controlling-the-number-of-concurrent-jobs}}

By default, parallel runs one job per CPU core. You can control the number of jobs that will be run in parallel with the \texttt{-j} command-line argument, which is short for \emph{jobs}. Simply specifying a number means that many jobs will be run in parallel. If you put a plus sign in front of the number then parallel will run \emph{N} jobs plus the number of CPU cores. If you put a minus sign in front of the number then parallel will run \emph{N-M} jobs. Where \emph{N} is the number of CPU cores. You can also specify a percentage to the \texttt{-j} parameter. So, the default is 100\% of the number of CPU cores. The optimal number of jobs to run in parallel depends on the actual commands you are running.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{seq}\NormalTok{ 5 }\KeywordTok{|} \ExtensionTok{parallel}\NormalTok{ -j0 }\StringTok{"echo Hi \{\}"}
\ExtensionTok{Hi}\NormalTok{ 1}
\ExtensionTok{Hi}\NormalTok{ 2}
\ExtensionTok{Hi}\NormalTok{ 3}
\ExtensionTok{Hi}\NormalTok{ 4}
\ExtensionTok{Hi}\NormalTok{ 5}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{seq}\NormalTok{ 5 }\KeywordTok{|} \ExtensionTok{parallel}\NormalTok{ -j200% }\StringTok{"echo Hi \{\}"}
\ExtensionTok{Hi}\NormalTok{ 1}
\ExtensionTok{Hi}\NormalTok{ 2}
\ExtensionTok{Hi}\NormalTok{ 3}
\ExtensionTok{Hi}\NormalTok{ 4}
\ExtensionTok{Hi}\NormalTok{ 5}
\end{Highlighting}
\end{Shaded}

If you specify \texttt{-j1}, then the commands will be run in serial. Even though this doesn't do the name of the tool of justice, it still has its uses. For example, when you need to access an API which only allows one connection at a time. If you specify \texttt{-j0}, then parallel will run as many jobs in parallel as possible. This can be compared to our loop with subshells. This is not advised.

\hypertarget{logging-and-output}{%
\subsection{Logging and Output}\label{logging-and-output}}

To save the output of each command, you might be tempted to the following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{seq}\NormalTok{ 5 }\KeywordTok{|} \ExtensionTok{parallel} \StringTok{"echo }\DataTypeTok{\textbackslash{}"}\StringTok{Hi \{\}}\DataTypeTok{\textbackslash{}"}\StringTok{ > data/ch08/hi-\{\}.txt"}
\end{Highlighting}
\end{Shaded}

This will save the output into individual files. Or, if you want to save everything into one big file you could do the following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{seq}\NormalTok{ 5 }\KeywordTok{|} \ExtensionTok{parallel} \StringTok{"echo Hi \{\}"} \OperatorTok{>>}\NormalTok{ data/ch08/one-big-file.txt}
\end{Highlighting}
\end{Shaded}

However, GNU Parallel offers the \texttt{-\/-results} option, which stores the output of each job into a separate file, where the filename is based on the input values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{seq}\NormalTok{ 5 }\KeywordTok{|} \ExtensionTok{parallel}\NormalTok{ --results data/ch08/outdir }\StringTok{"echo Hi \{\}"}
\ExtensionTok{Hi}\NormalTok{ 1}
\ExtensionTok{Hi}\NormalTok{ 2}
\ExtensionTok{Hi}\NormalTok{ 3}
\ExtensionTok{Hi}\NormalTok{ 4}
\ExtensionTok{Hi}\NormalTok{ 5}
\NormalTok{$ }\FunctionTok{find}\NormalTok{ data/ch08/outdir}
\ExtensionTok{data/ch08/outdir}
\ExtensionTok{data/ch08/outdir/1}
\ExtensionTok{data/ch08/outdir/1/1}
\ExtensionTok{data/ch08/outdir/1/1/stderr}
\ExtensionTok{data/ch08/outdir/1/1/stdout}
\ExtensionTok{data/ch08/outdir/1/3}
\ExtensionTok{data/ch08/outdir/1/3/stderr}
\ExtensionTok{data/ch08/outdir/1/3/stdout}
\ExtensionTok{data/ch08/outdir/1/5}
\ExtensionTok{data/ch08/outdir/1/5/stderr}
\ExtensionTok{data/ch08/outdir/1/5/stdout}
\ExtensionTok{data/ch08/outdir/1/2}
\ExtensionTok{data/ch08/outdir/1/2/stderr}
\ExtensionTok{data/ch08/outdir/1/2/stdout}
\ExtensionTok{data/ch08/outdir/1/4}
\ExtensionTok{data/ch08/outdir/1/4/stderr}
\ExtensionTok{data/ch08/outdir/1/4/stdout}
\end{Highlighting}
\end{Shaded}

When you're running multiple jobs in parallel, the order in which the jobs are run may not correspond to the order of the input. The output of jobs is therefore also mixed up. To keep the same order, simply specify the \texttt{-\/-keep-order} option or \texttt{-k} option.

Sometimes it's useful to record which input generated which output. GNU Parallel allows you to \emph{tag} the output with the \texttt{-\/-tag} option:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{seq}\NormalTok{ 5 }\KeywordTok{|} \ExtensionTok{parallel}\NormalTok{ --tag }\StringTok{"echo Hi \{\}"}
\ExtensionTok{1}\NormalTok{       Hi 1}
\ExtensionTok{2}\NormalTok{       Hi 2}
\ExtensionTok{3}\NormalTok{       Hi 3}
\ExtensionTok{4}\NormalTok{       Hi 4}
\ExtensionTok{5}\NormalTok{       Hi 5}
\end{Highlighting}
\end{Shaded}

\hypertarget{creating-parallel-tools}{%
\subsection{Creating Parallel Tools}\label{creating-parallel-tools}}

The \texttt{bc} tool, which we used in the beginning of the chapter, is not parallel by itself. However, we can parallelize it using \texttt{parallel}. The Data Science toolbox contains a tool called \texttt{pbc} \citep{pbc}. Its code is shown in Example \ref{exm:script-pbc}.

\begin{example}[Parallel bc]
\protect\hypertarget{exm:script-pbc}{}{\label{exm:script-pbc} \iffalse (Parallel bc) \fi{} }
\end{example}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#!/usr/bin/env bash}
\ExtensionTok{parallel}\NormalTok{ -C, -k -j100% }\StringTok{"echo '}\VariableTok{$1}\StringTok{' | bc -l"}
\end{Highlighting}
\end{Shaded}

This tool allows us to simplify the code used in the beginning of the chapter too:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{seq}\NormalTok{ 100 }\KeywordTok{|} \ExtensionTok{pbc} \StringTok{'\{1\}^2'} \KeywordTok{|} \FunctionTok{tail}
\ExtensionTok{8281}
\ExtensionTok{8464}
\ExtensionTok{8649}
\ExtensionTok{8836}
\ExtensionTok{9025}
\ExtensionTok{9216}
\ExtensionTok{9409}
\ExtensionTok{9604}
\ExtensionTok{9801}
\ExtensionTok{10000}
\end{Highlighting}
\end{Shaded}

\hypertarget{distributed-processing}{%
\section{Distributed Processing}\label{distributed-processing}}

Sometimes you need more power than your local machine, even with all its cores, can offer. Luckily, GNU Parallel can also leverage the power of remote machines, which really allows us to speed up our pipeline.

What's great is that GNU Parallel does not have to be installed on the remote machine. All that's required is that you can connect to the remote machine via SSH, which is also what GNU Parallel uses to distribute our pipeline. (Having GNU Parallel installed is helpful because it can then determine how many cores to employ on each remote machine; more on this later.)

First, we're going to obtain a list of running AWS EC2 instances. Don't worry if you don't have any remote machines, you can replace any occurrence of \texttt{-\/-slf\ hostnames}, which tells GNU Parallel which remote machines to use, with \texttt{-\/-sshlogin\ :}. This way, you can still follow along with the examples in this section.

Once we know which remote machines to take over, we're going to consider three flavors of distributed processing:

\begin{itemize}
\item
  Simply running ordinary commands on remote machines.
\item
  Distributing local data directly among remote machines.

  \begin{itemize}
  \tightlist
  \item
    Sending files to remote machines, process them, and retrieve the results.
  \end{itemize}
\end{itemize}

\hypertarget{get-list-of-running-aws-ec2-instances}{%
\subsection{Get List of Running AWS EC2 Instances}\label{get-list-of-running-aws-ec2-instances}}

In this section we're creating a file named \emph{hostnames} that will contain one hostname of a remote machine per line. We're using Amazon Web Services as an example. If you're using a different cloud computing service, or have your own servers, please make sure that you create a \emph{hostnames} file yourself.

We can obtain a list of running AWS EC2 instances from the commanding using \texttt{aws}, the command-line interface to the AWS API \citep{aws}. If you're not using the Data Science Toolbox, install \texttt{awscli} using \texttt{pip} \citep{pip} as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{pip}\NormalTok{ install awscli}
\end{Highlighting}
\end{Shaded}

With \texttt{aws}, you can virtually do everything you can do with the online AWS Management Console. We use this command to obtain a list of running EC2 instances from AWS, but it can do a lot more.

We assume that you know how to launch instances, either through the online Management Console or through the \texttt{aws} command-line tool.

The command \texttt{aws\ ec2\ describe-instances} returns a lot of information about all your EC2 instances in JSON format (see \url{http://docs.aws.amazon.com/cli/latest/reference/ec2/describe-instances.html}). We extract the relevant fields using \texttt{jq}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{aws}\NormalTok{ ec2 describe-instances }\KeywordTok{|} \ExtensionTok{jq} \StringTok{'.Reservations[].Instances[] | '}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{'\{public_dns: .PublicDnsName, state: .State.Name\}'}
\KeywordTok{\{}
  \StringTok{"state"}\NormalTok{: }\StringTok{"running"}\NormalTok{,}
  \StringTok{"public_dns"}\NormalTok{: }\StringTok{"ec2-54-88-122-140.compute-1.amazonaws.com"}
\KeywordTok{\}}
\KeywordTok{\{}
  \StringTok{"state"}\NormalTok{: }\StringTok{"stopped"}\NormalTok{,}
  \StringTok{"public_dns"}\NormalTok{: }\ExtensionTok{null}
\KeywordTok{\}}
\end{Highlighting}
\end{Shaded}

The possible states of an EC2 instance are: \emph{pending}, \emph{running}, \emph{shutting-down}, \emph{terminated}, \emph{stopping}, and \emph{stopped}. Since we can only distribute our pipeline to running instances, we filter out the non-running instances:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{aws}\NormalTok{ ec2 describe-instances }\KeywordTok{|} \ExtensionTok{jq}\NormalTok{ -r }\StringTok{'.Reservations[].Instances[] | '}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{'select(.State.Name=="running") | .PublicDnsName'} \OperatorTok{>}\NormalTok{ hostnames}
\NormalTok{$ }\FunctionTok{cat}\NormalTok{ hostnames}
\ExtensionTok{ec2-54-88-122-140.compute-1.amazonaws.com}
\ExtensionTok{ec2-54-88-89-208.compute-1.amazonaws.com}
\end{Highlighting}
\end{Shaded}

(If we would leave out \texttt{-r}, which stands for \emph{raw}, the hostnames would have been surrounded by double quotes.) We save the output to \emph{hostnames}, so that we can pass this to \texttt{parallel} later.

As mentioned, \texttt{parallel} employs \texttt{ssh} to connect to the EC2 instances. Add the following to \emph{\textasciitilde{}/.ssh/config}, so that \texttt{ssh} knows how to connect to the EC2 instances:

\begin{verbatim}
Host *.amazonaws.com
    IdentityFile ~/.ssh/MyKeyFile.pem
    User ubuntu
\end{verbatim}

Depending on your which distribution your running, your username may be different than \emph{ubuntu}.

\hypertarget{running-commands-on-remote-machines}{%
\subsection{Running Commands on Remote Machines}\label{running-commands-on-remote-machines}}

The first flavor of distributed processing is to simply run ordinary commands on remote machines. Let's first double check that parallel is working by running the command-line tools \texttt{hostname} List of hosts:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{parallel}\NormalTok{ --nonall --slf hostnames hostname}
\ExtensionTok{ip-172-31-23-204}
\ExtensionTok{ip-172-31-23-205}
\end{Highlighting}
\end{Shaded}

Here, \texttt{-\/-slf} is short for \texttt{-\/-sshloginfile} and \texttt{-\/-nonall} instructs \texttt{parallel} to execute the same command on every remote machine in the \emph{hostnames} file without using any parameters. Remember, if you don't have any remote machines to utilize, you can replace \texttt{-\/-slf\ hostnames} with \texttt{-\/-sshlogin\ :} so that the command is run on your local machine:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{parallel}\NormalTok{ --nonall --sshlogin : hostname}
\ExtensionTok{data-science-toolbox}
\end{Highlighting}
\end{Shaded}

Running the same command on every remote machine once only requires one core per machine. If we wanted to distribute the list of arguments passed in to \texttt{parallel} then it could potentially use more than one core. If the number of cores are not specified explicitly, \texttt{parallel} will try to determine this:

\begin{verbatim}
$ seq 2 | parallel --slf hostnames echo 2>&1 | fold
bash: parallel: command not found
parallel: Warning: Could not figure out number of cpus on ec2-54-88-122-140.comp
ute-1.amazonaws.com (). Using 1.
1
2
\end{verbatim}

In this case, we have \texttt{parallel} installed on one of the two remote machines. We're getting a warning message indicating that \texttt{parallel} is not found on one of them. As a result, \texttt{parallel} cannot determine the number of cores and will default to using one core. When you receive this warning message, you can do one of the following four things:

\begin{itemize}
\tightlist
\item
  Don't worry, and be happy with using one core per machine.
\item
  Specify the number of jobs per machine via \texttt{-j}.
\item
  Specify the number of cores to use per machine by putting, for example, \emph{2/} if you want two cores, in front of each hostname in the \emph{hostnames} file.
\item
  Install GNU Parallel using a package manager. For example, on Ubuntu:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{parallel}\NormalTok{ --nonall --slf hostnames }\StringTok{"sudo apt-get install -y parallel"}
\end{Highlighting}
\end{Shaded}

\hypertarget{distributing-local-data-among-remote-machines}{%
\subsection{Distributing Local Data among Remote Machines}\label{distributing-local-data-among-remote-machines}}

The second flavor of distributed processing is to distribute local data directly among remote machines. Imagine you have one very large data set that you want to process it using multiple remote machines. For simplicity, we're going to sum all integers from 1 to 1000. First, let's double check that our input is actually being distributed by printing the hostname of the remote machine and the length of the input it received using \texttt{wc}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{seq}\NormalTok{ 1000 }\KeywordTok{|} \ExtensionTok{parallel}\NormalTok{ -N100 --pipe --slf hosts  }\StringTok{"(hostname; wc -l) | paste -sd:"}
\ExtensionTok{ip-172-31-23-204}\NormalTok{:100}
\ExtensionTok{ip-172-31-23-205}\NormalTok{:100}
\ExtensionTok{ip-172-31-23-205}\NormalTok{:100}
\ExtensionTok{ip-172-31-23-204}\NormalTok{:100}
\ExtensionTok{ip-172-31-23-205}\NormalTok{:100}
\ExtensionTok{ip-172-31-23-204}\NormalTok{:100}
\ExtensionTok{ip-172-31-23-205}\NormalTok{:100}
\ExtensionTok{ip-172-31-23-204}\NormalTok{:100}
\ExtensionTok{ip-172-31-23-205}\NormalTok{:100}
\ExtensionTok{ip-172-31-23-204}\NormalTok{:100}
\end{Highlighting}
\end{Shaded}

We can verify that our 1000 numbers get distributed evenly in subsets of 100 (as specified by \texttt{-N100}). Now, we're ready to sum all those numbers:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{seq}\NormalTok{ 1000 }\KeywordTok{|} \ExtensionTok{parallel}\NormalTok{ -N100 --pipe --slf hosts }\StringTok{"paste -sd+ | bc"} \KeywordTok{|} \ExtensionTok{paste}\NormalTok{ -sd+ }\KeywordTok{|} \FunctionTok{bc}
\ExtensionTok{500500}
\end{Highlighting}
\end{Shaded}

Here, we immediately also sum the ten sums we get back from the remote machines. Let's double check the answer is correct:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{seq}\NormalTok{ 1000 }\KeywordTok{|} \ExtensionTok{paste}\NormalTok{ -sd+ }\KeywordTok{|} \FunctionTok{bc}
\ExtensionTok{500500}
\end{Highlighting}
\end{Shaded}

Good, that works. If you have a larger command that you want to execute on the remote machines, you can also put it in a separate script and upload it script with \texttt{parallel}.

Let's create a very simple command-line tool called \emph{sum}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#!/usr/bin/env bash}
\ExtensionTok{paste}\NormalTok{ -sd+ }\KeywordTok{|} \FunctionTok{bc}
\end{Highlighting}
\end{Shaded}

Don't forget to make it executable as discussed in \protect\hyperlink{chapter-4-creating-reusable-command-line-tools}{Chapter 4}. The following command first uploads the file \emph{sum}:

\begin{verbatim}
$ seq 1000 | parallel -N100 --basefile sum --pipe --slf hosts './sum' | ./sum
500500
\end{verbatim}

Of course, summing 1000 numbers is only a toy example. It would have been much faster to do this locally. However, we hope it's clear from this that GNU Parallel can be incredibly powerful.

\hypertarget{processing-files-on-remote-machines}{%
\subsection{Processing Files on Remote Machines}\label{processing-files-on-remote-machines}}

The third flavor of distributed processing is to send files to remote machines, process them, and retrieve the results. Imagine that we want to count for each borough of New York City, how often they receive service calls on 311. We don't have that data on our local machine yet, so let's first obtain it from \url{https://data.cityofnewyork.us/} using their great API:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{seq}\NormalTok{ 0 100 900 }\KeywordTok{|} \ExtensionTok{parallel}  \StringTok{"curl -sL 'http://data.cityofnewyork.us/resource'"}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{"'/erm2-nwe9.json?}\DataTypeTok{\textbackslash{}$}\StringTok{limit=100&}\DataTypeTok{\textbackslash{}$}\StringTok{offset=\{\}' | jq -c '.[]' | gzip > \{#\}.json.gz"}
\end{Highlighting}
\end{Shaded}

Note that \texttt{jq\ -c\ \textquotesingle{}.{[}{]}\textquotesingle{}} is used to flatten the array of JSON objects so that there's one line. We now have 10 files containing compressed JSON data. Let's see what one line of JSON looks like:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{zcat}\NormalTok{ 1.json.gz }\KeywordTok{|} \FunctionTok{head}\NormalTok{ -n 1 }\KeywordTok{|} \ExtensionTok{fold}
\NormalTok{\{}\StringTok{"school_region"}\NormalTok{:}\StringTok{"Unspecified"}\NormalTok{,}\StringTok{"park_facility_name"}\NormalTok{:}\StringTok{"Unspecified"}\NormalTok{,}\StringTok{"x_coordinate_}
\StringTok{state_plane"}\NormalTok{:}\StringTok{"945974"}\NormalTok{,}\StringTok{"agency_name"}\NormalTok{:}\StringTok{"Department of Health and Mental Hygiene"}\NormalTok{,}\StringTok{"u}
\StringTok{nique_key"}\NormalTok{:}\StringTok{"147"}\NormalTok{,}\StringTok{"facility_type"}\NormalTok{:}\StringTok{"N/A"}\NormalTok{,}\StringTok{"status"}\NormalTok{:}\StringTok{"Assigned"}\NormalTok{,}\StringTok{"school_address"}\NormalTok{:}\StringTok{"Uns}
\StringTok{pecified"}\NormalTok{,}\StringTok{"created_date"}\NormalTok{:}\StringTok{"2006-08-29T21:25:23"}\NormalTok{,}\StringTok{"community_board"}\NormalTok{:}\StringTok{"01 STATEN ISLA}
\StringTok{ND"}\NormalTok{,}\StringTok{"incident_zip"}\NormalTok{:}\StringTok{"10302"}\NormalTok{,}\StringTok{"school_name"}\NormalTok{:}\StringTok{"Unspecified"}\NormalTok{,}\StringTok{"location"}\NormalTok{:\{}\StringTok{"latitude"}\NormalTok{:}\StringTok{"4}
\StringTok{0.62745427115626"}\NormalTok{,}\StringTok{"longitude"}\NormalTok{:}\StringTok{"-74.13789056665027"}\NormalTok{,}\StringTok{"needs_recoding"}\NormalTok{:}\ExtensionTok{false}\NormalTok{\},}\StringTok{"comp}
\StringTok{laint_type"}\NormalTok{:}\StringTok{"Food Establishment"}\NormalTok{,}\StringTok{"city"}\NormalTok{:}\StringTok{"STATEN ISLAND"}\NormalTok{,}\StringTok{"park_borough"}\NormalTok{:}\StringTok{"STATEN I}
\StringTok{SLAND"}\NormalTok{,}\StringTok{"school_state"}\NormalTok{:}\StringTok{"Unspecified"}\NormalTok{,}\StringTok{"longitude"}\NormalTok{:}\StringTok{"-74.13789056665027"}\NormalTok{,}\StringTok{"intersecti}
\StringTok{on_street_1"}\NormalTok{:}\StringTok{"DECKER AVENUE"}\NormalTok{,}\StringTok{"y_coordinate_state_plane"}\NormalTok{:}\StringTok{"167905"}\NormalTok{,}\StringTok{"due_date"}\NormalTok{:}\StringTok{"200}
\StringTok{6-10-05T21:25:23"}\NormalTok{,}\StringTok{"latitude"}\NormalTok{:}\StringTok{"40.62745427115626"}\NormalTok{,}\StringTok{"school_code"}\NormalTok{:}\StringTok{"Unspecified"}\NormalTok{,}\StringTok{"sc}
\StringTok{hool_city"}\NormalTok{:}\StringTok{"Unspecified"}\NormalTok{,}\StringTok{"address_type"}\NormalTok{:}\StringTok{"INTERSECTION"}\NormalTok{,}\StringTok{"intersection_street_2"}\NormalTok{:}\StringTok{"}
\StringTok{BARRETT AVENUE"}\NormalTok{,}\StringTok{"school_number"}\NormalTok{:}\StringTok{"Unspecified"}\NormalTok{,}\StringTok{"resolution_action_updated_date"}\NormalTok{:}\StringTok{"}
\StringTok{2006-10-06T00:00:17"}\NormalTok{,}\StringTok{"descriptor"}\NormalTok{:}\StringTok{"Handwashing"}\NormalTok{,}\StringTok{"school_zip"}\NormalTok{:}\StringTok{"Unspecified"}\NormalTok{,}\StringTok{"loca}
\StringTok{tion_type"}\NormalTok{:}\StringTok{"Restaurant/Bar/Deli/Bakery"}\NormalTok{,}\StringTok{"agency"}\NormalTok{:}\StringTok{"DOHMH"}\NormalTok{,}\StringTok{"borough"}\NormalTok{:}\StringTok{"STATEN ISLAN}
\StringTok{D"}\NormalTok{,}\StringTok{"school_phone_number"}\NormalTok{:}\StringTok{"Unspecified"}\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

If we were to get the total number of service calls per borough on our local machine, we would run the following command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{zcat}\NormalTok{ *.json.gz }\KeywordTok{|}               
\OperatorTok{>} \ExtensionTok{./jq}\NormalTok{ -r }\StringTok{'.borough'} \KeywordTok{|}           
\OperatorTok{>} \FunctionTok{tr} \StringTok{'[A-Z] '} \StringTok{'[a-z]_'} \KeywordTok{|}         
\OperatorTok{>} \FunctionTok{sort} \KeywordTok{|} \FunctionTok{uniq}\NormalTok{ -c }\KeywordTok{|}               
\OperatorTok{>} \FunctionTok{awk} \StringTok{'\{print $2","$1\}'} \KeywordTok{|}        
\OperatorTok{>} \ExtensionTok{header}\NormalTok{ -a borough,count }\KeywordTok{|}      
\OperatorTok{>} \ExtensionTok{csvsort}\NormalTok{ -rc count }\KeywordTok{|} \ExtensionTok{csvlook}    
\KeywordTok{|}\ExtensionTok{----------------+--------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{borough}       \KeywordTok{|} \ExtensionTok{count}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{----------------+--------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{unspecified}   \KeywordTok{|} \ExtensionTok{467}    \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{manhattan}     \KeywordTok{|} \ExtensionTok{274}    \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{brooklyn}      \KeywordTok{|} \ExtensionTok{103}    \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{queens}        \KeywordTok{|} \ExtensionTok{77}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{bronx}         \KeywordTok{|} \ExtensionTok{44}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{staten_island} \KeywordTok{|} \ExtensionTok{35}     \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{----------------+--------}\KeywordTok{|}
\end{Highlighting}
\end{Shaded}

Because this is quite a long pipeline, and because we're using it again in a moment with \texttt{parallel}, it's worth to go over it:

\begin{itemize}
\item
  Expand all compressed files using \texttt{zcat}.
\item
  For each call, extract the name of the borough using \texttt{jq}.
\item
  Convert borough names to lowercase and replace spaces with underscores (because \texttt{awk} splits on whitespace by default).
\item
  Count the occurrences of each borough using \texttt{sort} and \texttt{uniq}.
\item
  Reverse the count and borough and make it comma delimited using \texttt{awk}.
\item
  Add a header using \texttt{header}.
\item
  Sort by count and print table using \texttt{csvsort} \citep{csvsort}.
\end{itemize}

Imagine, for a moment, that our own machine is so slow that we simply cannot perform this pipeline locally. We can use GNU Parallel to distribute the local files among the remote machines, let them do the processing, and retrieve the results:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{ls}\NormalTok{ *.json.gz }\KeywordTok{|}                                                            
\OperatorTok{>} \ExtensionTok{parallel}\NormalTok{ -v --basefile jq }\DataTypeTok{\textbackslash{} }                                              
\OperatorTok{>} \ExtensionTok{--trc} \DataTypeTok{\{.\}}\NormalTok{.csv }\DataTypeTok{\textbackslash{} }                                                          
\OperatorTok{>} \ExtensionTok{--slf}\NormalTok{ hostnames }\DataTypeTok{\textbackslash{} }                                                        
\OperatorTok{>} \StringTok{"zcat \{\} | ./jq -r '.borough' | tr '[A-Z] ' '[a-z]_' | sort | uniq -c |"}\KeywordTok{\textbackslash{}}
\OperatorTok{>} \StringTok{" awk '\{print }\DataTypeTok{\textbackslash{}$}\StringTok{2}\DataTypeTok{\textbackslash{}"}\StringTok{,}\DataTypeTok{\textbackslash{}"\textbackslash{}$}\StringTok{1\}' > \{.\}.csv"}                                    
\FunctionTok{zcat}\NormalTok{ 10.json.gz }\KeywordTok{|} \ExtensionTok{./jq}\NormalTok{ -r }\StringTok{'.borough'} \KeywordTok{|} \FunctionTok{sort} \KeywordTok{|} \FunctionTok{uniq}\NormalTok{ -c }\KeywordTok{|} \FunctionTok{awk} \StringTok{'\{print $2","$1\}'}
\FunctionTok{zcat}\NormalTok{ 2.json.gz }\KeywordTok{|} \ExtensionTok{./jq}\NormalTok{ -r }\StringTok{'.borough'} \KeywordTok{|} \FunctionTok{sort} \KeywordTok{|} \FunctionTok{uniq}\NormalTok{ -c }\KeywordTok{|} \FunctionTok{awk} \StringTok{'\{print $2","$1\}'}
\FunctionTok{zcat}\NormalTok{ 1.json.gz }\KeywordTok{|} \ExtensionTok{./jq}\NormalTok{ -r }\StringTok{'.borough'} \KeywordTok{|} \FunctionTok{sort} \KeywordTok{|} \FunctionTok{uniq}\NormalTok{ -c }\KeywordTok{|} \FunctionTok{awk} \StringTok{'\{print $2","$1\}'}
\FunctionTok{zcat}\NormalTok{ 3.json.gz }\KeywordTok{|} \ExtensionTok{./jq}\NormalTok{ -r }\StringTok{'.borough'} \KeywordTok{|} \FunctionTok{sort} \KeywordTok{|} \FunctionTok{uniq}\NormalTok{ -c }\KeywordTok{|} \FunctionTok{awk} \StringTok{'\{print $2","$1\}'}
\FunctionTok{zcat}\NormalTok{ 4.json.gz }\KeywordTok{|} \ExtensionTok{./jq}\NormalTok{ -r }\StringTok{'.borough'} \KeywordTok{|} \FunctionTok{sort} \KeywordTok{|} \FunctionTok{uniq}\NormalTok{ -c }\KeywordTok{|} \FunctionTok{awk} \StringTok{'\{print $2","$1\}'}
\FunctionTok{zcat}\NormalTok{ 5.json.gz }\KeywordTok{|} \ExtensionTok{./jq}\NormalTok{ -r }\StringTok{'.borough'} \KeywordTok{|} \FunctionTok{sort} \KeywordTok{|} \FunctionTok{uniq}\NormalTok{ -c }\KeywordTok{|} \FunctionTok{awk} \StringTok{'\{print $2","$1\}'}
\FunctionTok{zcat}\NormalTok{ 6.json.gz }\KeywordTok{|} \ExtensionTok{./jq}\NormalTok{ -r }\StringTok{'.borough'} \KeywordTok{|} \FunctionTok{sort} \KeywordTok{|} \FunctionTok{uniq}\NormalTok{ -c }\KeywordTok{|} \FunctionTok{awk} \StringTok{'\{print $2","$1\}'}
\FunctionTok{zcat}\NormalTok{ 7.json.gz }\KeywordTok{|} \ExtensionTok{./jq}\NormalTok{ -r }\StringTok{'.borough'} \KeywordTok{|} \FunctionTok{sort} \KeywordTok{|} \FunctionTok{uniq}\NormalTok{ -c }\KeywordTok{|} \FunctionTok{awk} \StringTok{'\{print $2","$1\}'}
\FunctionTok{zcat}\NormalTok{ 8.json.gz }\KeywordTok{|} \ExtensionTok{./jq}\NormalTok{ -r }\StringTok{'.borough'} \KeywordTok{|} \FunctionTok{sort} \KeywordTok{|} \FunctionTok{uniq}\NormalTok{ -c }\KeywordTok{|} \FunctionTok{awk} \StringTok{'\{print $2","$1\}'}
\FunctionTok{zcat}\NormalTok{ 9.json.gz }\KeywordTok{|} \ExtensionTok{./jq}\NormalTok{ -r }\StringTok{'.borough'} \KeywordTok{|} \FunctionTok{sort} \KeywordTok{|} \FunctionTok{uniq}\NormalTok{ -c }\KeywordTok{|} \FunctionTok{awk} \StringTok{'\{print $2","$1\}'}
\end{Highlighting}
\end{Shaded}

This long command breaks down as follows:

\begin{itemize}
\item
  Print the list of files and pipe it into \texttt{parallel}.
\item
  Transmit the \texttt{jq} binary to each remote machine. Lucklily, jq has no dependencies. This file will be removed from the remote machine at the end because we specified \texttt{-\/-trc} (which implies the \texttt{-\/-cleanup} command-line argument).
\item
  The command-line argument \texttt{-\/-trc\ \{.\}.csv} is short for \texttt{-\/-transfer\ -\/-return\ \{.\}.csv\ -\/-cleanup}. (The replacement string \emph{\{.\}} gets replaced with the input filename without the last extension.) Here, this means that the JSON file gets transfered to the remote machine, the CSV file gets returned to the local machine, and both files will be removed after each job from the remote machine.
\item
  Specify a list of hostnames. Remember, if you want to try this out locally, you can specify \texttt{-\/-sshlogin\ :} instead of \texttt{-\/-self\ hostnames}.
\item
  Note the escaping in the \texttt{awk} expression. Quoting can sometimes be tricky. Here, the dollar signs and the double quotes are escaped. In quoting ever gets too confusing, remember that you put the pipeline into a separate command-line tool just as we did with \texttt{sum}.
\end{itemize}

If we, at some point during this command, run \texttt{ls} on one of the remote machines, we could see that \texttt{parallel} indeed transfers (and cleans up) the binary \texttt{jq}, the JSON files, and CSV files:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{ssh} \VariableTok{$(}\FunctionTok{head}\NormalTok{ -n 1 hostnames}\VariableTok{)}\NormalTok{ ls}
\ExtensionTok{1.json.csv}
\ExtensionTok{1.json.gz}
\ExtensionTok{jq}
\end{Highlighting}
\end{Shaded}

Each CSV file looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{cat}\NormalTok{ 1.json.csv}
\ExtensionTok{bronx}\NormalTok{,3}
\ExtensionTok{brooklyn}\NormalTok{,5}
\ExtensionTok{manhattan}\NormalTok{,24}
\ExtensionTok{queens}\NormalTok{,3}
\ExtensionTok{staten_island}\NormalTok{,2}
\ExtensionTok{unspecified}\NormalTok{,63}
\end{Highlighting}
\end{Shaded}

We can sum the counts in each CSV file using Rio and the \texttt{aggregate} function in R:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{cat}\NormalTok{ *.csv }\KeywordTok{|} \ExtensionTok{header}\NormalTok{ -a borough,count }\KeywordTok{|}
\OperatorTok{>} \ExtensionTok{Rio}\NormalTok{ -e }\StringTok{'aggregate(count ~ borough, df, sum)'} \KeywordTok{|}
\OperatorTok{>} \ExtensionTok{csvsort}\NormalTok{ -rc count }\KeywordTok{|} \ExtensionTok{csvlook}
\KeywordTok{|}\ExtensionTok{----------------+--------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{borough}       \KeywordTok{|} \ExtensionTok{count}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{----------------+--------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{unspecified}   \KeywordTok{|} \ExtensionTok{467}    \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{manhattan}     \KeywordTok{|} \ExtensionTok{274}    \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{brooklyn}      \KeywordTok{|} \ExtensionTok{103}    \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{queens}        \KeywordTok{|} \ExtensionTok{77}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{bronx}         \KeywordTok{|} \ExtensionTok{44}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{staten_island} \KeywordTok{|} \ExtensionTok{35}     \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{----------------+--------}\KeywordTok{|}
\end{Highlighting}
\end{Shaded}

Or, if you prefer to use SQL to aggregate results, you can use \texttt{csvsql} as discussed in \protect\hyperlink{chapter-5-scrubbing-data}{Chapter 5}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{cat}\NormalTok{ *.csv }\KeywordTok{|} \ExtensionTok{header}\NormalTok{ -a borough,count }\KeywordTok{|}
\OperatorTok{>} \ExtensionTok{csvsql}\NormalTok{ --query }\StringTok{'SELECT borough, SUM(count) AS count FROM stdin '}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{'GROUP BY borough ORDER BY count DESC'} \KeywordTok{|} \ExtensionTok{csvlook}
\KeywordTok{|}\ExtensionTok{----------------+--------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{borough}       \KeywordTok{|} \ExtensionTok{count}  \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{----------------+--------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{unspecified}   \KeywordTok{|} \ExtensionTok{467}    \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{manhattan}     \KeywordTok{|} \ExtensionTok{274}    \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{brooklyn}      \KeywordTok{|} \ExtensionTok{103}    \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{queens}        \KeywordTok{|} \ExtensionTok{77}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{bronx}         \KeywordTok{|} \ExtensionTok{44}     \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{staten_island} \KeywordTok{|} \ExtensionTok{35}     \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{----------------+--------}\KeywordTok{|}
\end{Highlighting}
\end{Shaded}

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

As data scientists, we work with data, and sometimes a lot of data. This means that sometimes we need to run a command multiple times or distribute data-intensive commands over multiple cores. In this chapter we have shown you how easy it is to parallelize commands. GNU Parallel is a very powerful and flexible tool to speed up ordinary command-line tools and distribute them over multiple cores and remote machines. It offers a lot of functionality and in this chapter we've only been able to scratch the surface. Some features of GNU Parallel are that we haven't covered:

\begin{itemize}
\tightlist
\item
  Different ways of specifying input.
\item
  Keep a log of all the jobs.
\item
  Only start new jobs when the machine is under a certain load.
\item
  Timeout, resume, and retry jobs.
\end{itemize}

Once you have a basic understanding of GNU Parallel and its most important options, we recommend that you take a look at its tutorial listed in the Further Reading section.

\hypertarget{further-reading}{%
\section{Further Reading}\label{further-reading}}

\begin{itemize}
\tightlist
\item
  Tange, O. 2011. ``GNU Parallel - the Command-Line Power Tool.'';Login: The USENIX Magazine 36 (1). Frederiksberg, Denmark:42--47. http://www.gnu.org/s/parallel.
\item
  Tange, Ole. 2014. ``GNU Parallel.'' http://www.gnu.org/software/parallel.
\item
  Services, Amazon Web. 2014. ``AWS Command Line Interface.'' http://aws.amazon.com/cli.
\end{itemize}

\hypertarget{chapter-9-modeling-data}{%
\chapter{Modeling Data}\label{chapter-9-modeling-data}}

In this chapter we're going to perform the fourth and last step of the OSEMN model that we can do on a computer: modeling data. Generally speaking, to model data is to create an abstract or higher-level description of your data. Just like with creating visualizations, it's like taking a step back from the individual data points.

However, visualizations, on the one hand, are characterized by shapes, positions, and colors such that we can interpret them by looking at them. Models, on the other hand, are internally characterized by a bunch of numbers, which means that computers can use them, for example, to make predictions about a new data points. (We can still visualize models so that we can try to understand them and see how they are performing.)

In this chapter we'll consider four common types of algorithms to model data:

\begin{itemize}
\tightlist
\item
  Dimensionality reduction.
\item
  Clustering.
\item
  Regression.
\item
  Classification.
\end{itemize}

These four algorithms come from the field of machine learning. As such, we're going to change our vocabulary a bit. Let's assume that we have a CSV file, also known as a \emph{data set}. Each row, except for the header, is considered to be a \emph{data point}. For simplicity we assume that each column that contains numerical values is an input \emph{feature}. If a data point also contains a non-numerical field, such as the \emph{species} column in the Iris data set, then that is known as the data point's \emph{label}.

The first two types of algorithms (dimensionality reduction and clustering) are most often unsupervised, which means that they create a model based on the features of the data set only. The last two types of algorithms (regression and classification) are by definition supervised algorithms, which means that they also incorporate the labels into the model.

\begin{rmdcaution}
This is by no means an introduction to machine learning. That implies that we must skim over many details. We strongly advise that you become familiar with an algorithm before applying it blindly to your data.
\end{rmdcaution}

\hypertarget{overview}{%
\section{Overview}\label{overview}}

In this chapter, you'll learn how to:

\begin{itemize}
\tightlist
\item
  Reduce the dimensionality of your data set.
\item
  Identify groups of data points with three clustering algorithms.
\item
  Predict the quality of white wine using regression.
\item
  Classify wine as red or white via a prediction API.
\end{itemize}

\hypertarget{more-wine-please}{%
\section{More Wine Please!}\label{more-wine-please}}

In this chapter, we'll be using a data set of wine tastings. Specifically, red and white Portuguese ``Vinho Verde'' wine. Each data point represents a wine, and consists of 11 physicochemical properties: (1) fixed acidity, (2) volatile acidity, (3) citric acid, (4) residual sugar, (5) chlorides, (6) free sulfur dioxide, (7) total sulfur dioxide, (8) density, (9) pH, (10) sulphates, and (11) alcohol. There is also a quality score. This score lies between 0 (very bad) and 10 (excellent) and is the median of at least three evaluation by wine experts. More information about this data set is available at \url{http://archive.ics.uci.edu/ml/datasets/Wine+Quality}.

There are two data sets: one for white wine and one for red wine. The very first step is to obtain the two data sets using \texttt{curl} (and of course \texttt{parallel} because we haven't got all day):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{cd}\NormalTok{ ~/book/ch09}
\NormalTok{$ }\ExtensionTok{parallel} \StringTok{"curl -sL http://archive.ics.uci.edu/ml/machine-learning-databases"}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{"/wine-quality/winequality-\{\}.csv > data/wine-\{\}.csv"}\NormalTok{ ::: red white}
\end{Highlighting}
\end{Shaded}

The triple colon is yet another way we can pass data to \texttt{parallel}. Let's inspect both data sets using \texttt{head} and count the number of rows using \texttt{wc\ -l}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{head}\NormalTok{ -n 5 wine-}\DataTypeTok{\{red,white\}}\NormalTok{.csv }\KeywordTok{|} \ExtensionTok{fold}
\NormalTok{==}\OperatorTok{>} \ExtensionTok{wine-red.csv} \OperatorTok{<}\NormalTok{==}
\StringTok{"fixed acidity"}\NormalTok{;}\StringTok{"volatile acidity"}\NormalTok{;}\StringTok{"citric acid"}\NormalTok{;}\StringTok{"residual sugar"}\NormalTok{;}\StringTok{"chlorides"}\NormalTok{;}\StringTok{"f}
\StringTok{ree sulfur dioxide"}\NormalTok{;}\StringTok{"total sulfur dioxide"}\NormalTok{;}\StringTok{"density"}\NormalTok{;}\StringTok{"pH"}\NormalTok{;}\StringTok{"sulphates"}\NormalTok{;}\StringTok{"alcohol"}\NormalTok{;}
\StringTok{"quality"}
\ExtensionTok{7.4}\KeywordTok{;}\ExtensionTok{0.7}\KeywordTok{;}\ExtensionTok{0}\KeywordTok{;}\ExtensionTok{1.9}\KeywordTok{;}\ExtensionTok{0.076}\KeywordTok{;}\ExtensionTok{11}\KeywordTok{;}\ExtensionTok{34}\KeywordTok{;}\ExtensionTok{0.9978}\KeywordTok{;}\ExtensionTok{3.51}\KeywordTok{;}\ExtensionTok{0.56}\KeywordTok{;}\ExtensionTok{9.4}\KeywordTok{;}\ExtensionTok{5}
\ExtensionTok{7.8}\KeywordTok{;}\ExtensionTok{0.88}\KeywordTok{;}\ExtensionTok{0}\KeywordTok{;}\ExtensionTok{2.6}\KeywordTok{;}\ExtensionTok{0.098}\KeywordTok{;}\ExtensionTok{25}\KeywordTok{;}\ExtensionTok{67}\KeywordTok{;}\ExtensionTok{0.9968}\KeywordTok{;}\ExtensionTok{3.2}\KeywordTok{;}\ExtensionTok{0.68}\KeywordTok{;}\ExtensionTok{9.8}\KeywordTok{;}\ExtensionTok{5}
\ExtensionTok{7.8}\KeywordTok{;}\ExtensionTok{0.76}\KeywordTok{;}\ExtensionTok{0.04}\KeywordTok{;}\ExtensionTok{2.3}\KeywordTok{;}\ExtensionTok{0.092}\KeywordTok{;}\ExtensionTok{15}\KeywordTok{;}\ExtensionTok{54}\KeywordTok{;}\ExtensionTok{0.997}\KeywordTok{;}\ExtensionTok{3.26}\KeywordTok{;}\ExtensionTok{0.65}\KeywordTok{;}\ExtensionTok{9.8}\KeywordTok{;}\ExtensionTok{5}
\ExtensionTok{11.2}\KeywordTok{;}\ExtensionTok{0.28}\KeywordTok{;}\ExtensionTok{0.56}\KeywordTok{;}\ExtensionTok{1.9}\KeywordTok{;}\ExtensionTok{0.075}\KeywordTok{;}\ExtensionTok{17}\KeywordTok{;}\ExtensionTok{60}\KeywordTok{;}\ExtensionTok{0.998}\KeywordTok{;}\ExtensionTok{3.16}\KeywordTok{;}\ExtensionTok{0.58}\KeywordTok{;}\ExtensionTok{9.8}\KeywordTok{;}\ExtensionTok{6}

\NormalTok{==}\OperatorTok{>} \ExtensionTok{wine-white.csv} \OperatorTok{<}\NormalTok{==}
\StringTok{"fixed acidity"}\NormalTok{;}\StringTok{"volatile acidity"}\NormalTok{;}\StringTok{"citric acid"}\NormalTok{;}\StringTok{"residual sugar"}\NormalTok{;}\StringTok{"chlorides"}\NormalTok{;}\StringTok{"f}
\StringTok{ree sulfur dioxide"}\NormalTok{;}\StringTok{"total sulfur dioxide"}\NormalTok{;}\StringTok{"density"}\NormalTok{;}\StringTok{"pH"}\NormalTok{;}\StringTok{"sulphates"}\NormalTok{;}\StringTok{"alcohol"}\NormalTok{;}
\StringTok{"quality"}
\ExtensionTok{7}\KeywordTok{;}\ExtensionTok{0.27}\KeywordTok{;}\ExtensionTok{0.36}\KeywordTok{;}\ExtensionTok{20.7}\KeywordTok{;}\ExtensionTok{0.045}\KeywordTok{;}\ExtensionTok{45}\KeywordTok{;}\ExtensionTok{170}\KeywordTok{;}\ExtensionTok{1.001}\KeywordTok{;}\ExtensionTok{3}\KeywordTok{;}\ExtensionTok{0.45}\KeywordTok{;}\ExtensionTok{8.8}\KeywordTok{;}\ExtensionTok{6}
\ExtensionTok{6.3}\KeywordTok{;}\ExtensionTok{0.3}\KeywordTok{;}\ExtensionTok{0.34}\KeywordTok{;}\ExtensionTok{1.6}\KeywordTok{;}\ExtensionTok{0.049}\KeywordTok{;}\ExtensionTok{14}\KeywordTok{;}\ExtensionTok{132}\KeywordTok{;}\ExtensionTok{0.994}\KeywordTok{;}\ExtensionTok{3.3}\KeywordTok{;}\ExtensionTok{0.49}\KeywordTok{;}\ExtensionTok{9.5}\KeywordTok{;}\ExtensionTok{6}
\ExtensionTok{8.1}\KeywordTok{;}\ExtensionTok{0.28}\KeywordTok{;}\ExtensionTok{0.4}\KeywordTok{;}\ExtensionTok{6.9}\KeywordTok{;}\ExtensionTok{0.05}\KeywordTok{;}\ExtensionTok{30}\KeywordTok{;}\ExtensionTok{97}\KeywordTok{;}\ExtensionTok{0.9951}\KeywordTok{;}\ExtensionTok{3.26}\KeywordTok{;}\ExtensionTok{0.44}\KeywordTok{;}\ExtensionTok{10.1}\KeywordTok{;}\ExtensionTok{6}
\ExtensionTok{7.2}\KeywordTok{;}\ExtensionTok{0.23}\KeywordTok{;}\ExtensionTok{0.32}\KeywordTok{;}\ExtensionTok{8.5}\KeywordTok{;}\ExtensionTok{0.058}\KeywordTok{;}\ExtensionTok{47}\KeywordTok{;}\ExtensionTok{186}\KeywordTok{;}\ExtensionTok{0.9956}\KeywordTok{;}\ExtensionTok{3.19}\KeywordTok{;}\ExtensionTok{0.4}\KeywordTok{;}\ExtensionTok{9.9}\KeywordTok{;}\ExtensionTok{6}
\NormalTok{$ }\FunctionTok{wc}\NormalTok{ -l wine-}\DataTypeTok{\{red,white\}}\NormalTok{.csv}
  \ExtensionTok{1600}\NormalTok{ wine-red.csv}
  \ExtensionTok{4899}\NormalTok{ wine-white.csv}
  \ExtensionTok{6499}\NormalTok{ total}
\end{Highlighting}
\end{Shaded}

At first sight this data appears to be very clean already. Still, let's scrub this data a little bit so that it conforms more with what most command-line tools are expecting. Specifically, we'll:

\begin{itemize}
\tightlist
\item
  Convert the header to lowercase.
\item
  Convert the semi-colons to commas.
\item
  Convert spaces to underscores.
\item
  Remove unnecessary quotes.
\end{itemize}

These things can all be taken care of by `tr`. Let's use a for loop this time---for old times' sake---to process both data sets:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{for} \ExtensionTok{T}\NormalTok{ in red white}\KeywordTok{;} \KeywordTok{do}
\OperatorTok{<} \ExtensionTok{wine-}\VariableTok{$T}\NormalTok{.csv tr }\StringTok{'[A-Z]; '} \StringTok{'[a-z],_'} \KeywordTok{|} \FunctionTok{tr}\NormalTok{ -d }\DataTypeTok{\textbackslash{}"} \OperatorTok{>}\NormalTok{ wine-}\VariableTok{$\{T\}}\NormalTok{-clean.csv}
\KeywordTok{done}
\end{Highlighting}
\end{Shaded}

Let's also create a data set by combining the two data sets. We'll use \texttt{csvstack} to add a column named ``type'' which will be ``red'' for rows of the first file, and ``white'' for rows of the second file:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\VariableTok{HEADER=}\StringTok{"}\VariableTok{$(}\FunctionTok{head}\NormalTok{ -n 1 wine-red-clean.csv}\VariableTok{)}\StringTok{,type"}
\NormalTok{$ }\ExtensionTok{csvstack}\NormalTok{ -g red,white -n type wine-}\DataTypeTok{\{red,white\}}\NormalTok{-clean.csv }\KeywordTok{|}
\OperatorTok{>} \ExtensionTok{csvcut}\NormalTok{ -c }\VariableTok{$HEADER} \OperatorTok{>}\NormalTok{ wine-both-clean.csv}
\end{Highlighting}
\end{Shaded}

The new column \emph{type} is added to the beginning of the table. Because some of the command-line tools that we'll use in this chapter assume that the class label is the last column, we'll rearrange the columns by using \texttt{csvcut}. Instead of typing all 13 columns, we temporary store the desired header into a variable \emph{\$HEADER} before we call \texttt{csvstack}.

It's good to check whether there are any missing values in this data set:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{csvstat}\NormalTok{ wine-both-clean.csv --nulls}
  \ExtensionTok{1.}\NormalTok{ fixed_acidity: False}
  \ExtensionTok{2.}\NormalTok{ volatile_acidity: False}
  \ExtensionTok{3.}\NormalTok{ citric_acid: False}
  \ExtensionTok{4.}\NormalTok{ residual_sugar: False}
  \ExtensionTok{5.}\NormalTok{ chlorides: False}
  \ExtensionTok{6.}\NormalTok{ free_sulfur_dioxide: False}
  \ExtensionTok{7.}\NormalTok{ total_sulfur_dioxide: False}
  \ExtensionTok{8.}\NormalTok{ density: False}
  \ExtensionTok{9.}\NormalTok{ ph: False}
 \ExtensionTok{10.}\NormalTok{ sulphates: False}
 \ExtensionTok{11.}\NormalTok{ alcohol: False}
 \ExtensionTok{12.}\NormalTok{ quality: False}
 \ExtensionTok{13.}\NormalTok{ type: False}
\end{Highlighting}
\end{Shaded}

Excellent! Just out of curiosity, let's see what the how the distribution of quality looks like for both red and white wines.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{wine-both-clean.csv}\NormalTok{ Rio -ge }\StringTok{'g+geom_density(aes(quality, '}\NormalTok{\textbackslash{}}
\StringTok{'fill=type), adjust=3, alpha=0.5)'} \KeywordTok{|} \ExtensionTok{display}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=32.81in]{images/ch09-wine-quality-density} 

}

\end{figure}

From the density plot we can see the quality of white wine is distributed more towards higher values. Does this mean that white wines are overall better than red wines, or that the white wine experts more easily give higher scores than red wine experts? That's something that the data doesn't tell us. Or is there perhaps a correlation between alcohol and quality? Let's use Rio and ggplot again to find out:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{wine-both-clean.csv}\NormalTok{ Rio -ge }\StringTok{'ggplot(df, aes(x=alcohol, y=quality, '}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{'color=type)) + geom_point(position="jitter", alpha=0.2) + '}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{'geom_smooth(method="lm")'} \KeywordTok{|} \ExtensionTok{display}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=32.81in]{images/ch09-wine-alcohol-vs-quality} 

}

\end{figure}

Eureka! Ahem, let's carry on with some modeling, shall we?

\hypertarget{dimensionality-reduction-with-tapkee}{%
\section{Dimensionality Reduction with Tapkee}\label{dimensionality-reduction-with-tapkee}}

The goal of dimensionality reduction is to map high-dimensional data points onto a lower dimensional mapping. The challenge is to keep similar data points close together on the lower-dimensional mapping. As we've seen in the previous section, our wine data set contained 13 features. We'll stick with two dimensions because that's straight forward to visualize.

Dimensionality reduction is often regarded as being part of exploring step. It's useful for when there are too many features for plotting. You could do a scatter-plot matrix, but that only shows you two features at a time. It's also useful as a pre-processing step for other machine learning algorithms.

Most dimensionality reduction algorithms are unsupervised. This means that they don't employ the labels of the data points in order to construct the lower-dimensional mapping.

In this section we'll look at two techniques: PCA, which stands for Principal Components Analysis \citep{Pearson1901} and t-SNE, which stands for t-distributed Stochastic Neighbor Embedding \citep{van2008visualizing}.

\hypertarget{introducing-tapkee}{%
\subsection{Introducing Tapkee}\label{introducing-tapkee}}

Tapkee is a C++ template library for dimensionality reduction \citep{Lisitsyn2013}. The library contains implementations of many dimensionality reduction algorithms, including:

\begin{itemize}
\tightlist
\item
  Locally Linear Embedding
\item
  Isomap
\item
  Multidimensional scaling
\item
  PCA
\item
  t-SNE
\end{itemize}

Tapkee's website: \url{http://tapkee.lisitsyn.me/}, contains more information about these algorithms. Although Tapkee is mainly a library that can be included in other applications, it also offers a command-line tool. We'll use this to perform dimensionality reduction on our wine data set.

\hypertarget{installing-tapkee}{%
\subsection{Installing Tapkee}\label{installing-tapkee}}

If you aren't running the Data Science Toolbox, you'll need to download and compile Tapkee yourself. First make sure that you have \texttt{CMake} installed. On Ubuntu, you simply run:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{sudo}\NormalTok{ apt-get install cmake}
\end{Highlighting}
\end{Shaded}

Please consult Tapkee's website for instructions for other operating systems. Then execute the following commands to download the source and compile it:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{curl}\NormalTok{ -sL https://github.com/lisitsyn/tapkee/archive/master.tar.gz }\OperatorTok{>}\NormalTok{ \textbackslash{}}
\OperatorTok{>}\NormalTok{ tapkee-master.tar.gz}
\NormalTok{$ }\FunctionTok{tar}\NormalTok{ -xzf tapkee-master.tar.gz}
\NormalTok{$ }\BuiltInTok{cd}\NormalTok{ tapkee-master}
\NormalTok{$ }\FunctionTok{mkdir}\NormalTok{ build }\KeywordTok{&&} \BuiltInTok{cd}\NormalTok{ build}
\NormalTok{$ }\FunctionTok{cmake}\NormalTok{ ..}
\NormalTok{$ }\FunctionTok{make}
\end{Highlighting}
\end{Shaded}

This creates a binary executable named \texttt{tapkee}.

\hypertarget{linear-and-non-linear-mappings}{%
\subsection{Linear and Non-linear Mappings}\label{linear-and-non-linear-mappings}}

First, we'll scale the features using standardization such that each feature is equally important. This generally leads to better results when applying machine learning algorithms.

To scale we use a combination of \texttt{cols} and \texttt{Rio}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{wine-both.csv}\NormalTok{ cols -C type Rio -f scale }\OperatorTok{>}\NormalTok{ wine-both-scaled.csv}
\end{Highlighting}
\end{Shaded}

Now we apply both dimensionality reduction techniques and visualize the mapping using \texttt{Rio-scatter}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{wine-both-scaled.csv}\NormalTok{ cols -C type body tapkee --method pca }\KeywordTok{|}
\OperatorTok{>} \ExtensionTok{header}\NormalTok{ -r x,y,type }\KeywordTok{|} \ExtensionTok{Rio-scatter}\NormalTok{ x y type }\KeywordTok{|}
\OperatorTok{>} \FunctionTok{tee}\NormalTok{ tapkee-wine-pca.png }\KeywordTok{|} \ExtensionTok{display}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=7in]{images/tapkee-wine-pca} 

}

\caption{PCA}\label{fig:unnamed-chunk-15}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{wine-both-scaled.csv}\NormalTok{ cols -C type body tapkee --method t-sne }\KeywordTok{|}
\OperatorTok{>} \ExtensionTok{header}\NormalTok{ -r x,y,type }\KeywordTok{|} \ExtensionTok{Rio-scatter}\NormalTok{ x y type }\KeywordTok{|}
\OperatorTok{>} \FunctionTok{tee}\NormalTok{ tapkee-wine-t-sne.png }\KeywordTok{|} \ExtensionTok{display}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=7in]{images/tapkee-wine-t-sne} 

}

\caption{t-SNE}\label{fig:unnamed-chunk-17}
\end{figure}

Note that there's not a single GNU core util (i.e., classic command-line tool) in this one-liner. Now that's the power of the command line!

\hypertarget{clustering-with-weka}{%
\section{Clustering with Weka}\label{clustering-with-weka}}

In this section we'll be clustering our wine data set into groups. Like, dimensionality reduction, clustering is usually unsupervised. It can be used go gain an understanding of how your data is structured. Once the data has been clustered, you can visualize the result by coloring the data points according to their cluster assignment. For most algorithms you specify upfront how many groups you want the data to be clustered in; some algorithms are able to determine a suitable number of groups.

For this task we'll use Weka, which is being maintained by the Machine Learning Group at the University of Waikato \citep{Hall2009}. If you already know Weka, then you probably know it as a software with a graphical user interface. However, as you'll see, Weka can also be used from the command line (albeit with some modifications). Besides clustering, Weka can also do classification and regression, but we're going to be using other tools for those machine learning tasks.

\hypertarget{introducing-weka}{%
\subsection{Introducing Weka}\label{introducing-weka}}

You may ask, surely there are better command-line tools for clustering? And you are right. One reason we include Weka in this chapter is to show you how you can work around these imperfections by building additional command-line tools. As you spend more time on the command line and try out other command-line tools, chances are that you come across one that seems very promising at first, but does not work as you expected. A common imperfection is the command-line tool does not handle standard in or standard out correctly. In the next section we'll point out these imperfections and demonstrate how we work around them.

\hypertarget{taming-weka-on-the-command-line}{%
\subsection{Taming Weka on the Command Line}\label{taming-weka-on-the-command-line}}

Weka can be invoked from the command line, but it's definitely not straightforward or user friendly. Weka is programmed in Java, which means that you have to run \texttt{java}, specify the location of the \emph{weka.jar} file, and specify the individual class you want to call. For example, Weka has a class called \emph{MexicanHat}, which generates a toy data set. To generate 10 data points using this class, you would run:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{java}\NormalTok{ -cp ~/bin/weka.jar weka.datagenerators.classifiers.regression.MexicanHat\textbackslash{}}
\OperatorTok{>}\NormalTok{  -n 10 }\KeywordTok{|} \ExtensionTok{fold}
\ExtensionTok{%}
\ExtensionTok{%}\NormalTok{ Commandline}
\ExtensionTok{%}
\ExtensionTok{%}\NormalTok{ weka.datagenerators.classifiers.regression.MexicanHat -r weka.datagenerators.c}
\ExtensionTok{lassifiers.regression.MexicanHat-S_1_-n_10_-A_1.0_-R_-10..10_-N_0.0_-V_1.0}\NormalTok{ -S 1}
\ExtensionTok{-n}\NormalTok{ 10 -A 1.0 -R -10..10 -N 0.0 -V 1.0}
\ExtensionTok{%}
\ExtensionTok{@relation}\NormalTok{ weka.datagenerators.classifiers.regression.MexicanHat-S_1_-n_10_-A_1.0}
\ExtensionTok{_-R_-10..10_-N_0.0_-V_1.0}

\ExtensionTok{@attribute}\NormalTok{ x numeric}
\ExtensionTok{@attribute}\NormalTok{ y numeric}

\ExtensionTok{@data}

\ExtensionTok{4.617564}\NormalTok{,-0.215591}
\ExtensionTok{-1.798384}\NormalTok{,0.541716}
\ExtensionTok{-5.845703}\NormalTok{,-0.072474}
\ExtensionTok{-3.345659}\NormalTok{,-0.060572}
\ExtensionTok{9.355118}\NormalTok{,0.00744}
\ExtensionTok{-9.877656}\NormalTok{,-0.044298}
\ExtensionTok{9.274096}\NormalTok{,0.016186}
\ExtensionTok{8.797308}\NormalTok{,0.066736}
\ExtensionTok{8.943898}\NormalTok{,0.051718}
\ExtensionTok{8.741643}\NormalTok{,0.072209}
\end{Highlighting}
\end{Shaded}

Don't worry about the output of this command, we'll discuss that later. At this moment, we're concerned with the usage of Weka. There are a couple of things to note here:

\begin{itemize}
\tightlist
\item
  You need run \texttt{java}, which is counter-intuitive.
\item
  The jar file contains over 2000 classes, and only about 300 of those can be used from the command line directly. How do you know which ones?
\item
  You need to specify entire namespace of the class: \texttt{weka.datagenerators.classifiers.regression.MexicanHat}. How are you supposed to remember that?
\end{itemize}

Does this mean that we're going to give up on Weka? Of course not! Since Weka does contain a lot of useful functionality, we're going to tackle these issues in the next three subsections.

\hypertarget{an-improved-command-line-tool-for-weka}{%
\subsubsection{An Improved Command-line Tool for Weka}\label{an-improved-command-line-tool-for-weka}}

Save the following snippet as a new file called \texttt{weka} and put it somewhere on your \emph{PATH}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#!/usr/bin/env bash}
\ExtensionTok{java}\NormalTok{ -Xmx1024M -cp }\VariableTok{$\{WEKAPATH\}}\NormalTok{/weka.jar }\StringTok{"weka.}\VariableTok{$@}\StringTok{"}
\end{Highlighting}
\end{Shaded}

Subsequently, add the following line to your \emph{.bashrc} file so that \texttt{weka} can be called from anywhere:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{export} \VariableTok{WEKAPATH=}\NormalTok{/home/vagrant/repos/weka}
\end{Highlighting}
\end{Shaded}

We can now call the previous example with:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{weka}\NormalTok{ datagenerators.classifiers.regression.MexicanHat -n 10}
\end{Highlighting}
\end{Shaded}

\hypertarget{usable-weka-classes}{%
\subsubsection{Usable Weka Classes}\label{usable-weka-classes}}

As mentioned, the file \emph{weka.jar} contains over 2000 classes. Many of them cannot be used from the command line directly. We consider a class usable from the command line when it provides us with a help message if we invoke it with \texttt{-h}. For example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{weka}\NormalTok{ datagenerators.classifiers.regression.MexicanHat -h}

\ExtensionTok{Data}\NormalTok{ Generator options:}

\ExtensionTok{-h}
        \ExtensionTok{Prints}\NormalTok{ this help.}
\ExtensionTok{-o} \OperatorTok{<}\NormalTok{file}\OperatorTok{>}
        \ExtensionTok{The}\NormalTok{ name of the output file, otherwise the generated data is}
        \ExtensionTok{printed}\NormalTok{ to stdout.}
\ExtensionTok{-r} \OperatorTok{<}\NormalTok{name}\OperatorTok{>}
        \ExtensionTok{The}\NormalTok{ name of the relation.}
\ExtensionTok{-d}
        \ExtensionTok{Whether}\NormalTok{ to print debug informations.}
\ExtensionTok{-S}
        \ExtensionTok{The}\NormalTok{ seed for random function (default 1)}
\ExtensionTok{-n} \OperatorTok{<}\NormalTok{num}\OperatorTok{>}
        \ExtensionTok{The}\NormalTok{ number of examples to generate (default 100)}
\ExtensionTok{-A} \OperatorTok{<}\NormalTok{num}\OperatorTok{>}
        \ExtensionTok{The}\NormalTok{ amplitude multiplier (default 1.0)}\ExtensionTok{.}
\ExtensionTok{-R} \OperatorTok{<}\NormalTok{num}\OperatorTok{>}\NormalTok{..}\OperatorTok{<}\NormalTok{num}\OperatorTok{>}
        \ExtensionTok{The}\NormalTok{ range x is randomly drawn from (default -10.0..10.0)}\ExtensionTok{.}
\ExtensionTok{-N} \OperatorTok{<}\NormalTok{num}\OperatorTok{>}
        \ExtensionTok{The}\NormalTok{ noise rate (default 0.0)}\ExtensionTok{.}
\ExtensionTok{-V} \OperatorTok{<}\NormalTok{num}\OperatorTok{>}
        \ExtensionTok{The}\NormalTok{ noise variance (default 1.0)}\ExtensionTok{.}
\end{Highlighting}
\end{Shaded}

Now that's usable. This, for example, is not a usable class:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{weka}\NormalTok{ filters.SimpleFilter -h}
\ExtensionTok{java.lang.ClassNotFoundException}\NormalTok{: -h}
        \ExtensionTok{at}\NormalTok{ java.net.URLClassLoader}\VariableTok{$1}\NormalTok{.run(URLClassLoader.java:202)}
        \ExtensionTok{at}\NormalTok{ java.security.AccessController.doPrivileged(Native Method)}
        \ExtensionTok{at}\NormalTok{ java.net.URLClassLoader.findClass(URLClassLoader.java:190)}
        \ExtensionTok{at}\NormalTok{ java.lang.ClassLoader.loadClass(ClassLoader.java:306)}
        \ExtensionTok{at}\NormalTok{ sun.misc.Launcher}\VariableTok{$AppClassLoader}\NormalTok{.loadClass(Launcher.java:301)}
        \ExtensionTok{at}\NormalTok{ java.lang.ClassLoader.loadClass(ClassLoader.java:247)}
        \ExtensionTok{at}\NormalTok{ java.lang.Class.forName0(Native Method)}
        \ExtensionTok{at}\NormalTok{ java.lang.Class.forName(Class.java:171)}
        \ExtensionTok{at}\NormalTok{ weka.filters.Filter.main(Filter.java:1344)}
\ExtensionTok{-h}
\end{Highlighting}
\end{Shaded}

The following pipeline runs \texttt{weka} with every class in \emph{weka.jar} and \texttt{-h} and saves the standard output and standard error to a file with the same name as the class:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{unzip}\NormalTok{ -l }\VariableTok{$WEKAPATH}\NormalTok{/weka.jar }\KeywordTok{|}
\OperatorTok{>} \FunctionTok{sed}\NormalTok{ -rne }\StringTok{'s/.*(weka)\textbackslash{}/([^g])([^$]*)\textbackslash{}.class$/\textbackslash{}2\textbackslash{}3/p'} \KeywordTok{|}
\OperatorTok{>} \FunctionTok{tr} \StringTok{'/'} \StringTok{'.'} \KeywordTok{|}
\OperatorTok{>} \ExtensionTok{parallel}\NormalTok{ --timeout 1 -j4 -v }\StringTok{"weka \{\} -h > \{\} 2>&1"}
\end{Highlighting}
\end{Shaded}

We now have 749 files. With the following command we save the filename of every files which does not contain the string \emph{Exception} to \emph{weka.classes}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{grep}\NormalTok{ -L }\StringTok{'Exception'}\NormalTok{ * }\KeywordTok{|} \FunctionTok{tee} \VariableTok{$WEKAPATH}\NormalTok{/weka.classes}
\end{Highlighting}
\end{Shaded}

This still comes down to 332 classes! Here are a few classes that might be of interest):

\begin{itemize}
\item
  \texttt{attributeSelection.PrincipalComponents}
\item
  \texttt{classifiers.bayes.NaiveBayes}
\item
  \texttt{classifiers.evaluation.ConfusionMatrix}
\item
  \texttt{classifiers.functions.SimpleLinearRegression}
\item
  \texttt{classifiers.meta.AdaBoostM1}
\item
  \texttt{classifiers.trees.RandomForest}
\item
  \texttt{clusterers.EM}
\item
  \texttt{filters.unsupervised.attribute.Normalize}
\end{itemize}

As you can see, \texttt{weka} offers a whole range of classes and functionality.

\hypertarget{adding-tab-completion}{%
\subsubsection{Adding Tab Completion}\label{adding-tab-completion}}

At this moment, you still need to type in the entire class name yourself. You can add so-called tab completion by adding the following snippet to your \emph{.bashrc} file after you export \emph{WEKAPATH}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{_completeweka()} \KeywordTok{\{}
  \BuiltInTok{local} \VariableTok{curw=$\{COMP_WORDS[COMP_CWORD]\}}
  \BuiltInTok{local} \VariableTok{wordlist=$(}\FunctionTok{cat} \VariableTok{$WEKAPATH}\NormalTok{/weka.classes}\VariableTok{)}
  \VariableTok{COMPREPLY=($(}\BuiltInTok{compgen}\NormalTok{ -W }\StringTok{'$\{wordlist[@]\}'}\NormalTok{ -- }\StringTok{"}\VariableTok{$curw}\StringTok{"}\VariableTok{))}
  \BuiltInTok{return}\NormalTok{ 0}
\KeywordTok{\}}
\BuiltInTok{complete}\NormalTok{ -o nospace -F _completeweka weka}
\end{Highlighting}
\end{Shaded}

This function makes use of the \emph{weka.classes} file we generated earlier. If you now type: \texttt{weka\ clu\textless{}Tab\textgreater{}\textless{}Tab\textgreater{}\textless{}Tab\textgreater{}} on the command line, you are presented with a list of all classes that have to do with clustering:

\begin{verbatim}
$ weka clusterers.
clusterers.CheckClusterer
clusterers.CLOPE
clusterers.ClusterEvaluation
clusterers.Cobweb
clusterers.DBSCAN
clusterers.EM
clusterers.FarthestFirst
clusterers.FilteredClusterer
clusterers.forOPTICSAndDBScan.OPTICS_GUI.OPTICS_Visualizer
clusterers.HierarchicalClusterer
clusterers.MakeDensityBasedClusterer
clusterers.OPTICS
clusterers.sIB
clusterers.SimpleKMeans
clusterers.XMeans
\end{verbatim}

Creating a command-line tool \texttt{weka} and adding tab completion makes sure that Weka is a little bit more friendly to use on the command line.

\hypertarget{converting-between-csv-to-arff-data-formats}{%
\subsection{Converting between CSV to ARFF Data Formats}\label{converting-between-csv-to-arff-data-formats}}

Weka uses ARFF as a file format. This is basically CSV with additional information about the columns. We'll use two convenient command-line tools to convert between CSV and ARFF, namely \texttt{csv2arff} (see Example \ref{exm:csv2arff} ) and \texttt{arff2csv} (see Example \ref{exm:arff2csv}).

\begin{example}[Convert CSV to ARFF]
\protect\hypertarget{exm:csv2arff}{}{\label{exm:csv2arff} \iffalse (Convert CSV to ARFF) \fi{} }
\end{example}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#!/usr/bin/env bash}
\ExtensionTok{weka}\NormalTok{ core.converters.CSVLoader /dev/stdin}
\end{Highlighting}
\end{Shaded}

\begin{example}[Convert ARFF to CSV]
\protect\hypertarget{exm:arff2csv}{}{\label{exm:arff2csv} \iffalse (Convert ARFF to CSV) \fi{} }
\end{example}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#!/usr/bin/env bash}
\ExtensionTok{weka}\NormalTok{ core.converters.CSVSaver -i /dev/stdin}
\end{Highlighting}
\end{Shaded}

\hypertarget{comparing-three-cluster-algorithms}{%
\subsection{Comparing Three Cluster Algorithms}\label{comparing-three-cluster-algorithms}}

Unfortunately, in order to cluster data using Weka, we need yet another command-line tool to help us with this. The \emph{AddCluster} class is needed to assign data points to the learned clusters. Unfortunately, this class does not accept data from standard input, not even when we specify \emph{-i /dev/stdin} because it expects a file with the \emph{.arff} extension. We consider this to be bad design. The source code of \texttt{weka-cluster} is:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#!/usr/bin/env bash}
\VariableTok{ALGO=}\StringTok{"}\VariableTok{$@}\StringTok{"}
\VariableTok{IN=$(}\FunctionTok{mktemp}\NormalTok{ --tmpdir weka-cluster-XXXXXXXX}\VariableTok{)}\NormalTok{.arff}

\FunctionTok{finish ()} \KeywordTok{\{}
        \FunctionTok{rm}\NormalTok{ -f }\VariableTok{$IN}
\KeywordTok{\}}
\BuiltInTok{trap}\NormalTok{ finish EXIT}

\ExtensionTok{csv2arff} \OperatorTok{>} \VariableTok{$IN}
\ExtensionTok{weka}\NormalTok{ filters.unsupervised.attribute.AddCluster -W }\StringTok{"weka.}\VariableTok{$\{ALGO\}}\StringTok{"}\NormalTok{ -i }\VariableTok{$IN}\NormalTok{ \textbackslash{}}
\NormalTok{-o /dev/stdout }\KeywordTok{|} \ExtensionTok{arff2csv}
\end{Highlighting}
\end{Shaded}

Now we can apply the EM clustering algorithm and save the assignment as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{cd}\NormalTok{ data}
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{wine-both-scaled.csv}\NormalTok{ csvcut -C quality,type }\KeywordTok{|}          
\OperatorTok{>} \ExtensionTok{weka-cluster}\NormalTok{ clusterers.EM -N 5 }\KeywordTok{|}                        
\OperatorTok{>} \ExtensionTok{csvcut}\NormalTok{ -c cluster }\OperatorTok{>}\NormalTok{ data/wine-both-cluster-em.csv        }
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Use the scaled features, and don't use the features quality and type for the cluster.
\item
  Apply the algorithm using \texttt{weka-cluster}.
\item
  Only save the cluster assignment.
\end{itemize}

We'll run the same command again for \emph{SimpleKMeans} and \emph{Cobweb} algorithms. Now we have three files with cluster assignments. Let's create a t-SNE mapping in order to visualize the cluster assignments:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{wine-both-scaled.csv}\NormalTok{ csvcut -C quality,type }\KeywordTok{|} \ExtensionTok{body}\NormalTok{ tapkee --method t-sne }\KeywordTok{|}
\OperatorTok{>} \ExtensionTok{header}\NormalTok{ -r x,y }\OperatorTok{>}\NormalTok{ wine-both-xy.csv}
\end{Highlighting}
\end{Shaded}

Next, the cluster assignments are combined with the t-SNE mapping using \texttt{paste} and a scatter plot is created using \texttt{Rio-scatter}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{parallel}\NormalTok{ -j1 }\StringTok{"paste -d, wine-both-xy.csv wine-both-cluster-\{\}.csv | "}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{"Rio-scatter x y cluster | display"}\NormalTok{ ::: em simplekmeans cobweb}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=32.81in]{images/ch09-wine-cluster-em} 

}

\caption{EM}\label{fig:unnamed-chunk-33}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=32.81in]{images/ch09-wine-cluster-simplekmeans} 

}

\caption{SimpleKMeans}\label{fig:unnamed-chunk-34}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=32.81in]{images/ch09-wine-cluster-cobweb} 

}

\caption{Cobweb}\label{fig:unnamed-chunk-35}
\end{figure}

Admittedly, we have through a lot of trouble taming Weka. The exercise was worth it, because some day you may run into a command-line tool that works different from what you expect. Now you know that there are always ways to work around such command-line tools.

\hypertarget{regression-with-scikit-learn-laboratory}{%
\section{Regression with SciKit-Learn Laboratory}\label{regression-with-scikit-learn-laboratory}}

In this section, we'll be predicting the quality of the white wine, based on their physicochemical properties. Because the quality is a number between 0 and 10, we can consider predicting the quality as a regression task. Generally speaking, using training data points, we train three regression models using three different algorithms.

We'll be using the SciKit-Learn Laboratory (or SKLL) package for this. If you're not using the Data Science Toolbox, you can install SKLL using \texttt{pip}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{pip}\NormalTok{ install skll}
\end{Highlighting}
\end{Shaded}

If you're running Python 2.7, you also need to install the following packages:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{pip}\NormalTok{ install configparser futures logutils}
\end{Highlighting}
\end{Shaded}

\hypertarget{preparing-the-data}{%
\subsection{Preparing the Data}\label{preparing-the-data}}

SKLL expects that the train and test data have the same filenames, located in separate directories. However, in this example, we're going to use cross-validation, meaning that we only need to specify a training data set. Cross-validation is a technique that splits up the whole data set into a certain number of subsets. These subsets are called folds. (Usually, five or ten folds are used.)

We need to add an identifier to each row so that we can easily identify the data points later (the predictions are not in the same order as the original data set):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\FunctionTok{mkdir}\NormalTok{ train}
\NormalTok{$ }\ExtensionTok{wine-white-clean.csv}\NormalTok{ nl -s, -w1 -v0 }\KeywordTok{|} \FunctionTok{sed} \StringTok{'1s/0,/id,/'} \OperatorTok{>}\NormalTok{ train/features.csv}
\end{Highlighting}
\end{Shaded}

\hypertarget{running-the-experiment}{%
\subsection{Running the Experiment}\label{running-the-experiment}}

Create a configuration file called \emph{predict-quality.cfg}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{[General]}
\DataTypeTok{experiment_name }\OtherTok{=}\StringTok{ Wine}
\DataTypeTok{task }\OtherTok{=}\StringTok{ cross_validate}

\KeywordTok{[Input]}
\DataTypeTok{train_location }\OtherTok{=}\StringTok{ train}
\DataTypeTok{featuresets }\OtherTok{=}\StringTok{ [["features.csv"]]}
\DataTypeTok{learners }\OtherTok{=}\StringTok{ ["LinearRegression","GradientBoostingRegressor","RandomForestRegressor"]}
\DataTypeTok{label_col }\OtherTok{=}\StringTok{ quality}

\KeywordTok{[Tuning]}
\DataTypeTok{grid_search }\OtherTok{=}\StringTok{ }\KeywordTok{false}
\DataTypeTok{feature_scaling }\OtherTok{=}\StringTok{ both}
\DataTypeTok{objective }\OtherTok{=}\StringTok{ r2}

\KeywordTok{[Output]}
\DataTypeTok{log }\OtherTok{=}\StringTok{ output}
\DataTypeTok{results }\OtherTok{=}\StringTok{ output}
\DataTypeTok{predictions }\OtherTok{=}\StringTok{ output}
\end{Highlighting}
\end{Shaded}

We run the experiment using the \emph{run\_experiment} command-line tool \[cite:run\_experiment\]:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{run_experiment}\NormalTok{ -l evaluate.cfg}
\end{Highlighting}
\end{Shaded}

The \texttt{-l} command-line argument indicates that we're running in local mode. SKLL also offers the possibility to run experiments on clusters. The time it takes to run the experiment depends on the complexity of the chosen algorithms.

\hypertarget{parsing-the-results}{%
\subsection{Parsing the Results}\label{parsing-the-results}}

Once all algorithms are done, the results can now be found in the directory \emph{output}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\BuiltInTok{cd}\NormalTok{ output}
\NormalTok{$ }\FunctionTok{ls}\NormalTok{ -1}
\ExtensionTok{Wine_features.csv_GradientBoostingRegressor.log}
\ExtensionTok{Wine_features.csv_GradientBoostingRegressor.predictions}
\ExtensionTok{Wine_features.csv_GradientBoostingRegressor.results}
\ExtensionTok{Wine_features.csv_GradientBoostingRegressor.results.json}
\ExtensionTok{Wine_features.csv_LinearRegression.log}
\ExtensionTok{Wine_features.csv_LinearRegression.predictions}
\ExtensionTok{Wine_features.csv_LinearRegression.results}
\ExtensionTok{Wine_features.csv_LinearRegression.results.json}
\ExtensionTok{Wine_features.csv_RandomForestRegressor.log}
\ExtensionTok{Wine_features.csv_RandomForestRegressor.predictions}
\ExtensionTok{Wine_features.csv_RandomForestRegressor.results}
\ExtensionTok{Wine_features.csv_RandomForestRegressor.results.json}
\ExtensionTok{Wine_summary.tsv}
\end{Highlighting}
\end{Shaded}

SKLL generates four files for each learner: one log, two with results, and one with predictions. Moreover, SKLL generates a summary file, which contains a lot of information about each individual fold (too much to show here). We can extract the relevant metrics using the following SQL query:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{Wine_summary.tsv}\NormalTok{ csvsql --query }\StringTok{"SELECT learner_name, pearson FROM stdin "}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{"WHERE fold = 'average' ORDER BY pearson DESC"} \KeywordTok{|} \ExtensionTok{csvlook}
\KeywordTok{|}\ExtensionTok{----------------------------+----------------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{learner_name}              \KeywordTok{|} \ExtensionTok{pearson}        \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{----------------------------+----------------}\KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{RandomForestRegressor}     \KeywordTok{|} \ExtensionTok{0.741860521533} \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{GradientBoostingRegressor} \KeywordTok{|} \ExtensionTok{0.661957860603} \KeywordTok{|}
\KeywordTok{|}  \ExtensionTok{LinearRegression}          \KeywordTok{|} \ExtensionTok{0.524144785555} \KeywordTok{|}
\KeywordTok{|}\ExtensionTok{----------------------------+----------------}\KeywordTok{|}
\end{Highlighting}
\end{Shaded}

The relevant column here is \emph{pearson}, which indicates the Pearson's ranking correlation. This is value between -1 and 1 that indicates the correlation between the true ranking (of quality scores) and the predicted ranking. Let's paste all the predictions back to the data set:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{parallel} \StringTok{"csvjoin -c id train/features.csv <(< output/Wine_features.csv_\{\}"}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{".predictions | tr '\textbackslash{}t' ',') | csvcut -c id,quality,prediction > \{\}"}\NormalTok{ ::: \textbackslash{}}
\OperatorTok{>}\NormalTok{ RandomForestRegressor GradientBoostingRegressor LinearRegression}
\NormalTok{$ }\ExtensionTok{csvstack}\NormalTok{ *Regres* -n learner --filenames }\OperatorTok{>}\NormalTok{ predictions.csv}
\end{Highlighting}
\end{Shaded}

And create a plot using \texttt{Rio}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{predictions.csv}\NormalTok{ Rio -ge }\StringTok{'g+geom_point(aes(quality, round(prediction), '}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{'color=learner), position="jitter", alpha=0.1) + facet_wrap(~ learner) + '}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{'theme(aspect.ratio=1) + xlim(3,9) + ylim(3,9) + guides(colour=FALSE) + '}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{'geom_smooth(aes(quality, prediction), method="lm", color="black") + '}\NormalTok{\textbackslash{}}
\OperatorTok{>} \StringTok{'ylab("prediction")'} \KeywordTok{|} \ExtensionTok{display}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=32.81in]{images/ch09-wine-quality-predictions} 

}

\end{figure}

\hypertarget{classification-with-bigml}{%
\section{Classification with BigML}\label{classification-with-bigml}}

In this fourth and last modeling section we're going to classify wines as either red or wine. For this we'll be using a solution called BigML, which provides a prediction API. This means that the actual modeling and predicting takes place in the cloud, which is useful if you need a bit more power than your own computer can offer.

Although prediction APIs are relatively young, they are upcoming, which is why we've included one in this chapter. Other providers of prediction APIs are Google (see \url{https://developers.google.com/prediction}) and PredictionIO (see \url{http://prediction.io}). One advantage of BigML is that they offer a convenient command-line tool called \texttt{bigmler} \citep{bigmler} that interfaces with their API. We can use this command-line like any other presented in this book, but behind the scenes, our data set is being sent to BigML's servers, which perform the classification and send back the results.

\hypertarget{creating-balanced-train-and-test-data-sets}{%
\subsection{Creating Balanced Train and Test Data Sets}\label{creating-balanced-train-and-test-data-sets}}

First, we create a balanced data set to ensure that both class are represented equally. For this, we use \texttt{csvstack} \citep{csvstack}, \texttt{shuf} \citep{shuf}, \texttt{head}, and \texttt{csvcut}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{csvstack}\NormalTok{ -n type -g red,white wine-red-clean.csv }\DataTypeTok{\textbackslash{} }                  
\OperatorTok{>} \OperatorTok{<(<} \ExtensionTok{wine-white-clean.csv}\NormalTok{ body shuf }\KeywordTok{|} \FunctionTok{head}\NormalTok{ -n 1600}\OperatorTok{)} \KeywordTok{|}                 
\OperatorTok{>} \ExtensionTok{csvcut}\NormalTok{ -c fixed_acidity,volatile_acidity,citric_acid,}\DataTypeTok{\textbackslash{} }              
\OperatorTok{>} \ExtensionTok{residual_sugar}\NormalTok{,chlorides,free_sulfur_dioxide,total_sulfur_dioxide,\textbackslash{}}
\OperatorTok{>}\NormalTok{ density,ph,sulphates,alcohol,type }\OperatorTok{>}\NormalTok{ wine-balanced.csv}
\end{Highlighting}
\end{Shaded}

This long command breaks down as follows:

\begin{itemize}
\tightlist
\item
  \texttt{csvstack} is used to combine multiple data sets. It creates a new column \emph{type}, which has the value \emph{red} for all rows coming from the first file \emph{wine-red-clean.csv} and \emph{white} for all rows coming from the second file.
\item
  The second file is passed to \texttt{csvstack} using file redirection. This allows us to create a temporary file using \texttt{shuf}, which creates a random permutation of the \emph{wine-white-clean.csv} and \texttt{head} which only selects the header and the first 1559 rows.
\item
  Finally, we reorder the columns of this data set using \texttt{csvcut} because by default, \texttt{bigmler} assumes that the last column is the label.
\end{itemize}

Let's verify that \emph{wine-balanced.csv} is actually balanced by counting the number of instances per class using \texttt{parallel} and \texttt{grep}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{parallel}\NormalTok{ --tag grep -c \{\} }\ExtensionTok{wine-balanced.csv}\NormalTok{ ::: red white}
\FunctionTok{red}\NormalTok{      1599}
\ExtensionTok{white}\NormalTok{    1599}
\end{Highlighting}
\end{Shaded}

As you can see, the data set \emph{wine-balanced.csv} contains both 1599 red and 1599 white wines. Next we split into train and test data sets using \texttt{split} \citep{split}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\OperatorTok{<} \ExtensionTok{wine-balanced.csv}\NormalTok{ header }\OperatorTok{>}\NormalTok{ wine-header.csv                   }
\NormalTok{$ }\FunctionTok{tail}\NormalTok{ -n +2 wine-balanced.csv }\KeywordTok{|} \ExtensionTok{shuf} \KeywordTok{|} \FunctionTok{split}\NormalTok{ -d -n r/2          }
\NormalTok{$ }\ExtensionTok{parallel}\NormalTok{ --xapply }\StringTok{"cat wine-header.csv x0\{1\} > wine-\{2\}.csv"} \DataTypeTok{\textbackslash{} }
\OperatorTok{>}\NormalTok{ ::: }\ExtensionTok{0}\NormalTok{ 1 ::: train test}
\end{Highlighting}
\end{Shaded}

This is another long command that deserves to be broken down:

\begin{itemize}
\tightlist
\item
  Get the header using \texttt{header} and save it to a temporary file named \emph{wine-header.csv}
\item
  Mix up the red and white wines using \texttt{tail} and \texttt{shuf} and split it into two files named \emph{x00} and \emph{x01} using a round robin distribution.
\item
  Use \texttt{cat} to combine the header saved in \emph{wine-header.csv} and the rows stored in \emph{x00} to save it as \emph{wine-train.csv}; similarly for \emph{x01} and \emph{wine-test.csv}. The \texttt{-\/-xapply} command-line argument tells \texttt{parallel} to loop over the two input sources in tandem.
\end{itemize}

Let's check again number of instances per class in both \emph{wine-train.csv} and \emph{wine-test.csv}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{parallel}\NormalTok{ --tag grep -c }\DataTypeTok{\{2\}}\NormalTok{ wine-}\DataTypeTok{\{1\}}\NormalTok{.csv ::: train test ::: red white}
\ExtensionTok{train}\NormalTok{ red       821}
\ExtensionTok{train}\NormalTok{ white     778}
\BuiltInTok{test}\NormalTok{ white      821}
\BuiltInTok{test}\NormalTok{ red        778}
\end{Highlighting}
\end{Shaded}

That looks like are data sets are well balanced. We're now ready to call the prediction API using \texttt{bigmler}.

\hypertarget{calling-the-api}{%
\subsection{Calling the API}\label{calling-the-api}}

\begin{rmdnote}
You can obtain a BigML username and API key at \url{https://bigml.com/developers}. Be sure to set the variables \emph{BIGML\_USERNAME} and \emph{BIGML\_API\_KEY} in \emph{.bashrc} with the appropriate values.
\end{rmdnote}

The API call is quite straightforward, and the meaning of each command-line argument is obvious from it's name.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{bigmler}\NormalTok{ --train data/wine-train.csv \textbackslash{}}
\OperatorTok{>}\NormalTok{ --test data/wine-test-blind.csv \textbackslash{}}
\OperatorTok{>}\NormalTok{ --prediction-info full \textbackslash{}}
\OperatorTok{>}\NormalTok{ --prediction-header \textbackslash{}}
\OperatorTok{>}\NormalTok{ --output-dir output \textbackslash{}}
\OperatorTok{>}\NormalTok{ --tag wine \textbackslash{}}
\OperatorTok{>}\NormalTok{ --remote}
\end{Highlighting}
\end{Shaded}

The file \emph{wine-test-blind.csv} is just \emph{wine-test} with the \emph{type} column (so the label) removed. After this call is finished, the results can be found in the \emph{output} directory:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{tree}\NormalTok{ output}
\ExtensionTok{output}
\NormalTok{├── }\ExtensionTok{batch_prediction}
\NormalTok{├── }\ExtensionTok{bigmler_sessions}
\NormalTok{├── }\ExtensionTok{dataset}
\NormalTok{├── }\ExtensionTok{dataset_test}
\NormalTok{├── }\ExtensionTok{models}
\NormalTok{├── }\ExtensionTok{predictions.csv}
\NormalTok{├── }\BuiltInTok{source}
\NormalTok{└── }\ExtensionTok{source_test}

\ExtensionTok{0}\NormalTok{ directories, 8 files}
\end{Highlighting}
\end{Shaded}

\hypertarget{inspecting-the-results}{%
\subsection{Inspecting the Results}\label{inspecting-the-results}}

The file which is of most interest is \emph{output/predictions.csv}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{csvcut}\NormalTok{ output/predictions.csv -c type }\KeywordTok{|} \FunctionTok{head}
\BuiltInTok{type}
\ExtensionTok{white}
\ExtensionTok{white}
\FunctionTok{red}
\FunctionTok{red}
\ExtensionTok{white}
\FunctionTok{red}
\FunctionTok{red}
\ExtensionTok{white}
\FunctionTok{red}
\end{Highlighting}
\end{Shaded}

We can compare these predicted labels with the labels in our test data set. Let's count the number of misclassifications:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\ExtensionTok{paste}\NormalTok{ -d, }\OperatorTok{<(}\ExtensionTok{csvcut}\NormalTok{ -c type data/wine-test.csv}\OperatorTok{)} \DataTypeTok{\textbackslash{} }       
\OperatorTok{>} \OperatorTok{<(}\ExtensionTok{csvcut}\NormalTok{ -c type output/predictions.csv}\OperatorTok{)} \KeywordTok{|}
\OperatorTok{>} \FunctionTok{awk}\NormalTok{ -F, }\StringTok{'\{ if ($1 != $2) \{sum+=1 \} \} END \{ print sum \}'} 
\ExtensionTok{766}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  First, we combine the \emph{type} columns of both \emph{data/wine-test.csv} and \emph{output/predictions.csv}.
\item
  Then, we use \texttt{awk} to keep count of when the two columns differ in value.
\end{itemize}

As you can see, BigML's API misclassified 766 wines out of 1599. This isn't a good result, but please note that we just blindly applied an algorithm to a data set, which we normally wouldn't do.

\hypertarget{conclusion}{%
\subsection{Conclusion}\label{conclusion}}

BigML's prediction API has proven to be easy to use. As with many of the command-line tools discussed in this book, we've barely scratched the surface with BigML. For completeness, we should mention that:

\begin{itemize}
\tightlist
\item
  BigML's command-line tool also allows for local computations, which is useful for debugging.
\item
  Results can also be inspected using BigML's web interface.
\item
  BigML can also perform regression tasks.
\end{itemize}

Please see \url{https://bigml.com/developers} for a complete overview of BigML's features.

Although we've only been able to experiment with one prediction API, we do believe that prediction APIs in general are worthwhile to consider for doing data science.

\hypertarget{further-reading}{%
\section{Further Reading}\label{further-reading}}

\begin{itemize}
\tightlist
\item
  Cortez, P., A. Cerdeira, F. Almeida, T. Matos, and J. Reis. 2009. ``Modeling Wine Preferences by Data Mining from Physicochemical Properties.'' Decision Support Systems 47 (4). Elsevier:547--53.
\item
  Hall, Mark, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. ``The WEKA Data Mining Software: An Update.'' SIGKDD Explorations 11 (1). ACM.
\item
  Pearson, K. 1901. ``On Lines and Planes of Closest Fit to Systems of Points in Space.'' Philosophical Magazine 2 (11):559--72.
\item
  Maaten, Laurens van der, and Geoffrey Everest Hinton. 2008. ``Visualizing Data Using T-SNE.'' Journal of Machine Learning Research 9:2579--2605.
\end{itemize}

\hypertarget{chapter-10-conclusion}{%
\chapter{Conclusion}\label{chapter-10-conclusion}}

In this final chapter, the book comes to a close. We'll first recap what we have discussed in the previous nine chapters, and will then offer you three pieces of advice and provide some resources to further explore the related topics we touched upon. Finally, in case you have any questions, comments, or new command-line tools to share, we provide a few ways to get in touch.

\hypertarget{lets-recap}{%
\section{Let's Recap}\label{lets-recap}}

This book explored the power of employing the command line to perform data science tasks. It is an interesting observation that the challenges posed by this relatively young field can be tackled by such a time-tested technology. It is our hope that you now see what the command line is capable of. The many command-line tools offer all sorts of possibilities that are well suited to the variety of tasks encompassing data science.

There are many definitions for data science available. In \protect\hyperlink{chapter-1-introduction}{Chapter 1}, we introduced the OSEMN model as defined by Mason and Wiggens, because it is a very practical one that translates to very specific tasks. The acronym OSEMN stands for obtaining, scrubbing, exploring, modeling, and interpreting data. \protect\hyperlink{chapter-1-introduction}{Chapter 1} also explained why the command line is very suitable for doing these data science tasks.

In \protect\hyperlink{chapter-2-getting-started}{Chapter 2}, we explained how you can set up your own Data Science Toolbox and install the bundle that is associated with this book. \protect\hyperlink{chapter-2-getting-started}{Chapter 2} also provided an introduction to the essential tools and concepts of the command line.

The OSEMN model chapters---\protect\hyperlink{chapter-3-obtaining-data}{Chapter 3} (obtaining), \protect\hyperlink{chapter-5-scrubbing-data}{Chapter 5} (scrubbing), \protect\hyperlink{chapter-7-exploring-data}{Chapter 7} (exploring), and \protect\hyperlink{chapter-9-modeling-data}{Chapter 9} (modeling)---focused on performing those practical tasks using the command line. We haven't devoted a chapter to the fifth step, interpreting data, because, quite frankly, the computer, let alone the command line, is of very little use here. We have, however, provided some pointers for further reading on this topic.

In the three intermezzo chapters, we looked at some broader topics of doing data science at the command line, topics which are not really specific to one particular step. In \protect\hyperlink{chapter-4-creating-reusable-command-line-tools}{Chapter 4}, we explained how you can turn one-liners and existing code into reusable command-line tools. In \protect\hyperlink{chapter-6-managing-your-data-workflow}{Chapter 6}, we described how you can manage your data workflow using a command-line tool called Drake. In \protect\hyperlink{chapter-8-parallel-pipelines}{Chapter 8}, we demonstrated how ordinary command-line tools and pipelines can be run in parallel using GNU Parallel. These topics can be applied at any point in your data workflow.

It is impossible to demonstrate all command-line tools that are available and relevant for doing data science. New command-line tools are created on a daily basis. As you may have come to understand by now, this book is more about the idea of using the command line, rather than giving you an exhaustive list of tools.

\hypertarget{three-pieces-of-advice}{%
\section{Three Pieces of Advice}\label{three-pieces-of-advice}}

You probably spent quite some time reading these chapters and perhaps also following along with the code examples. In the hope that it maximizes the return on this investment and increases the probability that you'll continue to incorporate the command line into your data science workflow, we would like to offer you three pieces of advice: (1) be patient, (2) be creative, and (3) be practical. In the next three subsections we elaborate on each piece of advice.

\hypertarget{be-patient}{%
\subsection{Be Patient}\label{be-patient}}

The first piece of advice that we can give is to be patient. Working with data on the command line is different from using a programming language, and therefore it requires a different mindset.

Moreover, the command-line tools themselves are not without their quirks and inconsistencies. This is partly because they have been developed by many different people, over the course of multiple decades. If you ever find yourself at a loss regarding their mind-dazzling options, don't forget to use --help, man, or your favorite search engine to learn more.

Still, especially in the beginning, it can be a frustrating experience. Trust us, you will become more proficient as you practice using the command line and its tools. The command line has been around for many decades, and will be around for many more to come. It is a worthwhile investment.

\hypertarget{be-creative}{%
\subsection{Be Creative}\label{be-creative}}

The second, related piece of advice is to be creative. The command line is very flexible. By combining the command-line tools, you can accomplish more than you might think.

We encourage you to not immediately fall back onto your programming language. And when you do have to use a programming language, think about whether the code can be generalized or reused in some way. If so, consider creating your own command-line tool with that code using the steps we discussed in \protect\hyperlink{chapter-4-creating-reusable-command-line-tools}{Chapter 4}. If you believe your command-line tool may be beneficial for others, you could even go one step further by making it open source.

\hypertarget{be-practical}{%
\subsection{Be Practical}\label{be-practical}}

The third piece of advice is to be practical. Being practical is related to being creative, but deserves a separate explanation. In the previous subsection, we mentioned that you should not immediately fall back to a programming language. Of course, the command line has its limits. Throughout the book, we have emphasized that the command line should be regarded as a companion approach to doing data science.

We've discussed four steps for doing data science at the command line. In practice, the applicability of the command line is higher for step 1 than it is for step 4. You should use whatever approach works best for the task at hand. And it's perfectly fine to mix and match approaches at any point in your workflow. The command line is wonderful at being integrated with other approaches, programming languages, and statistical environments. There's a certain trade-off with each approach, and part of becoming proficient at the command line is to learn when to use which.

In conclusion, when you're patient, creative, and practical, the command line will make you a more efficient and productive data scientist.

\hypertarget{where-to-go-from-here}{%
\section{Where To Go From Here?}\label{where-to-go-from-here}}

As this book is on the intersection of the command line and data science, many related topics have only been touched upon. Now, it's up to you to further explore these topics. The following subsections provide a list of topics and suggested resources to consult.

\hypertarget{apis}{%
\subsection{APIs}\label{apis}}

\begin{itemize}
\tightlist
\item
  Russell, Matthew. 2013. Mining the Social Web. 2nd Ed. O'Reilly Media.
\item
  Warden, Pete. 2011. Data Source Handbook. O'Reilly Media.
\end{itemize}

\hypertarget{shell-programming}{%
\subsection{Shell Programming}\label{shell-programming}}

\begin{itemize}
\tightlist
\item
  Winterbottom, David. 2014. ``Commandlinefu.com.'' http://www.commandlinefu.com.
\item
  Peek, Jerry, Shelley Powers, Tim O'Reilly, and Mike Loukides. 2002. Unix Power Tools. 3rd Ed. O'Reilly Media.
\item
  Goyvaerts, Jan, and Steven Levithan. 2012. Regular Expressions Cookbook. 2nd Ed. O'Reilly Media.
\item
  Cooper, Mendel. 2014. ``Advanced Bash-Scripting Guide.'' http://www.tldp.org/LDP/abs/html.
\item
  Robbins, Arnold, and Nelson H. F. Beebe. 2005. Classic Shell Scripting. O'Reilly Media.
\end{itemize}

\hypertarget{python-r-and-sql}{%
\subsection{Python, R, and SQL}\label{python-r-and-sql}}

\begin{itemize}
\tightlist
\item
  Wickham, Hadley. 2009. ggplot2: Elegant Graphics for Data Analysis. Springer.
\item
  McKinney, Wes. 2012. Python for Data Analysis. O'Reilly Media.
\item
  Rossant, Cyrille. 2013. Learning Ipython for Interactive Computing and Data Visualization. Packt Publishing.
\end{itemize}

\hypertarget{interpreting-data}{%
\subsection{Interpreting Data}\label{interpreting-data}}

\begin{itemize}
\tightlist
\item
  Shron, Max. 2014. Thinking with Data. O'Reilly Media.
\item
  Patil, DJ. 2012. Data Jujitsu. O'Reilly Media.
\end{itemize}

\hypertarget{getting-in-touch}{%
\section{Getting in Touch}\label{getting-in-touch}}

This book would not have been possible without the many people who created the command line and the numerous command-line tools. It's safe to say that the current ecosystem of command-line tools for data science is a community effort. We have only been able to give you a glimpse of the many command-line tools available. New ones are created everyday, and perhaps some day you will create one yourself. In that case, we would love to hear from you. We'd also appreciate it if you would drop us a line whenever you have a question, comment, or suggestion. There are a couple of ways to get in touch:

\begin{itemize}
\tightlist
\item
  Email: \href{mailto:jeroen@datascienceworkshops.com}{\nolinkurl{jeroen@datascienceworkshops.com}}
\item
  Twitter: \href{https://twitter.com/jeroenhjanssens/}{@jeroenhjanssens}
\item
  Book website: \url{http://datascienceatthecommandline.com/}
\item
  Book GitHub repository: \url{https://github.com/jeroenjanssens/data-science-at-the-command-line}
\end{itemize}

  \bibliography{library.bib,tools.bib}

\end{document}
